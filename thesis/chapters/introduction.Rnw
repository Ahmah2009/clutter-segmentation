\section{Intended Purpose}

Object recognition is one of the fundamental tasks in computer vision. One of
its applications is in robotics. Especially in household environments it is
often not only necessary for a robot to merely detect the presence of an
object, but also to identify and locate an object in order make further
decisions on semantic level.

A particular application addressed by this work is a robot in a household
environment who is given the task to go shopping, and which empties the
shopping bag onto a table, and then plans to move the bought items to
appropriate storage facilities. The shopping items on the table form a
cluttered scene. Most of the shopping items can be distinguished by
characteristic texture, made of pictorial descriptions of the product, labels,
and company logos. Recognition of these shopping items is rendered difficult by
high levels of occlusion and the presence of many such textured objects in one
single scene. The objects might be arbitrarily oriented and prior assumptions
about their principal axes are likely to be invalid.

The system presented in this work equips a PR2 robot with the ability to locate
one object in the cluttered scene and provide sufficient information to a
grasping pipeline, such that the clutter can be resolved by removing items
one-by-one from the scene. Although, in the shopping scenario, the robot might
have prior knowledge about which objects to locate in the scene, this
information is not used in this work, in order to develop a system that is
general enough to be applied to other application scenarios.

\section{Problem Statement}

The challenge is therefore to correctly label and determine the pose of a
textured object in a scene with clutter and high occlusion, and finally to
provide enough information to allow the robot grasp the object and remove it
from the scene.

\section{Related Work}

In literature, many approaches to object recognition have been described. They
differ in scope, targeted application and techniques. The approaches that use
local features have been especially relevant to this work. They have in common
that objects are described by a set of local features, that similarity between
these features gives evidence for the object being present on the scene and a
technique is required that, given the evidence, infers poses of objects within
the scene.

Tuytelaars and Mikolajczyk give a qualitative review of local feature detectors
and descriptors \cite{Tuytelaars2007}, and provide guidance in making an
appropriate choice between a multitude of available detectors and descriptors.
Rosten and Drummond introduced the FAST feature detector \cite{Rosten2006},
while Agrawal et al. present the CenSurE feature detector \cite{Agrawal2008},
both designed to be efficient. Calonder et al. recently invented BRIEF, a
feature descriptor which is simple to compute and efficient to match
\cite{Calonder2010}. Matching observed features with model features is a common
sub-task in object recognition. For this, Muja describes the FLANN library
\cite{Muja2009}. Slaney and Casey give an introduction into Locality Sensitive
Hashing \cite{Slaney2008}, an algorithm for finding approximate nearest
neighbors that has been originally introduced by TODO, and which can also be
adapted to match binary features such as BRIEF. A combination of the FAST
detector and an oriented version of the BRIEF descriptor has been recently
included into OpenCV trunk. This feature detector and descriptor is called ORB
and will be more closely evaluated in section TODO. Lowe showed how to
recognize objects up to a 20 degree rotation using the scale-invariant feature
transform (SIFT), making use of the Hough Transform where correspondences 
vote for the pose of an object \cite{Lowe1999}. Hough voting has also been used
in a system presented by Drost et al.\cite{Drost2010}. The RANSAC algorithm
presented by Fischler and Bolles constitutes a widely used TODO method for 3D
pose estimation. Dogar and Srinivasa recently spent research on robustly
grasping objects in a cluttered scene \cite{Dogar2010}. Alpaydin's excellent
introduction to machine learning \cite{Alpaydin2010} has influenced the
decisions described in section TODO.  Fawcett \cite{Fawcett2006} published a
detailed guide on how to evaluate the performance of classifiers through
analysis of receiver operating characteristics, which are tightly bound to the
field of decision theory, which again is covered by Melsa and Cohn
\cite{Melsa1978}.

% Haltakov and Pangercic have developed the ODUFinder library for the Robot
% Operating System to detect both non-textured objects and textured objects,
% the latter by using SIFT and vocabulary trees.

\section{Selected Approach}

Our system, as described in TODO, is built on top of the existing Textured
Object Detection ({\it tod}) package for the Robot Operating System, targeting
the PR2 robot. It solely makes use of 2-d local features to describe objects.
It is possible to plug-in different feature detectors and descriptors, yet the
focus was on Oriented BRIEF. Matching these local features results in 
correspondences between features of the models and the query scene. RANSAC
generates pose estimates that correspond to consensus set. Each consensus set
is a group of correspondences voting for the same pose. The pose estimate
corresponding to the largest consensus set is then refined in a the second step
to improve accuracy and an attempt to obtain an even larger consensus set,
which will provide useful for grasping. If the second step fails to do so,
other pose estimates will be tried iteratively to produce a robust pose
estimate. The system heavily exploits the assumption that a robot requires only
to detect one object in the scene. An experiment runner conveniently allows to
test different parameter configurations against a validation set, and such
supports parameter optimization.

