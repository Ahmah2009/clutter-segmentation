\section{Intended Purpose}

Object recognition is one of the fundamental tasks in computer vision. One of
its applications is in robotics. Especially in household environments, it is
often necessary for a robot to detect the presence of an object, as well as
find the pose of the object such that the robot can grasp it and make further
decisions on a semantic level.

A particular application addressed by this work involves a robot in a household
environment. The robot is given the task to go shopping, to empty the shopping
bag onto a table, and to move the items to the appropriate storage facilities.
When the items are laid out on the table, they form a cluttered scene.  Most of
these objects are rigid bodies that can be distinguished by their
characteristic texture, which is composed of pictorial descriptions of the
product, labels, and company logos. High levels of occlusion and the presence
of several such textured objects in a single scene renders recognition of these
shopping items difficult. Since the objects may be arbitrarily oriented, prior
assumptions about their principal axes are likely invalid.

The system presented in this work enables a Personal Robot 2 (PR2\footnote{http://www.willowgarage.com/pages/pr2/overview}) to detect an object and
estimate its pose in the cluttered scene to provide sufficient information to a
grasping pipeline, such that the clutter can be resolved by removing items
one-by-one from the scene. Although, in the shopping scenario, the robot may
have prior knowledge about what objects are present in the scene, this
information is not used in this work, in order to develop a system that is
general enough to be applied to other application scenarios.

\section{Problem Statement}
\label{section:problem-statement}

The challenge is therefore to correctly identify a rigid textured object, to
determine its pose in a scene with clutter and high occlusion, and to
subsequently provide enough information to allow the robot to grasp the object
and to remove it from the cluttered scene.

\section{Related Work}

In literature many different approaches to object recognition are described.
The techniques using local features have been especially relevant to this work. 

Tuytelaars and Mikolajczyk qualitatively review local feature detectors
and descriptors \cite{Tuytelaars2007}, and provide guidance for making an
appropriate choice between a multitude of available detectors and descriptors.
Lowe's work on the scale-invariant feature transform explains the principles
of the scale-space pyramid and rotation invariance of features \cite{Lowe1999}.
Rosten and Drummond introduce the FAST feature detector \cite{Rosten2006}.
% while Agrawal et al. presented the CenSurE feature detector \cite{Agrawal2008},
% both designed to be efficient.
Calonder et al.\ describe BRIEF, a feature descriptor whose generated
descriptor vectors are efficient to match \cite{Calonder2010}. Oriented BRIEF,
derived from FAST and BRIEF, is the feature detector and descriptor chosen for
this work \cite{Rublee2011}.  A common sub-task in object recognition is matching observed features
with model features. % For this, Muja described the FLANN library \cite{Muja2009}.
Gionis et al.\ introduce, and Slaney and Casey summarize Locality Sensitive
Hashing \cite{Gionis1999, Slaney2008}, which finds approximate
nearest neighbours.
% Lowe showed how to recognize
% objects up to a 20 degree rotation using the scale-invariant feature transform
% (SIFT), making use of the Hough Transform where correspondences vote for the
% pose of an object \cite{Lowe1999}.
% Hough voting has also been used in a system presented by Drost et
% al. \cite{Drost2010}.
Fischler and Bolles excellently explain RANSAC and deal with the
perspective-n-point problem \cite{Fischler1981}.  Dogar and Srinivasa recently
researched the problem of robustly grasping objects in a cluttered scene
\cite{Dogar2010}. Forsyth and Ponce's work on computer vision served as a
reference and is what influenced the terminology in this work \cite{Forsyth2003}. The
same applies to Gonzalez and Woods in the field of image processing
\cite{Gonzalez2010}.  Alpaydin's excellent introduction to machine learning
\cite{Alpaydin2010} has influenced our experiment design and parameter
optimization. Fawcett published a detailed guide on how to
evaluate the performance of classifiers through analysis of receiver operating
characteristics \cite{Fawcett2006}. Melsa and Cohn cover decision and estimation
theory \cite{Melsa1978}.

% Haltakov and Pangercic have developed the ODUFinder library for the Robot
% Operating System to detect both non-textured objects and textured objects,
% the latter by using SIFT and vocabulary trees.

\section{Selected Approach}

\subsubsection{General Approach}

Objects generally differ in shape and appearance. Recognizing the shopping
items in a scene as shown in \refFigure{figure:clutter-pr2-kinect} (a) is
difficult. In general, shopping items are often shaped similarly. Worse,
different products are sold in packagings of the same shape. The distinguishing
feature of these objects is texture.

Thus, it seems natural to rely on texture to recognize objects.  We originally
considered to combine the information from textured surfaces with the
information contained in the shapes of the rigid bodies, however, we solely
used texture information, and based our system on the existing Textured Object
Detection (\tod) stack in the Robot Operating System.

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{clutter-pr2-kinect}
    \end{center}
    \caption[A cluttered scene, a Kinect depth image, and a PR2 robot]{From
        left to right: (a) Four rigid textured objects occluding each other in a
        cluttered scene; (b) a 3D-visualization of the same scene, obtained with a
        Kinect RGB-D camera; (c) the PR2 robot at Technische Universit채t M체nchen with a
        Kinect camera mounted on its top.}
    \label{figure:clutter-pr2-kinect}
\end{figure}

We required our system to run on a PR2 robot at Technische Universit채t M체nchen,
as depicted in \refFigure{figure:clutter-pr2-kinect} (c). The PR2 is a robotic
platform produced by Willow Garage. A Kinect RGB-depth camera is mounted on top
of the robot. It simultaneously provides colour images and depth measurements. 
The Kinect camera measures the time-of-flight of near-infrared rays to compute
the distance between camera and 3D points. The typical depth range of
a Kinect camera is between 0.8 and 3.5 meters \cite{PrimeSense2010}. A depth
image obtained by this camera is visualized in
\refFigure{figure:clutter-pr2-kinect} (b).

In the presented approach, we first learn a model from each rigid textured
object. An object is rotated in front of the camera, which takes
images and 3D scans from different views of the object. For each view, local 2D
features are extracted from the greyscale image.  \refFigure{figure:keypoints}
shows both an image of a textured object (a) and the keypoints extracted from a
greyscale version of the image (b). The local 2D features, together with the
associated 3D points on the object's surface, form the {\it \glspl{model
feature}}. The model features make up the model of this object. The 3D part of
the model, a sparse point cloud, is described in a standard coordinate system.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{keypoints}
    \end{center}
    \caption[The textured object \haltbareMilch and extracted keypoints]{From left to right: (a) Textured object \haltbareMilch; (b) the
        keypoints of local 2D features detected on textured surface of \haltbareMilch}
    \label{figure:keypoints}
\end{figure}

Given an image of a scene, we can use the models to recognize instances of the
\glspl{template object}. For clarity, we call a scene a {\it \gls{query scene}} if we want
to recognize objects in it. First, local 2D features are extracted from a
greyscale {\it \gls{query image}} depicting the query scene. These features are
called {\it \glspl{query feature}} in order to distinguish them from model features.
Nearest-neighbour search permits to find {\it \glspl{correspondence}} between a query
feature and one or more model features. Some of these correspondences for a
query scene are shown in \refFigure{figure:matches}. Based on these
correspondences, a model-fitting algorithm that is robust with respect to
outliers, aligns models to the 3D scene such that their projections onto the
image plane explain the observed data.  Finally, if an {\it \gls{aligned model}}
sufficiently explains the correspondences, the system {\it \glspl{guess}} the object
to appear in the query scene at the pose of the aligned model. A {\it \gls{confidence value}}
measures the degree to which a guess explains the observed data. Although we do
not explicitly make use of a statistical model, the general idea is based on
the likelihood principle.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=\textwidth]{matches}
    \end{center}
    \caption[The correspondences between a query image and a selected model
view]{The (hypothetical) correspondences between the query image (left), and a
selected view of the template object (right)} \label{figure:matches}
\end{figure}

\subsubsection{Implementation}

The basic steps in learning models and recognizing objects are already
implemented in the Textured Object Detection (\tod) stack. \tod supports the
use of different local 2D feature detectors and descriptors. It supports a
range of nearest-neighbour search algorithms that find correspondences between
feature sets. Finally, \tod uses RANSAC as the afore-mentioned model-fitting
algorithm for estimating the objects' poses. The confidence value is measured by
the number of correspondences consistent with this pose. 

We encounter cluttered scenes (\refFigure{figure:clutter-pr2-kinect} (a)) that do
not only exhibit high occlusion rates, but also show objects in arbitrary
orientations. For our application, it is sufficient to resolve the clutter
object by object. Hence, we chose to concentrate on finding the pose of only
one object in the scene. In the following, when we are talking about {\it
recognizing} an object, we mean both detecting its presence and estimating its
pose.

We present a new system called \clutseg, built on top of \tod. It has three
purposes: Primarily, it adapts \tod to our purpose of resolving a cluttered
scene by locating one object in the scene.  Second, it helps choosing between
the multitude of available feature detectors and descriptors, and selecting one
of the nearest-neighbour search algorithms. Third, it helps to find good parameter
values since \tod and \clutseg have many parameters.

\clutseg locates an object by ranking the initial guesses from \tod by
confidence value. Then, the best-ranked guesses are refined until a guess with
sufficiently high confidence value is found.

We chose Oriented BRIEF (ORB) as a feature detector and descriptor. It yields
binary descriptor vectors, which can be compared efficiently. We traded
exactness for speed and chose Locality Sensitive Hashing for finding
approximate nearest neighbours between these binary vectors. ORB is a recent
development, and we were curious about how it performs in an application.

% The new Oriented
% BRIEF 2D local feature detector additionally aroused our curiosity.

% Our system has been built on top of the Textured Object Detection packa
% targeting the PR2 robot. It describes objects by a set of local 2D features
% extracted from multiple views in order to account for instances appearing in
% arbitrary orientations. Oriented BRIEF, a feature detector and binary feature
% descriptor, is used to generate local 2D features for both the models and the
% query scenes. Local Sensitive Hashing, an approximate nearest neighbour search
% algorithm, finds correspondences between features of the models and the query
% scene. RANSAC is used to solve the perspective-n-point problem to generate a
% set of pose estimates. The pose estimate with the largest consensus set
% returned by RANSAC is then refined. The refinement involves matching features
% against a particular model and repeating the pose estimation process.

% Larger consensus sets are preferred, because they make robotic grasping easier.
% The system heavily exploits the requirement that a robot needs only to locate
% one object in the scene. An experiment runner conveniently permits to test
% different parameter configurations against a validation set, and such supports
% parameter optimization.

