\SweaveOpts{echo=FALSE}


<<>>=
    source("../scripts/common.R")
@

This chapter presents the experimental results of the \clutseg system, on the
validation set, on a test set and results from a live test on the PR2 robot at
Technische Universität München. It covers how the modelbase was built, how
validation data, ground truth, and the test data have been obtained. We discuss
the parameter set found by experimentation. Scenes are presented in which the
system performs well.  We also cover the limitations and issues that were
revealed during the evaluation.

\section{Data Acquisition}
\label{section:data-acquisition}

This section shows how all the data required for constructing the modelbases,
the validation set and the test set was obtained. Those three artifacts all
required raw data, which have been collected in a similar fashion, using the
same hardware and software setup.

% The Robot
% Operating System running on the PR2 robot is organized as a distributed system
% of nodes, which communicate via topics. Nodes can subscribe and publish to
% topics. Data from topics can be recorded in bag files.

\subsection{Common Setup}

We used the Willow Garage PR2 robot at TUM to collect raw data.  On top of the
PR2, a Kinect RGB-D camera was mounted. A wooden board with two chessboards
attached was setup in about one meter distance from the camera. This board
served as a rotating table. The chessboards were printed on white DIN-A4 paper
using a laser printer, and around them was enough white margin, as recommended
in the documentation of method {\tt findChessboardCorners} in \opencv. The
recording took place in an indoor laboratory environment whose lighting
conditions were roughly similar to the ones to be expected in an application
scenario. The board was manually rotated, carefully and not too fast.  The
objects must not move with respect to the fiducial markers, which are attached
to the object coordinate system.

  The resolution of the image was 1280x1024 pixels, and the dense point cloud
had a resolution of 640x480. The sampling rate was set to about one depth image
per second. The camera matrix needs to be recorded as well, for it is required
at least in solving the point-to-point correspondence problem in pose
estimation. 

\subsection{Modelbases}
\label{subsection:modelbase}

% Acquiring modelbases was done in two independent steps. First, raw data
% was collected once. Second, several modelbases were built according to 
% the feature parameter specifications in the experiments. 


\tod requires multiple views from the template object. For the modelbases, we
saved about 60--70 views per object to a bag file, which has been observed to
work better than a bag with only 40 views. For each object, around 1 Gigabyte
of data was recorded. The next preparatory steps consisted in uncompressing the
bag files with a script included in both \clutseg and \tod, in estimating the
object-view transformation for each view, and in masking for each view as
described in \refSection{subsection:learning}.  These post-processed raw data
provide the masks and the object-view transformation which can be used to
create several modelbases using different feature detectors and descriptors.
The resulting models of each template object were much smaller in size than the
original raw data. The models took up around 1 Megabyte of disk space each,
three magnitudes less than the original raw data.  The feature detectors are
responsible for this data reduction.  In the experiments, ORB selected 394
keypoints on average per image of the modelbase raw data.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{media/modelbase}
        \caption[The four textured objects used in evaluation]{The four objects \assamTea, \haltbareMilch, \jacobsCoffee, and \icedtea, which were used in evaluation}
        \label{figure:modelbase}
    \end{center}
\end{figure}

We collected raw data for four different rigid textured objects
(\refFigure{figure:modelbase}).  These are items commonly found in German
supermarkets. These are \assamTea, \haltbareMilch, \icedtea, and \jacobsCoffee.
The objects have different properties. For example, \assamTea is smaller than
\icedtea. The front of \icedtea exhibits characteristic texture. Its back,
though, only shows smallprint.  \jacobsCoffee shows texture that repeats on
different sides of the object.  Finally, \haltbareMilch contains characteristic
texture which is confined only to certain regions on its surface.

% Would be nice to have:
% \begin{figure}
%    \begin{center}
%        \includegraphics[width=0.3\textwidth]{media/modelbase-3D}
%        \caption[The model features of a selected modelbase]{Model features of a selected modelbase}
%        \label{figure:modelbase-3D}
%    \end{center}
% \end{figure}

A practical consequence of \refEquation{equation:fiducial-object} is that a
template object must not move with respect to the fiducial markers when
collecting raw data. This would be equivalent to redefining the object
coordinate system.

\subsection{Validation Set}

The validation set, which was used for optimizing parameter values, was
collected similarly to the data for the modelbases. Again, the fiducial markers
were required for computing ground truth for the objects on the validation
scenes.  We recorded around 50--60 scenes, each of them showing three instances
of objects in the modelbase. Out of those, we chose 21 scenes at roughly equal
angle increments to form the validation set. The scenes show the objects with
varying levels of occlusion. Each object appears in upright orientation.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{media/validation-scene}
\caption[The object coordinate systems defined in a validation scene]{From left to right: (a) A validation scene showing \jacobsCoffee,
\haltbareMilch, \icedtea, and the ground truth; (b) three coordinate systems
that must align with the object coordinate systems of the three instances.}
\label{figure:validation-scene}
\end{center}
\end{figure}


Given a scene, ground truth requires the computation of the object-view transformations
$\oTvi{i}$, $\oTvi{j}$, and $\oTvi{k}$ for the three objects $i$, $j$ and $k$
in the scene. \refFigure{figure:validation-scene} shows three coordinate
systems, defined with respect to the fiducial markers. The object coordinate
system was defined for an object when the model was being learnt. Therefore, an
object must be placed in the validation scene such that its object coordinate
system aligns with one of the three coordinate systems as depicted in
\refFigure{figure:validation-scene}. We placed object $j$ in the center between
the fiducial markers, such that 

\begin{equation}
    \oTvi{j}: \vec{p} \mapsto\ \fTv \pth{\vec{p}}
\end{equation}

In other words, object $j$ was placed on the rotating table in the same way as
when raw data for a modelbase was being collected. Pencil markers on the
template objects helped us to remember the correct arrangement. Imprecision was
difficult to avoid when we placed the objects manually on the table. This
introduces some noise in the ground truth data.

The other two objects $i$ and $k$ were placed upright next to object $i$, with
offset of $-0.15$ and $0.13$ (in metres) in x-direction of fiducial coordinates

\begin{equation}
    \oTvi{i}: \vec{p} \mapsto\ \fTv \pth{ \vec{p} - \begin{pmatrix} 0.15 \\ 0 \\ 0 \end{pmatrix} }
\end{equation}

\begin{equation}
    \oTvi{k}: \vec{p} \mapsto\ \fTv \pth{ \vec{p} + \begin{pmatrix} 0.13 \\ 0 \\ 0 \end{pmatrix} }
\end{equation}



\subsection{Test Set}

The test set was made of cluttered scenes of various character. All four
objects are shown on the test scene. Some scenes showed objects in upright
position, but some objects occluding others. Other scenes showed objects in
arbitrary orientations and with medium occlusion. Other, yet more difficult
scenes, showed objects in arbitrary orientations and with severe occlusion.

The test set data have been collected similarly to the validation set, but
ground truth was not computed. The evaluation on the test set was therefore
restricted to visual analysis.

\section{Experiments}

Here we describe the regions in parameter space that have been covered by our
experiments. We discuss the observations made during experimentation and
present the parameter set that has been found to work well on the validation
set.

\subsection{Searched Parameter Space}


<<>>=
    # Get the number of experiments in total
    e_all = dbGetQuery(con, paste(
        "select count(e.id) as cnt,",
        "sum(r.train_runtime) as total_train_runtime,",
        "avg(r.train_runtime) as avg_train_runtime,",
        "sum(r.test_runtime) as total_test_runtime,",
        "avg(r.test_runtime) as avg_test_runtime",
        "from experiment e",
        "join response r on e.response_id = r.id"));
    # Get the number of experiments with FAST and RBRIEF
    e_fast_rbrief = dbGetQuery(con, paste(
          "select count(e.id) as cnt",
          "from experiment e",
          "join paramset p on e.paramset_id = p.id",
          "join pms_fe t on p.train_pms_fe_id = t.id",
          "join pms_fe r on p.recog_pms_fe_id = r.id",
          "where e.response_id not null",
          "and t.detector_type = 'FAST'",
          "and t.descriptor_type = 'rBRIEF'",
          "and r.detector_type = 'FAST'",
          "and r.descriptor_type = 'rBRIEF'"))
    # Get the number of experiments with ORB
    e_orb = dbGetQuery(con, paste(
          "select count(e.id) as cnt",
          "from experiment e",
          "join paramset p on e.paramset_id = p.id",
          "join pms_fe t on p.train_pms_fe_id = t.id",
          "join pms_fe r on p.recog_pms_fe_id = r.id",
          "where e.response_id not null",
          "and t.detector_type = 'ORB'",
          "and t.descriptor_type = 'ORB'",
          "and t.extractor_type = 'ORB'",
          "and r.detector_type = 'ORB'",
          "and r.descriptor_type = 'ORB'",
          "and r.extractor_type = 'ORB'"))
@

In total, \Sexpr{e_all$cnt} experiments with different parameter sets have been
conducted. In \Sexpr{e_fast_rbrief$cnt} experiments, we used FAST as a feature
detector and rBRIEF as a feature descriptor. In \Sexpr{e_orb$cnt} experiments,
we chose ORB instead. 

Altogether, the experiments took more than \Sexpr{floor(e_all$total_test_runtime / 3600)}~hours
of wall-clock time to compute, an average of
\Sexpr{round(e_all$avg_test_runtime / 60, 1)}~minutes per experiment.
\refTable{table:hardsoftware-parameter-optimization} lists the hardware and software
configuration.

<<>>=
    rm(e_all)
    rm(e_fast_rbrief)
    rm(e_orb)
@

\subsection{Observations}

Finding good parameters has been an interactive process. New parameter sets
have been evaluated in batches. A smoothed scatter plot shows the achieved
success rates on the validation set over time
(\refFigure{figure:experiment-progress}~a). Experiments 3105--4129 have
explored the neighbourhood of a promising experiment 2336. This explains the
plateau starting with experiment 3100. The few experiments where \clutseg
achieved a success rate of greater than $90\%$ on the validation set
(\refFigure{figure:experiment-progress}~b) are outliers. This is indicated by
averaging runs with the same parameter values as a prior experiment with 
a high success rate.

\begin{figure}
    \begin{center}
  \setkeys{Gin}{width=\textwidth} 
<<fig=true,png=true,height=4, width=10>>=
    r_all = dbGetQuery(con, paste(
        "select experiment_id, succ_rate",
        "from view_experiment_response"));
    xy = lowess(r_all$experiment_id, r_all$succ_rate, f = 1./150)
    # plot(r_all$experiment_id, r_all$succ_rate, type="lines")
    layout(matrix(c(1, 2), 1, 2))
    plot(xy$x, xy$y, type="lines", main="Success rate over time", xlab="Experiment identifier", ylab = "Success rate")
    hist(r_all$succ_rate, xlab = "Success rate", main="Histogram of success rates")
@
    \end{center}
    \caption[The observed success rates in the experiments]{From left to right:
(a) the smoothed scatter plot of the success rates. Identifiers are assigned to
experiments in increasing order over time. (b) The observed distribution of
success rates. Useless parameter values result in a zero success rate.
Only a few parameter sets led to success rates greater than $90\%$.}
    \label{figure:experiment-progress}
\end{figure}

\subsection{Selected Parameter Set}

<<>>=
    sps_id = 4180
    sps = dbGetQuery(con, paste(
          "select e.id as id,",
          # parameters
          "tf.detector_type as detector_type,",
          "tf.descriptor_type as descriptor_type,",
          "tf.extractor_type as extractor_type,",
          "tf.n_features as n_features,",
          "tf.octaves as octaves,",
          "tf.scale_factor as scale_factor,",
          "dm.matcher_type as detect_matcher_type,",
          "dm.knn as detect_knn,",
          "dm.do_ratio_test as detect_do_ratio_test,",
          "dm.ratio_threshold as detect_ratio_threshold,",
          "dg.ransac_iterations_count as detect_ransac_iterations_count,",
          "dg.max_projection_error as detect_max_projection_error,",
          "dg.min_inliers_count as detect_min_inliers_count,",
          "rm.matcher_type as refine_matcher_type,",
          "rm.knn as refine_knn,",
          "rm.do_ratio_test as refine_do_ratio_test,",
          "rm.ratio_threshold as refine_ratio_threshold,",
          "rg.ransac_iterations_count as refine_ransac_iterations_count,",
          "rg.max_projection_error as refine_max_projection_error,",
          "rg.min_inliers_count as refine_min_inliers_count,",
          "g.accept_threshold as accept_threshold",
          # joins 
          "from experiment e",
          "join response r on e.response_id = r.id",
          "join paramset p on e.paramset_id = p.id",
          "join pms_fe tf on p.train_pms_fe_id = tf.id",
          "join pms_fe rf on p.recog_pms_fe_id = rf.id",
          "join pms_match dm on p.detect_pms_match_id = dm.id",
          "join pms_match rm on p.refine_pms_match_id = rm.id",
          "join pms_guess dg on p.detect_pms_guess_id = dg.id",
          "join pms_guess rg on p.refine_pms_guess_id = rg.id",
          "join pms_clutseg g on p.recog_pms_clutseg_id = g.id",
          "where tf.detector_type = 'ORB'",
          "and tf.descriptor_type = 'ORB'",
          "and tf.extractor_type = 'ORB'",
          "and rf.detector_type = tf.detector_type",
          "and rf.descriptor_type = tf.descriptor_type",
          "and rf.extractor_type = tf.extractor_type",
          "and rf.n_features = tf.n_features",
          "and rf.octaves = tf.octaves",
          "and rf.scale_factor = tf.scale_factor",
          "and e.id = ", sprintf("%d", sps_id)))
          #"order by r.succ_rate desc limit 1"))
@

Here we present a parameter set, dubbed SPS, that has been found to work well
on the validation set. It contains values for all configurable system
parameters. \refTable{table:selected-parameters-features} provides values for
feature extraction. We used the same feature extraction parameters for the
modelbase and for the query scenes. We used the default values from \opencv for
parameters {\tt octaves} and {\tt scale\_factor}.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{ll}
        Parameter & Value \\
        \hline
        {\tt detector\_type} & \Sexpr{sps$detector_type} \\
        {\tt descriptor\_type} & \Sexpr{sps$descriptor_type} \\
        {\tt extractor\_type} & \Sexpr{sps$extractor_type} \\
        {\tt n\_features} & \Sexpr{sps$n_features} \\
        {\tt octaves} & \Sexpr{sps$octaves} \\
        {\tt scale\_factor} & \Sexpr{sps$scale_factor} \\
    \end{tabular}
    \caption[The selected \clutseg parameters for feature extraction]{The
        selected \clutseg parameters for feature extraction.}
    \label{table:selected-parameters-features}
  \end{center}
\end{table}

\refTable{table:sps-match-guess} shows parameter values for the
detection and refinement stages. We used Locality Sensitive Hashing in both
stages to match the binary features produced by Oriented BRIEF.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{ll}
        Parameter & Value \\
        \hline
        {\tt detect\_matcher\_type} & \Sexpr{sps$detect_matcher_type} \\
        {\tt detect\_knn} & \Sexpr{sps$detect_knn} \\
        {\tt detect\_do\_ratio\_test} & \Sexpr{sps$detect_do_ratio_test} \\
        {\tt detect\_ratio\_threshold} & \Sexpr{sps$detect_ratio_threshold} \\
        {\tt detect\_ransac\_iterations\_count} & \Sexpr{sps$detect_ransac_iterations_count} \\
        {\tt detect\_max\_projection\_error} & \Sexpr{sps$detect_max_projection_error} \\
        {\tt detect\_min\_inliers\_count} & \Sexpr{sps$detect_min_inliers_count} \\
        {\tt refine\_matcher\_type} & \Sexpr{sps$refine_matcher_type} \\
        {\tt refine\_knn} & \Sexpr{sps$refine_knn} \\
        {\tt refine\_ransac\_iterations\_count} & \Sexpr{sps$refine_ransac_iterations_count} \\
        {\tt refine\_max\_projection\_error} & \Sexpr{sps$refine_max_projection_error} \\
        {\tt refine\_min\_inliers\_count} & \Sexpr{sps$refine_min_inliers_count} \\
        {\tt accept\_threshold} & \Sexpr{sps$accept_threshold}
    \end{tabular}
    \caption[The selected \clutseg parameters for feature matching and pose
            estimation]{The selected \clutseg parameters for feature matching and pose
            estimation.}
    \label{table:sps-match-guess}
  \end{center}
\end{table}

\section{Performance}

The performance of the \clutseg system has been tested on the validation set, on a
separate test set, and in a live test on the PR2 robot. This section reviews
\clutseg's performance for a parameter set that worked well on the validation
set. 
% It also covers observations made on a broad range of experiments.

\subsection{Performance on Validation Set}

<<>>=
    sps_avg = dbGetQuery(con, paste(
          "select e.id as id,",
          "avg(r.succ_rate) as avg_succ_rate,",
          "avg(r.value) as avg_value,",
          "avg(r.none_rate) as avg_none_rate,",
          "avg(r.mislabel_rate) as avg_mislabel_rate,",
          "avg(r.avg_succ_trans_err) as avg_succ_trans_err,",
          "avg(r.avg_succ_angle_err) as avg_succ_angle_err,",
          "avg(r.avg_trans_err) as avg_trans_err,",
          "avg(r.avg_angle_err) as avg_angle_err,",
          "avg(r.avg_detect_matches) as avg_detect_matches,",
          "avg(r.avg_detect_inliers) as avg_detect_inliers,",
          "avg(r.avg_detect_choice_inliers) as avg_detect_choice_inliers,",
          "avg(r.avg_refine_matches) as avg_refine_matches,",
          "avg(r.avg_refine_inliers) as avg_refine_inliers,",
          "avg(r.avg_refine_choice_inliers) as avg_refine_choice_inliers",
          # joins 
          "from experiment e",
          "join response r on e.response_id = r.id",
          "and (e.batch='run-16')"))

    avg_fail_angle_err = function(s) {
        return (s$avg_angle_err - s$avg_succ_rate * s$avg_succ_angle_err) / (1 - s$avg_succ_rate)
    }

    avg_fail_trans_err = function(s) {
        (s$avg_trans_err - s$avg_succ_rate * s$avg_succ_trans_err) / (1 - s$avg_succ_rate)
    }
@

For \clutseg uses randomized algorithms, we have run it with the SPS
values for 20 times on the validation set to measure the average performance.
The system correctly recognized an object within error bounds of 3 cm and 20
degrees in rotation in
\Sexpr{signif(sps_avg$avg_succ_rate * 100, 2)}\% of the validation scenes. One
of the scenes where \clutseg successfully recognized an object is visualized in
\refFigure{figure:validation-scene-success}, which shows the ground truth with
labeled axes, the estimated pose, and the keypoints of query features that
found both a match with a model feature and support the pose estimate.

The guesses that remained within the afore-mentioned error bounds showed an average
error of \Sexpr{signif(100 * sps_avg$avg_succ_trans_err, 2)} cm in translation,
and \Sexpr{signif(sps_avg$avg_succ_angle_err * 180 / pi, 1)} degrees in
rotation. When only considering the guesses which exceeded the error bounds,
the average translational error was \Sexpr{signif(100 * avg_fail_trans_err(sps_avg), 2)} cm,
and the average rotational error was \Sexpr{signif(180 / pi * avg_fail_angle_err(sps_avg), 1)} degrees.
\clutseg achieved an average guess score of
\Sexpr{signif(sps_avg$avg_value, 2)}.
<<>>=
    if (sps_avg$avg_none_rate > 0) {
        m1 = sprintf("No guess was made in %.1f percent of the scenes.")
    } else {
        m1 = sprintf("A guess was made in every scene.")
    }
    if (sps_avg$avg_mislabel_rate > 0) {
        m2 = sprintf("An object was mistakenly believed to be in the scene in %.1f percent of all scenes.")
    } else {
        m2 = sprintf("Objects were not confused in any of the scenes.")
    }
@
\Sexpr{m1}
\Sexpr{m2}
<<>>=
    rm(m1)
    rm(m2)
@

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=\textwidth]{media/validation-scene-success}
        \caption[A validation scene where \icedtea is recognized]{From left to right: (a) A validation scene; (b) a visualization of the estimated pose for \icedtea.}
        \label{figure:validation-scene-success}
    \end{center}
\end{figure}

<<>>=
    e_orb = dbGetQuery(con, paste(
        "select distinct avg_keypoints",
        "from response r",
        "join experiment e on r.id = e.response_id",
        "join paramset p on p.id = e.paramset_id",
        "join pms_fe f on f.id = p.recog_pms_fe_id",
        "where f.extractor_type='ORB'",
        "and f.detector_type='ORB'",
        "and f.descriptor_type='ORB'",
        "and f.n_features=5000",
        "and e.id=", sps_id,
        "and e.test_set='ias_kinect_test_grounded_21'"))
@

On average, \Sexpr{round(e_orb$avg_keypoints)} keypoints have been extracted
from a validation scene. In the detection stage, where \tod generates a set of
initial guesses (\refSection{subsection:recognition}), LSH found
\Sexpr{round(sps_avg$avg_detect_matches)} correspondences on average. At least
\Sexpr{round(e_orb$avg_keypoints - sps_avg$avg_detect_matches)} query features
remained unmatched. On average, an initial guess was supported by
\Sexpr{round(sps_avg$avg_detect_inliers)} correspondences.

In refinement, LSH found \Sexpr{round(sps_avg$avg_refine_matches)}
correspondences on average, all of them matching a single object. The average
guess in refinement was supported by \Sexpr{round(sps_avg$avg_refine_inliers)}
correspondences. The average pose estimate returned by \clutseg had a
confidence value of \Sexpr{round(sps_avg$avg_refine_choice_inliers)}.

The initial guess that was selected for refinement had a support of
\Sexpr{round(sps_avg$avg_detect_choice_inliers)} correspondences.  The
confidence values in the detection stage and the refinement stage cannot be
compared because which of the correspondences are designated inlier depends on
the {\tt refine\_max\_projection\_error} parameter.

<<>>=
    rm(e_orb)
@

The success rate of \Sexpr{round(100 * sps_avg$avg_succ_rate)}\% is over-optimistic with regard to the test scenes, because the
validation set was used for optimizing the system. For ground truth was
available for the validation set, more precise data can be presented on it than
for the test set. 

\subsection{Performance on Test Set}
\subsection{Performance in Live Test}

\subsection{Oriented BRIEF}

This subsection discusses results of experiments with Oriented BRIEF. It covers
a benchmark that compares computation speed of ORB, SIFT, and SURF. It shows
how ORB responds to the number of desired features, a parameter that cannot be
found in many other feature detectors.

\subsubsection{Computational Speed}

We first compared SIFT, SURF and ORB in terms of computational speed and the
quantity of features produced. We used the default parameters from \opencv (SVN
revision 5465), except for ORB, where the number of desired features was set to
1500.

<<>>=
    d=as.data.frame(read.table("../media/sift-surf-orb-benchmark.txt", header=TRUE))
    sift = which(d$extractor == "SIFT")
    surf = which(d$extractor == "SURF")
    orb = which(d$extractor == "ORB")
@

\refFigure{figure:sift-surf-orb-benchmark} shows the CPU time per image, the
CPU time per keypoint, and the number of keypoints, averaged over 21 images
from the validation set. None of the three detectors is parallelized, and the
measured CPU time was roughly equivalent to wall-clock time. The hardware and
software configuration of the notebook used for the benchmark is shown in
\refTable{table:hardsoftware-sift-surf-orb-benchmark}.

Oriented BRIEF ran about
\Sexpr{floor(d$img_cpu[sift]/d$img_cpu[orb])} times faster than SIFT, and about
\Sexpr{floor(d$img_cpu[surf]/d$img_cpu[orb])} times faster than SURF.  Because
ORB required only \Sexpr{d$img_cpu[orb]} milliseconds for computing keypoints and
descriptors for one image, the amount of time \clutseg spends in computing
query features becomes negligible compared to the time spent in matching and
pose estimation.

\begin{figure}
    \begin{center}
<<>>=
    png("sift-surf-orb-benchmark.png", 800, 400)
    layout(matrix(c(1, 2, 3), 1, 3))

    col = c("orange", "darkgreen", "blue", "lightblue") 
    par(cex=1.2)
    par(cex.main=0.9)

    barplot(d$img_cpu, names.arg=d$extractor, main="CPU Time per image",    ylab="time [ms]",  ylim=c(0, 800), col=col)
    barplot(d$kpt_cpu, names.arg=d$extractor, main="CPU Time per feature", ylab="time [ms]", ylim=c(0, 0.5), col=col)
    barplot(d$quantity, names.arg=d$extractor, main="Features per image", ylab="quantity", ylim=c(0, 2000), col=col)
    invisible(dev.off())
@
     \includegraphics[width=\textwidth]{sift-surf-orb-benchmark}
     \caption[A comparison of SIFT, SURF and ORB in terms of speed]
    {Comparison of SIFT, SURF and ORB in terms of speed and number of features.}
     \label{figure:sift-surf-orb-benchmark}
    \end{center}
\end{figure}

<<>>=
    rm(col)
    rm(d)
    rm(sift)
    rm(surf)
    rm(orb)
@

\subsubsection{Feature Quantity}

In this work, it has been necessary to control the number of features selected
by a feature detector.  In general, the number of features selected by a
feature detector should be configurable, using a simple and intuitive threshold
\cite{Tuytelaars2007}.

The parameter {\tt n\_features} specifies the number of desired features
produced by Oriented BRIEF. Other feature detectors and descriptors in \opencv,
such as SIFT and SURF do not provide such a parameter. Neither do the feature
detectors FAST and STAR. The parameter {\it n\_features} is simple and
intuitive. Yet, ORB does not make any guarantee about the actual number of
features extracted on the image. The parameter is just a hint. The question
arises how well ORB responds to different numbers of desired features.

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{media/orb-response-test-images}
        \caption[Four test images for Oriented BRIEF]{From left to right: (a)
            \assamTea, (b) {\it clutter}, (c) {\it house}, and (d) {\it mandrill}.}
            \label{figure:orb-response-test-images}
    \end{center}
\end{figure}

\begin{figure}

    \begin{center}
<<>>=
    png("orb-response-n-features.png", 500, 400)

    col = c("magenta", "darkgreen", "blue", "red") 
    par(cex=1.2)
    par(cex.main=0.9)

    r = read.table("../media/orb-response-n-features.txt", header=TRUE)

    assam_tea = r[names(r) == "assam_tea"]
    clutter = r[names(r) == "clutter"]
    house = r[names(r) == "house"]
    mandrill = r[names(r) == "mandrill"]

    plot(r[[1]], r[[2]],
         main="ORB response on desired number of features",
         xlab="n_features",
         ylab="actual number of features",
         type="o", pch=21,
         xlim=c(0, 4000), ylim=c(0, 2500), lty=2, col=col[1], asp=1)
    lines(r[[1]],  r[[3]], type="o", pch=22, col=col[2], lty=2)
    lines(r[[1]], r[[4]], type="o", pch=23, col=col[3], lty=2)
    lines(r[[1]], r[[5]], type="o", pch=24, col=col[4], lty=2)
    legend(3000, 2500, names(r)[2:5], pch=21:24, col=col)
    invisible(dev.off())
@
     \includegraphics[width=0.65\textwidth]{orb-response-n-features}
     \caption[The response of Oriented BRIEF on the number of desired features]
         {The response of Oriented BRIEF on the number of desired features.}
     \label{figure:orb-response-n-features}
    \end{center}
\end{figure}

\refFigure{figure:orb-response-n-features} plots the number of desired features
against the number of features that were actually obtained by ORB. The four
test images {\it assam\_tea}, {\it clutter}, {\it house}, and {\it mandrill}
(\refFigure{figure:orb-response-test-images}) carry different amount of
informations, though all were sampled at the same resolution of 512x512 pixels.
Increasing values for {\tt n\_features} have been tried until the number of
actual keypoints reaches its maximum for all four images.

{\it Mandrill} has fine-grained texture, and ORB extracted maximally
\Sexpr{max(mandrill)} keypoints on this image. This number is greater than for
image {\it house}, where only \Sexpr{max(house)} features were chosen for all
values of {\tt n\_features} greater or equal than \Sexpr{r[[1]][min(which(house == max(house)))]}.

<<>>=
   max_exceed = -min(min(r[[1]] - assam_tea),
          min(r[[1]] - clutter),
          min(r[[1]] - house),
          min(r[[1]] - mandrill))
@

The actual number was never observed to exceed the desired number of keypoints
by more than \Sexpr{max_exceed} keypoints.

<<>>=
    rm(r)
    rm(max_exceed)
@

\section{Issues}

% \subsection{Prior Assumption about Orientation}

% many presentations of object recognition systems seem to make the prior assumption of objects appearing in upright pose.
% the performance on the test set which showed objects in arbitrary orientation was below expectations
% the model features were extracted by a scan of the object from positions on a circle
% the query features are observed from a position on a sphere around the object
% features are rotation-invariant only to rotation of objects around an axis parallel to the optical axis 
% the number of matches significantly reduced on the images with objects in arbitrary orientation

% Many presentations of object recognition systems in media and literature seem
% to make the prior assumption that objects in query scenes are oriented in
% upright position. Our test set shows scenes with objects not only in clutter,
% but also in arbitrary orientation.  This adds extra difficulty. \clutseg had
% problems with locating an object at least in TODO of TODO test scenes. When we
% collected raw data for the modelbase, we made an implicit prior assumption
% about the orientation of objects in query scenes. We scanned the template
% object from several points on a circle around the object. Yet, the query scenes
% show instances from arbitrary points on spheres around them.

% Rotation-invariant feature detectors cope with 3D rotation of objects around an
% axis parallel to the optical axis. SIFT features still operate up to 20 degrees
% of rotation around an axis parallel to the image plane \cite{Lowe1999}. % TODO: confirm this


\subsection{Repetitions in texture}

The system had difficulties to recognize \icedtea when shown from a
certain angle.  It turned out that \icedtea has smallprint almost all over
its backside. The same set of letters repeatably appear over a large area,
which probably makes it hard to obtain informative features.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.2\textwidth]{media/icedtea_smallprint}
        \caption[Repetitive texture shown on the back of \icedtea]{Repetitive texture on template object \icedtea.}
    \end{center}
\end{figure}

\subsection{Consensus for Distant Pose Estimates}

Generally, the system relies on the size of the consensus set to be a strong
indicator for the goodness of an estimate. We observed that in one experiment
and scene, \clutseg estimated a distance of 88 metres between \icedtea and the
camera (\refFigure{figure:distant-pose-estimate}). This bad estimate was almost
two orders of magnitude off the ground truth.  Yet it still had support by 14
inliers --- enough in this scene to make it the best-ranked guess.

An explanation for the large consensus set is that the projection error is a
measure on the image plane. Given a scene, consider any correspondence $c_i =
(q_i, p_i) \in \real^2 \times \real^3 $ between a query feature with
keypoint $q_i$ and a model feature with 3D point $p_i$. Let $\oTvh$ be a
pose estimate. Let $U: \real^3 \to \real^2$ denote perspective
projection for the calibrated camera. Define the projection $\hat{q}_i$ of the
model point $p_i$, aligned to the scene by $\oTvh$, as

\begin{equation}
    \hat{q}_i := U\pth{ \ \oTvh \pth{ p_i}\ }
\end{equation}

The correspondence $c_i$ is designated inlier if and only if the L2-distance
between query keypoint and the projection of the model point aligned to the
scene is less than a threshold $r$, precisely 

\begin{equation}
    c_i\ \text{ is an inlier w.r.t. }\ \oTvh\quad \Leftrightarrow \quad \normtwo{\ q_i - \hat{q}_i\ } \le r  \quad \wedge \quad \text{$p_i$ is visible}
    \label{equation:inlier-definition}
\end{equation}

Now, let us model the scenario, where a distant pose estimate was generated
for an object that is close to the camera in truth. Let $\oTv$ denote
ground truth for $\oTvh$. 
 
\begin{equation}
    e_t \pth{ \oTvh,\ \oTv } \geq l
\end{equation}

Choose $l$ large enough that the projection of the aligned model spans only one
pixel $\hat{q}$ on the digital image plane
(\refFigure{figure:far-away-aligned-model}). For the $n$ correspondences
of this object, this means

\begin{equation}
    \forall{i \in \brt{n}}: \hat{q}_i \approx \hat{q}
    \label{equation:same-point}
\end{equation} 

Plugging \refEquation{equation:same-point} into \refEquation{equation:inlier-definition},
we obtain an inlier criteria for the bad estimate:
\begin{equation}
    \forall{i \in \brt{n}}:\quad c_i\ \text{ is an inlier w.r.t. }\ \oTvh\quad \Leftrightarrow \quad \normtwo{\ q_i - \hat{q}\ } \le r \quad \wedge \quad \text{$p_i$ is visible}
\end{equation}

All correspondences within a circle of radius $r$ around $\hat{q}$ are
designated inliers. If this circle contains many keypoints on the query image,
the resulting consensus set of the pose estimate might grow larger than
expected. The effect can be neglected if radius $r$ is small enough. In the
experiment that exhibited the phenomenon, the radius was set to $r = 12$
(pixels). Note that this radius is equivalent to the parameter {\tt
max\_projection\_error}.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.55\textwidth]{media/far-away-aligned-model}
            \caption[A schematic explanation of a far-away aligned model with many
inliers]{An example, where all points of a far-away aligned model are projected to the same point
($\triangle = \hat{q}$) on the image plane. All four correspondences $c_1$, $c_2$,
$c_3$, and $c_4$ are designated inliers because all the associated query keypoints ($\times$)
fall into the circle around point $\hat{p}$.}
\label{figure:far-away-aligned-model}
    \end{center}
\end{figure}

There are at least three hypothetical solutions to this issue. One might set
the parameter {\tt max\_projection\_error} to a lower value, which is what we have
done. The bad pose estimates could be pruned away by the a-priori assumption
that objects are within a certain range. Finally, the radius $r$ could be replaced
by a function that depends on the estimated distance
$\normtwo{\oTvh\pth{\vec{0}}}$.

% Proof is missing:
% A final note; let us assume that, as the only source of noise, we have Gaussian
% noise added to the 3D model points. Standard deviation then corresponds to
% spheres around the model points. Spheres appear as ellipses under perspective
% projection \cite{Moore1989}, and approximating an ellipse by a sphere seems
% reasonable.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.4\textwidth]{media/ransac-max-projection-error-deficiency-sample}
        \caption[An example scene with a far-away aligned model]{On a test scene, the badly aligned pose still results in many inliers.}
        \label{figure:distant-pose-estimate}
    \end{center}
\end{figure}

\subsection{Incomplete Models}

When recording data for a modelbase, the bottom of each template object is
facing the rotating table, and is thus invisible to the camera from all
viewpoints.  This does not matter as long as instances of these template
objects are standing upright. This does matter however, when instances occur in
arbitrary orientation.

\subsection{Systematic Error in Models}
\label{subsection:systematic-error-model}

Unfortunately, analysis of the 3D models used in the evaluation revealed that
the models have been corrupted by a systematic error. The 3D point clouds that
make up the 3D models do not meet our expectations. We compared our models with
the \tod models of \campbellsChickenNoodle, \downy, \fatFreeMilk, and
\goodEarthTea from the \tod tutorials
\footnote{http://vault.willowgarage.com/wgdata1/vol1/tod\_kinect\_bags/training
$\,$. Accessed 5 July 2011.}. Whilst \downy's model seems valid when projected
onto a plane (\refFigure{figure:systematic-error-model}~a), \icedtea's model
does not (\refFigure{figure:systematic-error-model}~b-c).

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{media/systematic-error-model}
        \caption[The 3D models of \downy and \icedtea extracted from multiple views]{From left to right: (a) Correct model of {\it downy}; (b) side view of {\icedtea} model; (c) top view of {\icedtea} model.}
        \label{figure:systematic-error-model}
    \end{center}
\end{figure}

\refFigure{figure:clutseg-model-xyhist} shows the {\it $xy$-coordinate histograms} of
the $x,y$-coordinates of the model points. These histograms were computed by
projecting the model points orthogonally onto the $xy$-plane, counting the
number of points that fall into the same bin of 320x320 bins (pixels) in total.
Outliers are not included in the histogram. Both object coordinate axes are
drawn onto the histograms. The histogram clearly shows the erroneous models,
especially in the case of \icedtea's; its $xy$-coordinate histogram has roughly the
form of a hash ('\#'). 

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=\textwidth]{media/clutseg-model-xyhist}
        \caption[The coordinate histograms of four \clutseg models]{From left to right: $xy$-coordinate histogram of (a) \assamTea, (b) \haltbareMilch, (c) \jacobsCoffee, (d) \icedtea.}
        \label{figure:clutseg-model-xyhist}
    \end{center}
\end{figure}

In comparison, the \tod model histograms look as expected
(\refFigure{figure:tod-model-xyhist}). Even the texture-less lid on the top of
\fatFreeMilk can be made out on the histogram
(\refFigure{figure:tod-model-xyhist}~c).  Just like \icedtea, \fatFreeMilk is a
cuboid. Hence, their $xy$-coordinate histograms should be similar, which is not the
case.
\begin{figure}[h]
    \begin{center}
        \includegraphics[width=\textwidth]{media/tod-model-xyhist}
        \caption[The coordinate histograms of four \tod models]{From left to right: $xy$-coordinate histogram of (a) {\it campbells\_chicken\_noodle}, (b) {\it downy}, (c) {\it fat\_free\_milk}, (d) {\it good\_earth\_tea}.}
        \label{figure:tod-model-xyhist}
    \end{center}
\end{figure}

Unfortunately, the source for the systematic error is yet unknown, although the
odds are that the issue is related to camera calibration and/or the accuracy of
the depth image.
