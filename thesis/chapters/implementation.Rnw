This chapter first covers the implementation of existing libraries from the
{\it Robot Operating System}. On top of these libraries, we implemented a
object recognition system, called the \clutseg system. This
chapter covers its design and the improvements we added to the existing
libraries. It finally explains how system parameters were found with the help
of a semi-automatic experiment runner, and how \clutseg can be
evaluated with regard to \refChapter{chapter:evaluation}.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.4\textwidth]{media/clutseg-dependency-stack}
        \caption[\clutseg software dependencies]{\clutseg depends on \tod, ROS, PCL and OpenCV}
    \end{center}
\end{figure}

\section{Textured Object Detection Library}

This section analyzes the third-party software underlying the \clutseg system,
and explains how it was modified and improved to better fulfill the
requirements.  It describes how to extract models of template objects from
multiple views, and covers the elementary pipeline that later recognizes
instances in query scenes.

The {\it Robot Operating System} (ROS) is a framework for the development of
robot applications, having its roots in the STAIR project at Stanford
University. It contains the third-party packages {\it tod}, {\it tod\_training}
and {\it tod\_detecting}. In the following, the system formed by these three
ROS packages is called \tod for easier reference. Throughout this work, \tod
has been used in SVN revision 50479, and modifications are explicitly mentioned
in the following. At this revision, \tod was experimental, unstable, and not
covered by automatic tests. The system participated in the {\it Solutions in
Perception Challenge} at ICRA 2011 in Shanghai for comparison.

\tod was developed to be used together with a Microsoft Kinect RGB-D camera.
Recognition is solely based on intensity and depth information, so color
information is not required. \tod offers two modes. One targets the Kinect
RGB-D camera.  The other mode was supposed to work better with a Prosilica MP 5
camera. The latter clusters correspondences prior to pose estimation, the
former does not. \clutseg uses \tod in the Kinect mode.

\subsection{Learning}
\label{subsection:learning}

In general, an object recognition system for needs models of the objects it
shall be able to recognize. \tod learns these descriptions by extracting local
features from multiple views of the template object. \tod extracts {\it model
features} from the template object, and {\it query features} from the query
scene. These features are asymmetric in nature.

\subsubsection{Model Features}

Given a single view, let $\vec{p}$ be a 3D point of a template object in object
coordinates. Let $\vec{p'}$ be its projection on the image plane, and $\vec{d}$
the descriptor vector generated by for $\vec{p'}$. Then we call $(\vec{p},
\vec{d})$ a {\it model feature}. Hence, such a model feature is more than a
mere local 2D feature, but less than a true local 3D feature that would include
true 3D information such as normal orientation. The model of a template object
in \tod is a set of model features. \footnote{A model is represented by a set
of instances of struct {\tt Features3d}.}

\subsubsection{Model Extraction}

\tod offers a toolchain that extracts model features from a template object on
a rotating table. The template object is either manually or automatically
rotated, and a depth camera takes images from multiple views. 
% TODO: RGB is not required!
The depth image are the basic unit processed in a pipeline that comprises three
stages for each view.  First, the computation of the object-view
transformation.  Second, the segmentation of the region of interest that shows
the object on the image. Third, the extraction of local 2D features and the
construction of model features, confined to the region of interest.

\subsubsection{Computation of object-view transformation}

First, the object-view transformation needs to be computed because the 3D
points in the depth images are given in view coordinates but need to be
transformed to object coordinates when defining model features. For this, \tod
uses a camera calibration technique from OpenCV based on two chessboard
fiducial markers that are glued to the rotating table on opposite sides.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{media/fiducial-object}
        \caption{Relation between fiducial markers and object coordinate system}
    \end{center}
\end{figure}

Note that there is a subtlety in the relationship of the fiducial markers and
the object coordinate system. In fact, the camera calibration computes the PRT
$\fTv$ between the {\it fiducial coordinate system} attached to the fiducial
markers and the view coordinate system. The origin in fiducial coordinates is
defined midway between the two chessboards. Yet, we need the object-view
transformation $\oTv$. The solution here is to just {\it define} the object
coordinate system to be the same as the coordinate system attached to the
fiducial markers, that is

\begin{equation}
    \oTv: \real^3 \to \real^3, \vec{p} \mapsto\ \fTv(\vec{p})
    \label{equation:fiducial-object}
\end{equation}

% TODO: use 2D instead of 2D, and 3D instead of 3D

Computation of $\fTv$ is yet another instance of the PnP problem. The 2D-3D
correspondences are obtained from detecting the chessboard corners in the
image.

As long as one of these chessboards is detected, the transformation can be
recovered. This robustness proves useful when the template object occludes the
chessboard further away from the camera. As the chessboards were sometimes not
detected on the image at all, we improved reliability using image processing
techniques. We observed that chessboard detection worked on dithered binary
images in all 23 cases where it failed on the original grayscale images, and
hence used them as a fallback. Several other image processing techniques have
been tested on 23 images where the chessboard was not detected. 

\begin{table}[h!]
    \begin{tabular}{llrr}
        Pre-processing method & Imagemagick command & Success rate \\
        \hline
        Dithering to Binary & {\tt convert -monochrome \$1 \$2} & 23/23 \\
        Color Reduction & {\tt convert -colors 2 \$1 \$2} & 23/23 \\
        Local Adaptive Thresh. & {\tt convert \$1 -lat 25x25 -threshold 50\% \$2} & 23/23 \\
        Thresholding & {\tt convert \$1 -threshold 50\% \$2} & 19/23 \\
        Sharpening & {\tt convert -sharpen 0x12 \$1 \$2} & 2/23 \\
        Closing & {\tt convert -morphology Close Disk \$1 \$2} & 1/23 \\
        Opening & {\tt convert -morphology Open Disk \$1 \$2} & 0/23 \\
        Median Filtering & {\tt convert -median 2 \$1 \$2} & 0/23 \\
    \end{tabular}
    \caption[Image pre-processing operations for camera calibration]{Success rates of chessboard detection after image processing
        operations on 23 images where the detection originally failed.}
\end{table}

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=\textwidth]{media/pose-pre-process}
        \caption[Pre-processed images for camera calibration]{From left to right: (1)
Original image where pose estimation failed; after (2) Dithering to Binary;
after (3) Color Reduction; after (4) Local Adaptive Thresholding. On images 2,
3, and 4, pose was estimated successfully.}
    \end{center}
\end{figure}

\subsubsection{Segmentation of the Region of Interest}

The model includes only features that belong to the template object.  \tod
segments the 3D point cloud of the template object using a box-shaped
pass-through filter of a predefined size at the object origin.  Remember that
\tod uses the object-view transformation to find the object origin in camera
coordinates, precisely $\oTv \pth{\vec{0}}$. Then, a radius outlier search
removes all points with less than 20 neighbors within 2 cm. \tod projects the
segmented point cloud onto the image plane. The set of projected points form a
mask that is supposed to align as close as possible to the true region of
interest. Due to some bug in underlying software, the estimate for the region
of interest sometimes included parts of the chessboards next to the template
objects. These undesired parts in the mask were observed to be separate
connected components.  Hence, we fixed the mask by opening the image with a
disk of radius $10.3$, removing all but the largest connected component and
dilating the mask with a disk of $2.3$ to compensate for the opening step.

% probably should not include a picture from other database?
% \begin{figure}[h!]
%    \begin{center}
%        \includegraphics[width=0.75\textwidth]{media/mask-post-process}
%        \caption[Post-processed image mask]{
%            Mask before and after correction. The latter is close
%            to the true region of interest.}
%        \end{center}
% \end{figure}

\subsubsection{Extraction of Model Features}

The final step in learning requires to construct the set of model features from
the depth image. Local 2D features are extracted using any one of the feature
extractors provided in OpenCV. Each of these features is described by a
keypoint $\vec{p'}$ on the image plane and a descriptor vector $\vec{d}$.
Assuming a calibrated camera, each keypoint $\vec{p'}$ uniquely corresponds to
a 3D point $\vec{p}$ in the scene. Applying the inverse object-view
transformation, a model feature $(\vTo \pth{\vec{p}}, \vec{d})$ is obtained.

Actually, \tod applies the inverse object-view transformation right before
calling RANSAC for pose estimation, but it is equivalent, conceptually simpler
and computationally faster to do this only once while extracting features.

\subsubsection{Modelbase}

\tod stores all knowledge about the template objects in a {\it modelbase}.
Each model is a set of model features, unified from several views. \tod uses a
simple heuristic to select which model features are incorporated in a model, it
selects the model features from views in roughly equal angle steps with respect
to the rotating table. \tod stores the model base in a hierarchical manner on
the filesystem, retaining data from views that are not actually included in the
models.
% TODO: refer to paper that select view based on informativeness, information content, entropy
\footnote{The model base is represented by struct {\tt TrainingBase} in \tod.}

\subsection{Recognition}
\label{subsection:recognition}

This section shows how \tod can be used to recognize instances of template
objects in query scenes, making use of the models learned in the previous
section. The recognition process in \tod can be split into three parts. First,
the extraction of query features. Second, the matching of query features and
model features to obtain a set of correspondences between query image and
models. Third, the estimation of object poses based on this set of
correspondences.

\subsubsection{Query Features}

% TODO: dot after or before footnote? concerns the whole document
% TODO: footnotes allowed? concerns the whole document
% TODO: remove footnote
Let $\vec{p}$ be a 2D point on a query image, and $\vec{d}$ its descriptor
vector. Then we call $(\vec{p}, \vec{d})$ a {\it query feature}. \footnote{The
query features for a specific scene are represented by an instance of struct
{\tt Features2d}.} Hence, such a query feature is nothing else than a
keypoint-descriptor pair produced by SIFT, SURF or ORB, or any other feature
extractor, and we introduce this term to clearly tell features from the query
and the model apart. Note that query features do not include 3D information.
This is subject of discussion, and is addressed in \refChapter{chapter:evaluation}.

% TODO: consistency chapter 4 and Chapter 4, use capital letter

\subsubsection{Extraction of Query Features}

Similarly to the extraction of model features, \tod uses one of the OpenCV
feature extractors to find a set of query features on the image. It is
reasonable to select the same feature extractor as in the model extraction.
Both query feature and model feature extraction can be configured, so \tod
allows to try different feature detectors with configurable parameters. 
In fact, \tod also provides rBRIEF, an experimental modified version of BRIEF,
which we used in some experiments but then replaced it in favor of Oriented BRIEF
from OpenCV.

\subsubsection{Feature Matching}
\label{subsubsection:tod-feature-matching}

Since the image is not segmented, these query features emanate from different
textured objects or textured background. Matching these features shall find
correspondences between these query features and model features. Let $C_i$ be
the set of correspondences where a query feature has been matched with a model
feature from the $i$-th template object. Finally, let $C = (C_1, \dots, C_n)$
denote the partition of all correspondences for the query scene, where $n$ is
the number of template objects described in the model base.  \tod integrates
different nearest-neighbor algorithms. There is FLANN, which we have not
tested.  \tod offers brute-force matching algorithms. We used Locally Sensitive
Hashing in \tod for matching.

% TODO: introduce parameter knn
% TODO: discuss ratio test from lowe
% TODO: introduce parameter do_ratio_test
% TODO: introduce parameter ratio_threshold

\subsubsection{Guess}

The hypotheses \tod generates for a given query scene are called {\it guesses}.
A guess is a triple $(i, \hat{T},\ S)$, where $i$ denotes the template object
which \tod believes to see on the scene, $\hat{T}$ the estimated pose of the
object in terms of a object-view transformation and $S \subseteq C_i$ the
consensus set of inliers. 

\subsubsection{Pose Estimation}
\label{subsubsection:tod-pose-estimation}

Given the correspondences $C$, \tod generates a set of guesses. The
correspondences are treated separately for each template object, that is \tod
looks at each correspondence set $C_i$. For the $i$-th template object, RANSAC
is called iteratively. If RANSAC returns with a pose estimate $\hat{T}_{ij}$ in
the $j$-th iteration and the size of the consensus set $S_{ij}$ is greater or
equal to a parameter {\tt min\_inliers\_count}, then \tod infers that there is
an instance of the template object at the estimated pose and generates a guess
$(i, \hat{T}_{ij}, S_{ij})$. Note that the inlier sets $S_{ij}$ are disjoint
because the RANSAC call in the $j$-th iteration is told to operate only on the
set $C_i - \bigcup_{k=1}^{j-1} S_{ik}$, and $C$ forms a partition. If not, then
iteration is stopped, and \tod continues with the correspondence set for the
next template object, if any. This means that the inlier sets of two different
guesses never share a correspondence.

ROS package {\tt posest} in stack {\tt vslam} contains an implementation of
RANSAC for the PNP problem, which is used by \tod. It is roughly similar to the
RANSAC/LD algorithm presented in \cite{Fischler1981}. It solves the P4P
problem.  The largest consensus set found is fed to the PNP-solver in OpenCV
({\tt cv::solvePnP}), which then computes the final pose estimate.

% TODO: introduce parameter max_projection_error

Altogether, for a given query scene, \tod takes the query image and generates a
set of guesses, which is the output of the \tod recognition process.
% The task is rendered difficult by possible confusion in the partition $C$ with respect
% to the true partition, that is some correspondences might have been matched to
% the wrong object. \tod iterates over each correspondence set $C_i$, and
% generates pose estimates for instances of the $i$-th template object.

\section{Clutter Segmentation Tool}

This section discusses the implementation of the \clutseg object recognition
system on top of \tod. We introduce three core concepts that shape \clutseg for
the specific application requirements of locating the next object to resolve
from the clutter. This means that only object in the scene needs to be located.
First, a guess ranking helps to discard guesses with low confidence. Second,
guess refinement aims at spending more computation resources on specifically
improving pose estimates for guesses with high confidence. Third, guess
rejection attenuates the probability of settling on the wrong guess.  Finally,
this section covers the tools that help to find parameters for the clutseg
system and help collect statistics to evaluate its performance.

The reasons why \clutseg was developed were three-fold. First, given the
robotic task of resolving the clutter in the scene requires the robot to locate
one object in the scene. \tod produces several guesses, and to select a
suitable one is within the scope of this work. Second, the performance of \tod
was expected to be improved by building a system that more specifically
targeted at the task at hand. Third, when working with experimental, unstable
third-party software, it often proves useful to wrap certain parts of it with a
more convenient interface.

\subsection{Guess Ranking}

At the core of \clutseg is the idea that we can have more confidence in a
guess, when it is supported by a large consensus set of inliers. The discussion
whether this holds true is deferred to \refChapter{chapter:evaluation}. It
remains an assumption in this chapter that more inliers lead to higher
probability of the guess being correct. Measures for correctness are defined in
\refSection{subsection:measured-statistics}.

A {\it guess ranking} is a function $r$ that assigns a real-valued score to a
guess $(i, \hat{T}, S)$. Different rankings can be defined and plugged into
\clutseg, such as a ranking based on proximity to the camera or a combination
of multiple guess rankings. Yet, the ranking of interest is a guess ranking
$r_{S}$ that assigns scores to guesses according to the number of inliers:

\begin{equation}
    r_S: (i, \hat{T}, S) \mapsto \norm{S}
\end{equation}

The number of inliers is not only interesting because of the assumed correlation
with probability of being correct. It also simplifies the robotic task of grabbing
an object by supplying more 3D points.

\subsection{Guess Refinement}

Given a guess, the idea of {\it guess refinement} is to spend additional
computation resources to lessen the expected error in pose estimates. Guess
refinement as in \clutseg takes a guess $(i, \hat{T}, S)$, matches the query
features and against the model features of the $i$-th template object alone,
and estimates poses for this objects as described in
\refSection{subsubsection:tod-feature-matching}, using a model base consisting
only of this single object model. The best guess according to the chosen guess
ranking is designated the {\it refined guess}.

Guess refinement aims at finding more matches between query features and the
single object whose pose is to be refined. By ignoring other models only
correspondences between the query scene and one model are found. Such an
approach is optimistic, and possibly dangerous, especially in cases where the
guess to refine is a false positive.  The validity of such an approach has at
least to empirically verified by experimentation.

\subsection{Guess Rejection}

A technique denoted as {\it guess rejection} attenuates the chance of returning
an incorrect guess, especially trying to avoid false positives. \clutseg
rejects guesses whose guess ranking is smaller than a configurable parameter
{\tt accept\_threshold}. How guess rejection exactly works and in what scenarios
it is useful is explained in the following section.

\subsection{Recognition}

This section describes the full recognition process. It puts the ideas of
guess ranking, guess refinement and guess rejection into context.

Given a query scene, the \clutseg recognition process first calls \tod once to
produce a set of initial guesses $D$ for all objects in the model base. A
ranking defines a total order on $D$. The inlier ranking $r_S$ is used for this
by default. \clutseg chooses the first ranked guess in $G_D$ and computes the
refined guess. If the ranking of the refined guess is greater or equal than
{\tt accept\_threshold}, it is accepted and returned as the result of the
recognition process. In case the refined guess is rejected or no refined guess
was made at all, the algorithm proceeds to the next guess in $D$ and repeats
the refinement step. This iteration is performed until either a refined guess
is accepted, or \clutseg declares the scene to be empty.

This scheme was built with the intention to avoid the problem of finding a good
operating point for \tod in ROC space. It is sufficient that \tod returns a set
$D$ where the high-ranked guesses are true positives. For one of the
high-ranked guesses, there are two cases to consider. If the high-ranked guess
is a true positive, then the refinement step should either confirm this guess,
or improve on it, respectively. If the high-ranked guess is a false positive
against expectations, then the refinement step might still lead to a guess.
Expectations are that this refined guess has low ranking and will be rejected,
and other high-ranked guesses will be examined.

This two-staged recognition process requires the query features to be extracted
once.  Matching must be done at least twice. In the refinement step, there are
less features to match than in the initial step, the {\it detection step}.

\section{Parameter Optimization}
\label{section:parameter-optimization}. 

This section reviews the parameters of the \clutseg system, discusses their
nature and explains the approach that has been used to find a reasonable
parameter set. It introduces statistics that measure performance of the
\clutseg system on sets of query scenes where ground truth is available.

\subsection{Parameter Space}
\label{subsection:parameter-space}

The \clutseg system has a fair amount of parameters. Many of the parameters
have already been introduced in the previous chapters, others yet need to be
described. Table~\ref{table:parameters} shows an overview of all configurable
system parameters, except the parameters for feature extraction, which depend
on the choice of feature extractor, and are treated separately.
% TODO: create table with feature extraction parameters

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{llll}
        Parameter & Range \\
        \hline
        {\tt detect\_matcher\_type} & categoric \\
        {\tt detect\_knn} & integer \\
        {\tt detect\_do\_ratio\_test} & boolean \\
        {\tt detect\_ratio\_threshold} & real \\
        {\tt detect\_ransac\_iterations\_count} & integer \\
        {\tt detect\_max\_projection\_error} & real \\
        {\tt detect\_min\_inliers\_count} & integer \\
        {\tt refine\_matcher\_type} & categoric \\
        {\tt refine\_knn} & integer \\
        {\tt refine\_ransac\_iterations\_count} & integer \\
        {\tt refine\_max\_projection\_error} & real \\
        {\tt refine\_min\_inliers\_count} & integer \\
        {\tt accept\_threshold} &  real \\
    \end{tabular}
    \caption{\clutseg parameters for matching and pose estimation}
    \label{table:parameters}
  \end{center}
\end{table}

Even if we consider only ten of these parameters as factors to five levels
each, we obtain $5^{10} \approx 10^6$ different parameter sets. Consider also,
we would like to test two different matchers and three different feature
detectors. This would result in around $6 \cdot 10^6$ parameter sets. A full
factorial design for finding optimal parameters, as recommended in
\cite{Alpaydin2010} becomes difficult. The pragmatic approach chosen in this
work is based on three ideas. First, we aim at reducing the dimensions of
parameter space by priorly selecting parameter values by reasoning and the
experience of other researchers. Second, tools were developed together with
\clutseg that support visual inspection of experiment results to support an
intuitive analysis. Third, the exploration of parameter space is done
selectively, and parameter sets in promising regions are more thoroughly
investigated into. The danger of such an approach is obviously the chance to
reach a local optimum instead of the targeted global optimum, and the drawback
is that results have to be formulated with even more care since they are not
supported by a full grid of samples in parameter space.

\subsubsection{Feature Extraction Parameters}

Feature extraction parameters control the extraction of local 2D features.  A
choice needs to be made for the feature detector and the feature descriptor.
Feature detectors again need to be configured. The requirements described in
\refChapter{chapter:introduction} call for a feature detector that produces
features in good quantities, and it shall be reasonably fast. Hence, the main
focus was set on Oriented BRIEF, which takes three main parameters. The number
of levels in the scale-space pyramid is denoted by {\tt octaves}, the
magnification factor between two levels is called {\tt scale\_factor}.
Parameter {\tt n\_features} is a hint for Oriented BRIEF on how many features
are desired.

\subsubsection{Feature Matching Parameters}

The matching process in the detection step and the refinement step have some
parameters in common. The choice of matcher is denoted by {\tt
detect\_matcher\_type} ({\tt refine\_matcher\_type}), but partly dictated by
the prior choice of feature descriptor. A binary feature descriptor calls for a
matcher that is well-designed to work with binary vectors. Also, brute-force is
hardly a choice except for use as comparison to approximate nearest neighbor
search algorithms. The focus is on the Locally Sensitive Hashing algorithm
implemented in \tod. Parameter {\tt detect\_knn} ({\tt refine\_knn}) is a hint
to the matching algorithm about how many neighbors per query feature shall be
retrieved. The ratio test can be enabled or disabled by {\tt
detect\_do\_ratio\_test} and configured by {\tt detect\_ratio\_threshold}
for the detection step. In the refinement step it does not make sense and is
disabled by default. The default value for {\tt detect\_ratio\_threshold} is selected
to be $0.8$, as recommended in \cite{Lowe1999}.

% TODO: check whether correspondences or what kind of points are removed in tod
% guess generator after each call to ransac
% TODO: describe k-NN matching and what this means in this context
% TODO: describe pruning of correspondences that find neighbors in many images

\subsubsection{Pose Estimation Parameters}

The pose estimation step in detection and refinement depends on three
parameters each. RANSAC is controlled by {\tt
detect\_ransac\_iterations\_count} ({\tt refine\_ransac\_iterations\_count})
and {\tt detect\_max\_projection\_error} ({\tt
refine\_max\_projection\_error}). No further pose estimates for an object are
generated once RANSAC returns an estimate with a consensus set of size less
than {\tt detect\_min\_inliers\_count} ({\tt refine\_min\_inliers\_count}).

\subsection{Measured Statistics}
\label{subsection:measured-statistics}

\clutseg supports measuring its performance. This section explains how errors
and scores are computed for given query scenes or whole sets of query scenes
where ground truth is available. It covers statistics that are recorded in the
learning and the recognition process, which are aimed to simplify reasoning
about the system's performance and the influence of parameters. 

\subsubsection{Learning Statistics}

When constructing the models from multiple views, some simple statistics are
recorded. The model construction is part of \tod's codebase and it had to be
modified to keep trace of the minimum, maximum and average number of features
extracted on the views. Also, the number of times where the computation of the
object-view transformation failed due to undetected chessboards is tracked.
The time required for building the model base is recorded as well. Results are
presented in \refChapter{chapter:evaluation}.

\subsubsection{Ground Truth}

The error \clutseg makes on a query scene can only be computed if ground truth
is available. Ground truth data for a scene consists of set of labels that tell
which objects can be seen at what location and orientation in the scene. Thus,
ground truth $G$ can be described as a set of labels

\begin{equation}
    G = \cbr{ (i,\ \oTvi{i}) : \text{object $i$ is on scene at pose $\oTvi{}$} }
\end{equation}

% TODO: introduce abbreviation for object-view transformation
% TODO: object-view transformation or object-view transformation, consistent with object coordinates or model coordinates

\subsubsection{Classification Error}

\clutseg only attempts to partially estimate ground truth. It does not try to
label all objects in the scene, but only one. Mapping to ROC terminology is
not straight-forward. Ignoring the estimated pose, it is possible to put a guess
made by \clutseg into one of four categories. True positive if the guessed
object is actually on the scene. False positive if the guessed object is not on
the scene. True negative if there is no guess and the scene is empty.  False
negative if the scene shows objects but \clutseg did not make a guess.
Unfortunately, this scheme does not cover the case in which two objects have
been confused. In the application scenario, several objects are expected to be
on the scene. Statistics record when \clutseg returns without making a guess.
% TODO: need to give numbers in evaluation section about none_rate

\subsubsection{Pose Estimation Error}

Error in estimated pose can only be computed in case the guessed object is
actually visible in the scene. In case it is, the translational error $e_t$ given
ground truth pose $\oTv$ and pose estimate $\oTvh$ is

\begin{equation}
    e_t = \normtwo{\ \oTvh \pth{ \vec{0} -\ \oTv \vec{0} }\ }
\end{equation}

The orientation error $e_\alpha$ is computed from the ground truth orientation $r$
and estimated orientation $\hat{r}$, specified in axis-angle representation, and 
using the dot-product

\begin{equation}
    e_\alpha = \norm{ \arccos \pth{ \frac{\vec{r}^t\ \hat{\vec{r}}}{\normtwo{\vec{r}}\ \normtwo{\hat{\vec{r}}}} } }
\end{equation}

Rodrigues' rotation formula can be used to efficiently convert back and forth
between rotation matrix and axis-angle representation, and is readily available
in OpenCV.
% TODO: need citation

\subsubsection{Scene Scores}

Here, we present a scene score that measures \clutseg's performance on a single
query scene. The goal is to find a score function that closely models the
utility of a guess made for a query scene.

If the guessed object is not on the scene, clearly the utility of such a guess
is zero. Also, utility does not decrease much more, if the orientation and
translation error is beyond a certain margin of error. The idea is therefore
to introduce a combined classification and estimation score. 

Let $\alpha_{max} = \frac{\pi}{9}$ and $t_{max} = 3\mbox{cm}$ be the maximum
tolerable errors in orientation and translation, respectively. The choices are
consistent with the {\it Solutions in Perceptions Challenge 2011}.  Then, for a
query scene with ground truth $G$ and a guess $g$, we define the scene score
function $u$ as

\begin{equation}
u: G, g \mapsto
    \begin{dcases}
         1 & \text{if scene is empty and no guess made} \\
         0 & \text{if guessed object is not on scene} \\
         1 - \min\{1, \frac{e_t^2}{t_{max}^2} + \frac{e_\alpha^2}{\alpha_{max}^2}\} & \text{if guessed object is on scene}
    \end{dcases}
\end{equation}

One strength of the presented scene score function is that it is continuous
that retains more information than a simple statistic that records the binary
observation whether the guess is within or beyond an error margin. The
influence of error in pose is bounded, and outliers do not influence average
the score function too much. The latter characteristic might also be formulated
as a weakness in case of using this score as a function to maximize in
optimization. It does not provide useful gradient information in regions of bad
parameter sets, given the function's flat surface beyond a certain region
defined by $t_{max}$ and $\alpha_{max}$.

% TODO: maybe future work, need more complete ground truth for more complex evaluation
% For example, let two objects A and B be on the scene. \clutseg makes a guess,
% but confusing objects A and B, it states that A was found at a pose close to
% object B, and also with inliers emanating from object B. In this case, the
% presented score function will still assign a classification score of 0.5. To
% detect this, the definition for ground truth would have to be extended to
% include the regions on the query image that belong to each object. Then it is
% possible to distinguish between true and false inliers.

\subsubsection{Recognition Statistics}

\clutseg also collects statistics about the recognition process that do not involve
ground truth. These statistics cover the number of guesses, matches, inliers in
the detection and in the refinement stage. The runtime is recorded as well.

\subsection{Experiment Strategy}

Given that it might take several minutes to evaluate \clutseg's performance on
a validation set, it was vital to make a decision which regions of parameter
space to explore first and which regions to explore next. In this work, the
strategy was to select a few regions, and then search in the neighborhood of
those parameter sets that performed well in the experiments conducted so far.
The strategy resembled genetic programming to a certain extent.

\subsection{Experiment Runner}

This section describes how \clutseg provides an experiment runner that tries to
efficiently test different parameter sets against a validation or test set. It
describes how a database was employed to keep track of parameter sets and
experiment results, the visualization tools that make the results accessible
for analysis, and the major implementation decisions that saved computing
resources.

\subsubsection{Experiment Database}

\clutseg employs a SQLITE3 database to keep track of parameter sets and the
recorded statistics, as introduced in Sections~\ref{subsection:parameter-space}
and \ref{subsection:measured-statistics}  The experiment runner can be executed
in the spirit of a daemon process, and polls the database for new parameter
sets when idle. New parameter sets are tested for validity with computationally
cheap tests.  The failing ones are marked and skipped, this one failing
experiment does not kill the process. A no-frills object-relational mapper
provides a convenient interface to work with the database. The primary purpose
of the database was to collect experiment inputs and outputs in a central,
accessible and convenient location.  Few efforts have been spent on
normalization, performance tuning or any other aspects that were not related to
its primary purpose. For convenience, data can be selected from various
database views.

\subsubsection{Modelbase Cache}

Extracting models is computationally expensive and may take several minutes.
As long as the feature parameters do not change in several experiments, there
is no need to regenerate the models and the modelbase is retrieved by the SHA1
hashcode of the feature parameters. A modelbase for four template objects takes
up around five Megabytes on the filesystem, depending on the choice of feature
detector and descriptor. \tod stores the modelbase as compressed YAML text
files. If disk space were scarce and reading the modelbase quickly into memory
were an issue, then binary files would be a good choice.

\subsubsection{Visualization Tools}

\clutseg connects with OpenCV and R project to visualize the recognition
results in query scene and the statistical data acquired in experiments.  It
provides functions for visualizing guesses and ground truth in parallel,
showing inliers, translational and rotational errors in the manner of a
heads-up display. It provides command-line tools to inspect views of the
modelbase, that is the region of interest, the extracted keypoints, and the
estimated pose. Imagemagick is used to generate montage images that show the
results of one experiment at once. Finally, \tod allows to visualize matches
between the query scene and the modelbase. R is used to pull data from the
database and plot them.

