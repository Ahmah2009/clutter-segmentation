This chapter provides the theoretical foundations for building the 3D object
recognition system as presented in \refChapter{chapter:implementation}. We
review the required mathematical tools together with the basics of 3D object
recognition where models are learnt from multiple views. We discuss Oriented
BRIEF, a new feature detector and descriptor. We resort to approximative
nearest neighbour search, which permits to quickly establish correspondences
between the query image and the models. Then, we show how pose estimation is
related to other problems, and how to solve it in the presence of noise and
mismatches in the correspondences.  Finally, this chapter shows how the
implemented system relates to the framework of classifiers and estimators,
which is helpful for the evaluation given in \refChapter{chapter:evaluation}.

\section{Prerequisites in Geometry}

A model of an object is described in its own coordinate system. Another
coordinate system is defined by the depth camera.  In this section, we explain
how a coordinate system attached to the camera and a coordinate system attached
to the object are related by proper rigid transformations. We precisely define the
pose of an object with respect to the camera in terms of a proper rigid
transformation. We also describe how pose estimation is related to camera
calibration and the perspective-n-point problem.

% standard coordinate system
% multiple views
% perspective transformation
% pose estimation is just estimation of a transformation

\subsection{Rigid Transformations}

A {\it rigid transformation} of coordinates is defined as an orthogonal
transformation followed by a translation in Euclidean space. An orthogonal
transformation is either a reflection or a rotation. A {\it proper rigid
transformation} additionally requires the orthogonal transformation to be a
rotation (the definitions of rigid transformations found in the literature are
inconsistent, though: Reflections are excluded from rigid transformations in
\cite{Forsyth2003}, but included in \cite{Galarza2007}, we stick to the
latter).  In three dimensions, a proper rigid transformation of vectors can be
represented by a rotation matrix $R \in \real^{3\times3}$ and a translation
vector $\vec{t} \in \real^3$.

\begin{equation}
    T: \real^3 \to \real^3, \vec{p} \mapsto R\vec{p} + \vec{t}
\end{equation}

The inverse transformation is given by
 
\begin{equation}
    T^{-1}: \real^3 \to \real^3, \vec{p} \mapsto R^{-1}(\vec{p} - \vec{t}) = R^T(\vec{p} - \vec{t})
\end{equation}

A proper rigid transformation (PRT) can be used to describe a rigid motion of
an object with respect to a reference coordinate system. In physics, this is
called an active transformation, and explains the relationship between two
different points in the same coordinate system.

Another interpretation is to see a PRT as a passive transformation. The PRT
transforms vector coordinates between two different coordinate system.  The
passive interpretation is used throughout this work.

\subsection{Object-View Transformations}
\label{section:object-view-transformations}

3D points can be specified in coordinates with respect to any coordinate
system. The question arises which coordinate systems are convenient and how
many are necessary. In our case, the two different coordinate systems discussed
in the following prove sufficient.

The {\it \gls{view coordinate system}} is attached to the camera. Two of its basis
vectors define the image plane and the remaining basis vector is directed
towards the scene. The view coordinate system is convenient for the robot. For
example, if the robot is programmed to grasp an object at two points, it needs
to know the view coordinates of the two grasping points. The {\it \gls{object
coordinate system}}, on the other hand, is attached to the object. The model
features are located in the object coordinate system, which is also called
model coordinate system in the literature \cite{Pope2000}. Both coordinate
systems are shown in \refFigure{figure:coordinate-frames}. The PRT between
object coordinates and view coordinates is called the {\it \gls{object-view
transformation}} and is denoted by $\oTv$. Subsequently, its inverse PRT is
called {\it \gls{view-object transformation}} and denoted by $\vTo$

\begin{equation}
  \vTo = \pth{ \oTv }^{-1}
\end{equation}

We stick to the subscript and superscript notation introduced in
\cite{Forsyth2003}.  Here is an example that indicates the usefulness of these
definitions. To compute the distance between the camera and the object origin,
we may use

\begin{equation}
d = \normtwo{\ \oTv \pth{ \vec{0} }\ }
\end{equation}

\begin{figure}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{media/coordinate-frames}
    \end{center}
    \caption{The view coordinate system and the object coordinate system}
    \label{figure:coordinate-frames}
\end{figure}

The object-view transformation defines the relationship between camera and
object. Since we are only interested in the {\em relative} location and
orientation of the object with respect to the camera, we simplify matters by
assuming the camera to be fixed and by always looking at scenes from the eyes
of the robot. Note that if there are multiple objects in a scene, then there
exists one object-view transformation for each object. For example, if there
are two objects $1$ and $2$ in the scene, the distance $d_{12}$ between them
may be computed by
\begin{equation}
d_{12} = \normtwo{\ \oTvi{1} \pth{ \vec{0} } -\ \oTvi{2} \pth{ \vec{0}} \ }
\end{equation}
where $\oTvi{1}$ and $\oTvi{2}$ are the respective object-view transformations,
which uniquely determine the object poses with respect to the camera.

We can reformulate our problem of recognizing objects as

\begin{quote}
    ``Find the object-view transformation for each
    of the objects in the scene.''
\end{quote}

In the special case of a single object

\begin{quote}
    ``Find the object-view transformation for one
    of the objects in the scene.''
\end{quote}

\subsection{Camera Model}

In our selected approach, we construct models by associating local 2D features
extracted from the image plane with corresponding 3D points in the scene, as
shown in \refFigure{figure:keypoints}. In the recognition process,
models are aligned to the scene and projected onto the image plane to verify
whether they explain the observed data. In both cases, it is required to know
the nature of the relationship between the scene and its 2D image, which
is explained by the camera model.

Since both \tod and \clutseg use OpenCV, an open source library for computer
vision, we adopt their camera model. It is a pinhole camera model, where a
perspective projection is used to determine the projection of a 3D point onto
the image plane \cite{Bradski2008}. This plane is conveniently defined in front
of the center of perspective \cite{Forsyth2003, Bradski2008}. 

% \begin{figure}[h]
%    \begin{center}
%        \includegraphics[width=0.8\textwidth]{media/camera-model}
%    \end{center}
%    \caption{}
%     \label{figure:camera-model}
% \end{figure}

In the previous section, it was shown that estimating the pose of an object
with respect to the camera is equivalent to estimating the object-view
transformation. There is a third way to look at this problem, and this is the
estimation of the extrinsic parameters of the camera. Unlike the intrinsic
camera parameters that describe properties of the camera independently of its
standpoint, the extrinsic parameters relate the camera pose to some given
coordinate system in its environment. Thus, equivalently to the problem
definitions given previously in \refSection{section:object-view-transformations},
we can see our object recognition problem as

\begin{quote}
    ``Find the extrinsic camera parameters with
    respect to the coordinate system of one of the objects
    in the scene.''
\end{quote}

Estimation of camera parameters belongs to the topic of {\it camera
calibration}.  \tod uses camera calibration techniques from OpenCV, where the
intrinsic parameters are known, but the extrinsic parameters remain to be
estimated.

Given correspondencies of $n$ 3D points with respect to a coordinate system,
and their $n$ projections onto the image are known, the extrinsic parameters
can be estimated. It is an instance of the {\it perspective-$n$-point} (PnP)
problem \cite{Fischler1981}.

\section{Object Recognition}

In the previous chapter, we discussed how to formulate our problem of object
recognition. Here we introduce the basic mechanisms for learning models from
multiple views, based on the characteristic features in the object's
appearance.  We show how feature detectors and descriptors find these features,
especially using Oriented BRIEF. We cover how nearest-neighbour search finds
correspondences between query features and model features. Finally, based on a
set of correspondences, we introduce a general RANSAC-based approach of
estimating the pose of objects in the scene.

% Object recognition is the problem of labeling or locating objects in a query
% scene. This section describes relevant mathematics and techniques used in 3D
% object recognition with local 2D features.
% TODO: provide references to other approaches, or different subxxx of Object Recognition (CAD based, etc.)
% instead of just talking about doing it
% References to other approaches,
% such as with true 3D features or object recognition using parametric models are
% provided but, being out of scope, not covered in detail.

% \subsubsection{Applications of Object Recognition}
% Mars mission
% Flood detection
% Earthquake detection
% Industrial quality control

\subsection{Local Features}

In object recognition, the question arises on how to generate a description of
an object, and how to describe a query scene. We can distinguish local
approaches from global approaches. An example for a global approach is template
matching. Yet, in scenes with occlusion, local methods are clearly preferable
\cite{Pope2000}.  Object recognition using local features is based on the
detection of keypoints in an image. A multitude of different methods is
available, but they all aim at choosing keypoints that are repeatable for the
same object over different images. A 2D local feature consists of a keypoint
and a feature descriptor that captures information about its local image
neighbourhood. Feature descriptors are often vectors and their distance in
terms of a vector norm provides a means for measuring the similarity between
two features. 

\subsubsection{Learning Models from a Single View}

Lowe describes an approach to object recognition based on local features where
each model is learnt from a single view of the template object \cite{Lowe1999}.
Given a query image, local features are extracted and matched against the model
(\refFigure{figure:single-view-or}). This approach works well, if the object in
the query scene has approximately the same orientation as in the template view.
Lowe reports that using SIFT features, discussed in
\refSection{subsection:SIFT}, it is possible to recognize objects up to a 20
degree rotation \cite{Lowe1999}. Unfortunately, considering only a single view
is insufficient if query scenes show objects in arbitrary orientations.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.55\textwidth]{media/single-view-or}
        \caption[Object recognition with models learned from a single view]{Object recognition with models learned from a single view. The example shows three correspondences
between query features ($\times$) and model features ($\bullet$).}
        \label{figure:single-view-or}
    \end{center}
\end{figure}

\subsubsection{Learning Models from Multiple Views}
\label{subsubsection:multi-view-or}

The single-view scheme can be extended to allow for recognizing objects in
arbitrary orientations. In order to accomplish this, a description of a
template object is based on local features extracted from multiple views of the
object. Local features from a query scene are matched against the model that
incorporates features recorded from different viewpoints
(\refFigure{figure:multi-view-or}).

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.7\textwidth]{media/multi-view-or}
        \caption[Object recognition with models learned from multiple
            views]{Object recognition with models learned from multiple views.  The example
            shows correspondences between query features ($\times$) and model features
            ($\bullet$). Correspondence $c_1$ is matched to the right model, but the
            location is incorrect. Correspondences $c_2$ and $c_3$ are correct.  Finally,
            correspondence $c_4$ is mismatched.}
        \label{figure:multi-view-or}
    \end{center}
\end{figure}

Generally, there is no need to incorporate all collected features into the
model. A strategy on how to select features from multiple views is presented in
\cite{Pope2000}. Learning models from multiple views is the basis of this work. 

\subsection{Feature Detectors}
\label{subsection:feature-detectors}

% TODO: strip section about feature detectors down to the bare essence

% FIXME: re-read Tuytelaars to get this right
A feature detector designates keypoints in an image. There exists a multitude
of feature detectors in the literature, and more efforts have been spent into
inventing new ones than comparing existing ones \cite{Rosten2010}.

% Some feature detector
% algorithms assign an orientation to each keypoint. Detectors can be roughly
% partitioned into three groups \cite{Tuytelaars2007}. First, there are {\it
% corner detectors}, such as Harris and SUSAN, that select points with high
% curvature. Second, {\it blob detectors} such as SURF or DoG look for points
% that are surrounded by brighter or darker pixels \cite{Bay2006}. Third, there
% are {\it region detectors}, such as MSER, which extract features by finding
% image regions.

There is generally no such thing as the best feature detector. As often in
engineering, all we can do is making the best choice for the task at hand.  An
ideal feature detector though, should be {\it efficient}, and it should find
features that are {\it repeatable}, {\it informative}, {\it local}, and {\it
accurate} \cite{Tuytelaars2007}.

Repeatability describes the chances that the same point on an object shown from
two different viewpoints and viewing conditions is designated keypoint on both
images. Thus, repeatability is important when trying to find
correspondences between two images. A feature detector designates keypoints
after examining the local image neighbourhood. These neighbourhoods can for
example be deformed by noise, image discretization, a change of viewpoint or a
change in lighting conditions \cite{Tuytelaars2007}. These deformations have to
be addressed by a feature detector to achieve repeatability. In order to
achieve invariance as regards scale and rotation, features can be extracted at
multiple scales, and orientations can be assigned to keypoints
(\refTable{table:feature-detectors}).

An informative feature identifies an image neighbourhood that shows high
variation in intensity. If not, the features would be difficult to distinguish.
For example, we would not like to have features detected on non-textured
background, or on areas that carry little information.

A local feature only carries information about a small neighbourhood of a point.
This ensures that it is still repeatable despite of occlusion. A small
neighbourhood also leads to less informative features, enforcing a trade-off
between locality and informativeness.

\subsection{Feature Descriptors}
\label{subsection:feature-descriptors}

A feature descriptor describes the image neighbourhood of a point. It is often a
vector. Ideally, two corresponding keypoints should have similar
descriptor vectors, such that correspondences can be found by nearest neighbour
search. A feature descriptor that permits for efficient matching is BRIEF
\cite{Calonder2010}. 

\begin{table}
    \begin{center}
        \begin{tabular}{lcccc}
            Detector & Multi-scale & Oriented Keypoints \\
            \hline
            SIFT & yes & yes \\
            SURF & yes & yes \\
            FAST & no & no \\
            ORB & yes & yes 
        \end{tabular}
    \end{center}
    \label{table:feature-detectors}
    \caption[Properties of selected feature detectors]{Properties of selected
            feature detectors. SIFT, SURF, and ORB are also feature descriptors}
\end{table}

\subsubsection{SIFT}
\label{subsection:SIFT}

The scale-invariant feature transform became popular in object recognition.  It
is desirable that images of objects at different scales produce more or less
the same features. SIFT achieves this by selecting keypoints from a scale-space
pyramid in order to generate scale-invariant features. SIFT is both a feature
detector and a descriptor. Lowe further describes a system that lets
correspondences vote in Hough space, whose dimensions are location, orientation
and scale \cite{Lowe1999}. When bins contain more than a predefined number of
votes, the object is said to be recognized at this pose. SIFT has also
successfully been applied to panorama stitching \cite{Brown2006}. 

\subsubsection{FAST}

FAST is a feature detector that designates keypoints by looking at a Bresenham
circle of 16 pixels in diameter around a candidate keypoint. If 9 contiguous
pixels of the diameter pixels are all brighter (a), or all darker (b) than the
center pixel plus (a), or minus (b) a certain threshold, then the circle center
is designated keypoint \cite{Rosten2006}. FAST is efficient because it can rule
out certain candidate keypoints by reasoning on the diameter pixels values
north, east, south and west of the center. FAST-ER builds on this idea, but
reasons about the diameter pixels in an order optimized by ID3; the information
gain is computed on a set of training images \cite{Rosten2010}. \opencv 2.2
just contains an implementation of FAST, presumably because of its smaller code
size.

\subsubsection{BRIEF}

BRIEF generates binary feature descriptors for a given set of keypoints. The
BRIEF descriptor vectors are sensitive to rotation \cite{Calonder2010}. They
are formed by a bit string computed from the image neighbourhood of a given
keypoint. Each bit stores the result of comparing pixel intensities between two
2D points in the neighbourhood, which are called {\it test locations}. The test
locations are initially sampled from a 2D Gaussian distribution once, and
together form a {\it test pattern} \cite{Calonder2010}. Finding nearest
neighbours of binary descriptors according to their Hamming distance is faster
than for real-valued features using the L2 norm.

\subsubsection{Oriented BRIEF}

{\it Oriented BRIEF} ({\it ORB}) is a combination of the FAST feature
detector and the BRIEF feature descriptor, plus modifications that render the
extracted features orientation-invariant. It was designed to outperform SIFT
and SURF at least in terms of computation time. An implementation is already
available in the development branch (trunk) of OpenCV, and a corresponding
paper is expected to be published at the {\it International Conference on
Computer Vision} 2011.

ORB computes image moments for assigning orientations to keypoints detected by
FAST. Image moments of order $(p + q)$ are defined for a 2D digital image $f$
of dimension $MxN$ as

\begin{equation}
    m_{pq} = \sum_{x=0}^{M-1} \sum_{y=0}^{N-1} x^p y^q f(x, y)
\end{equation}

ORB computes moments $m_{01}$, and $m_{10}$ for neighbourhoods $f$ of the
keypoints and designates orientations $\alpha$ as

\begin{equation}
    \alpha = \arctan \frac{m_{01}}{m_{10}}
\end{equation}

Consider the vector $\pth{ m_{01}, m_{10} }^T \in \real^2$. The angle of this
vector becomes the orientation of the keypoint. If $f$ were a probability
distribution, then this vector would contain the means of the marginal
distributions of $x$ and $y$, respectively.

% TODO: verify correlation problem and relation to descriptor entropy

A problem arises, when rotating the BRIEF test pattern according to the FAST
keypoint orientation. There is a correlation between the image moments and the
BRIEF vectors obtained from a rotated test pattern. Such a correlation reduces
the entropy of computed BRIEF descriptors. The solution is to compute a test
pattern that has only little correlation. Oriented BRIEF uses test locations as
depicted in \refFigure{figure:orb-test-locations}.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.4\textwidth]{media/orb-test-locations}
        \label{figure:orb-test-locations}
        \caption{The 256 test locations of Oriented BRIEF}
    \end{center}
\end{figure}

The description about ORB given above is based on a presentation at the {\it
International Conference on Robotics and Automation} (ICRA) 2011 in Shanghai,
and inferred from the implementation of ORB in the OpenCV library. 

\subsection{Feature Matching}
\label{subsection:feature-matching}

% TODO: speed-up must be backed up by a benchmark in the evaluation section

In object recognition that is based on local features, correspondences between
known views of the object and the query image can be established by matching
the query features with model features. The computation time depends on the
size of the descriptor vector, the nearest neighbour algorithm and the used
distance metric. If exactness is not required, i.e. approximate nearest
neighbours are sufficient, then speed-ups can be achieved. % TODO: cite
Computing the Hamming distance between binary features is faster than computing
the L2-norm for real-valued feature descriptors \cite{Calonder2010}. This holds
especially true when using the {\tt POPCNT} instruction, which is available at
least in AMD and Intel architectures \cite{AMD2009, Intel2007}.

\subsubsection{Locality Sensitive Hashing}

% TODO: cite can also be adapted to match binary features such as BRIEF. 
% TODO: consistent use of uppercase and lowercase in algorithm names

SIFT, ORB and other feature detectors and descriptors are used to describe an
image in terms of local features. Nearest neighbour matching can then find
correspondences between two feature sets. Locality Sensitive Hashing (LSH)
quickly finds nearest neighbours, and trades exactness in favour of a reduction
in computation time. Not always does it return the absolute nearest neighbour.
LSH is a randomized algorithm that performs multiple scalar projections of a
high-dimensional vector, and reasons that the scalar projections are similar
for similar input vectors. It can be extended to find nearest neighbours for
binary features. LSH has applications in object recognition, image retrieval,
music retrieval, and identification of duplicates
\cite{Slaney2008}.

\subsection{Pose Estimation}
\label{subsection:pose-estimation}

The previous sections showed how the problem of recognizing objects is related
to the estimation of extrinsic parameters, and in particular, to the
perspective-$n$-point (PnP) problem. The PnP problem can be solved by RANSAC.

RANSAC stands for {\it RANdom SAmple Consensus}. It is a randomized algorithm
that fits a parametrized model to the data. It is robust with respect to
outliers. The pose estimation problem in this context can be reduced to the
location determination problem or the equivalent PnP problem
\cite{Fischler1981}.  Generally, this algorithm randomly chooses a small subset
of the data that fully determines the model parameters.  Then it forms a
consensus set of {\it \glspl{inlier}}, that is, all data points that are consistent
with the model up to a certain margin of error. RANSAC constructs a fixed
number of such consensus sets and then chooses the model with the largest
consensus to compute the final estimate.

Pose estimation using RANSAC can be categorized as a method of {\it pose
consistency} or {\it alignment}, where first a pose is hypothesized, and
then the hypothesized pose is verified for support by the remaining data
\cite{Forsyth2003}.

% There are six degrees of freedom for an object to be located and oriented in
% space, three for translation and three for orientation. 
% Three 2D points in perspective projection are INsufficient to fully determine a pose.
% If there were no noise in the set of 2D correspondences, we would be able to
% find poses of an object by simply taking any three correspondences belonging to
% this object. Unfortunately, image noise, discretization effects and approximate
% nearest neighbours asks for a more sophisticated approach that incorporates
% strategies to deal with noise and inconsistency. RANSAC iteratively selects
% three correspondences to compute pose estimates. The 3D model of the template
% object aligned to the estimated pose is then projected on the query image plane
% to compute the projection error and the consensus set. The size of the
% resulting consensus sets plays an important role in this work.

\section{Machine Learning}

This section shows how object recognition relates to machine learning,
especially to classification and estimation theory. It also covers the basics
of a machine learning experiment, which are required for optimizing parameters
and for understanding the strengths and weaknesses of an object recognition
system.  When evaluating the system, a measure of {\it goodness} needs to be
defined.

\subsection{Classification}

It is difficult to perceive our object recognition problem as a pure
classification problem.  Assume, there were no interest in the pose of an
object. Assume further that objects are unique on an image. Then object
recognition can be seen as a {\it multi-label classification} problem, as
described in \cite{Tsoumakas2007}.  An image is described by a set of labels,
and symmetrically, the classification result is a set of labels that predict
which objects are in the image. The Receiver operating characteristics (ROC)
can be used to evaluate standard two-class classifiers \cite{Fawcett2006,
Melsa1978}. Standard measures of goodness are the true and false positive rates
that can be computed from confusion matrices. In case of multi-label
classification, it is possible to define similar metrics \cite{Tsoumakas2007}.
Unfortunately, even if object pose were not an issue, assuming objects to
appear uniquely on an image might be unrealistic. For example, a shopping bag,
as in the application scenario of this work, is best represented by a multiset,
and not by a multi-label set. Thus, also a scene with shopping items is best
represented by a multiset. Given a finite set of labels, the number of
multi-label sets is finite, the number of multisets is not. A problem with an
infinite number of decisions is an estimation problem rather than a
classification problem \cite{Melsa1978}. For example, trying to define a
specificity metric on it is difficult, since the number of negatives in a
multi-set is generally infinite.

\subsection{Estimation}

Estimation can be regarded as the continuous generalization of classification
\cite{Melsa1978}. Clearly, if it is known which objects are in a scene, object
recognition can be perceived as a pure estimation problem. For each object on
the scene, the six degrees of freedom of an object's pose have to be estimated.
The squared and absolute errors of an estimate are possible measures of
goodness that are generally available for estimators.

\subsection{Recognition}

Our recognition problem neither fits a pure classification nor a pure
estimation problem. In this work, a measure of ``goodness'' sees the object
recognition system as a combination of classifier and estimator. Estimation
``goodness'' is conditioned to the classifier making the right choice, and
scenes that contain multiple instances of the same object are not considered.

\subsection{Parameter Optimization}

In the presented system, the algorithms for detecting, describing and matching
features are parametrized, as well those for pose estimation and those for
making the final decision. This leads to the question of how to find a
parameter set such that the system performs well in the application scenario.

\subsubsection{Validation Set}

Choosing good parameters for parametrized algorithms is a general problem
in computer science. Solutions for these problems include adaptive methods that
estimate the parameters directly from the data. For example, in adaptive
numerical quadrature, grid points are selected according to an error estimate
gained directly from function samples. Optimization techniques find a parameter
set that optimizes performance on a validation set, where ground truth is
avaible. This optimization technique is well-known in machine learning.

\subsubsection{Test Set}

It is interesting to estimate the error which a classifier or estimator is
expected to make when working on new instances. This is the generalization
error which cannot be estimated on the validation set. Instead a separate test
set is required to estimate the generalization error \cite{Alpaydin2010}.
% , and is related to the problem of assuming appropriate priors
% in Bayesian inference.

% TODO: some concluding sentences
