This chapter presents the experimental results of the \clutseg system, on the
validation set, on a test set and results from a live test on a PR2 robot. It
covers how the modelbase was built, how validation data, ground truth and the
test data have been obtained. The parameter set found by experimentation is is
discussed. Scenarios are presented in which the system performed well, and also
limitations and issues that became apparent during the evaluation.

\section{Data Acquisition}
\label{section:data-acquisition}

This section shows how all the data required for constructing the modelbases,
the validation set and the test set was obtained. Those three artifacts all
required raw data, which was collected in a similar fashion, using the same
hardware and software setup.

% The Robot
% Operating System running on the PR2 robot is organized as a distributed system
% of nodes, which communicate via topics. Nodes can subscribe and publish to
% topics. Data from topics can be recorded in bag files.

\subsection{Common Setup}

We used the Willow Garage PR2 robot at TUM to collect raw data.  On top of the
PR2, a Kinect RGB-D camera was mounted. A wooden board with two chessboards
attached was setup in about TODO meters distance from the camera. This board
served as a rotating table. The chessboards were printed on white DIN-A4
paper using a laser printer, and around them was enough white margin as
recommended in the documentation of method {\tt findChessboardCorners} in
OpenCV. The recording took place in an indoor laboratory environment whose
lighting conditions were roughly similar to the ones to be expected in an
application scenario. The board was manually rotated, carefully and not too
fast.  The objects must not move with respect to the fiducial markers, which
are attached to the object coordinate system.

  The PR2 robot was controlled by the Robot Operating System, which is a
distributed system where nodes communicate by publishing and subscribing to
topics. In our case, the interesting topics were the image and the 3-d point
cloud, together forming the sought-after depth images. In ROS, data can be
recorded in so-called bags. For depth images, these bags quickly grow large.

  The resolution of the image was 1280x1024 pixels, and the dense point cloud
had a resolution of 640x480. The sampling rate was set to about one depth image
per second. The camera matrix needs to be recorded as well, for it is required
at least in solving the point-to-point correspondence problem in pose
estimation. 

% TODO: write chessboard, not chessboard, in order to be consistent with OpenCV, in order to be consistent with OpenCV


\subsection{Modelbases}
\label{subsection:modelbase}

% Acquiring modelbases was done in two independent steps. First, raw data
% was collected once. Second, several modelbases were built according to 
% the feature parameter specifications in the experiments. 

\tod requires multiple views from the template object. For the model base, we
saved about 60-70 views per object to a bag file, which has been observed to
work better than with only 40 views. For each object, around 1 Gigabyte of data
was recorded. The next preparatory step was to uncompress the bag files with a
script included in both \clutseg and \tod, then perform pose estimation and
masking as described in Section \ref{subsection:learning}. This being done, the
final step of model extraction was left up to the experiment runner, since it
depends on the feature parameters. The experiment runner needs access to the
raw data. The resulting models of each template object were much smaller in
size, and took up around 1 Megabyte of disk space each, three magnitudes less
than the original raw data.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{media/modelbase}
        \caption[Modelbase used in evaluation]{The modelbase with four different template objects used in evaluation}
    \end{center}
\end{figure}

Raw data was collected for four different household objects, commonly found in
German supermarkets. These are {\it assam\_tea}, {\it haltbare\_milch}, {\it
icedtea} and {\it jacobs\_coffee}. The objects have different properties. For
example, {\it assam\_tea} is smaller than {\it icedtea} whose frontside shows
much more characteristic texture than its back that is full of smallprint. {\it
jacobs\_coffee} show texture that repeats on different sides of the object.
Finally, {\it haltbare\_milch} contains characteristic texture which is
confined only to certain regions on its surface.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{media/fiducial-object}
        \caption{Relation between fiducial markers and object coordinate system}
    \end{center}
\end{figure}

Note that there is a subtlety in the relationship of the fiducial markers and
the object coordinate system. Let the {\it fiducial coordinate system} be
attached to the fiducial markers. Its origin is defined midway between the
two chessboards. Using 3-d camera calibration techniques, it is possible to
compute the transformation $_fT^v$ between between fiducial coordinates and view
(camera) coordinates. Yet, we need the object-view transformation $_oT^v$. The
solution here is to just {\it define} the object coordinate system to be the
same as the coordinate system attached to the fiducial markers

\begin{equation}
    _oT^v\ =\ _fT^v
    \label{equation:fiducial-object-constraint}
\end{equation}


% TODO: xyz and uvw are useless, rather use V, F, O to denote the different
% coordinate systems.

This constraint requires that a template object must not move with respect to
the fiducial markers, as this would be equivalent to redefining the object
coordinate system. We remembered this definition by putting a small pencil mark
on the point of the template object at the origin of the object coordinate
system.


\subsection{Validation Set}

The validation set, which was used to optimize parameter sets, was collected
similarly to the data for the modelbases. The fiducial markers were required
for computing ground truth for the objects on the validation scenes. We
recorded around 50-60 scenes, each of them showing three instances of objects
in the modelbase. Out of those, we chose 21 scenes at roughly equal angle steps
to form the validation set.

Given a scene, ground truth requires to compute the object-view transformations
$_oT^v_i$, $_oT^v_j$, and $_oT^v_k$ for the three objects $i$, $j$ and $k$ in
the scene. We placed object $j$ in the center between the fiducial markers, such
that constraint \ref{equation:fiducial-object-constraint} was satisfied, that is

\begin{equation}
    _oT^v_j: \vec{p} \mapsto\ _fT^v \left(\vec{p}\right)
\end{equation}

In other words, object $j$ was placed on the rotating table exactly as when
raw data for modelbase had been collected. The pencil markers helped to remember
the setup.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{media/fiducial-object-3}
        \caption{Relation between fiducial markers and three object coordinate systems}
    \end{center}
\end{figure}

The other two objects $i$ and $k$ were placed upright next to object $i$, with
offset of $-0.15$ and $0.13$ in x-direction of fiducial coordinates

\begin{equation}
    _oT^v_i: \vec{p} \mapsto\ _fT^v\left(\vec{p} - \begin{pmatrix} 0.15 \\ 0 \\ 0 \end{pmatrix} \right)
\end{equation}

\begin{equation}
    _oT^v_k: \vec{p} \mapsto\ _fT^v\left(\vec{p} + \begin{pmatrix} 0.13 \\ 0 \\ 0 \end{pmatrix} \right)
\end{equation}

Units are specified in metres. Computation of ground truth was made easier 
because the objects were placed upright and coplanar to the rotating table.

% TODO: future work might involve computation of ground truth in more complex
% configurations


% TODO: figure object-coordinate-syste

\subsection{Test Set}

The test set was made of cluttered scenes of various character. All four
objects are shown on the test scene. Some scenes showed objects in upright
position, but some objects occluding others. Other scenes showed objects in
arbitrary orientation and medium occlusion. Other, yet more difficult scenes,
showed objects in arbitrary orientation and severe occlusion.

The test set data has been collected similarly to the validation set, but
ground truth was not computed. The evaluation on the test set was therefore
restricted to visual analysis.

\section{Experiments}

This section describes the regions in parameter space that have been covered by
experiments. This knowledge is required to interpret data from multiple
experiments. It also presents a selected parameter set that has been found to
work well on the validation set.

\subsection{Searched Parameter Space}

In total, more than 2200 experiments with different parameter configurations
have been conducted. 
% TODO: number of experiments

<<echo=FALSE>>=
    library(RSQLite)
    con = dbConnect(dbDriver("SQLite"), dbname = paste(Sys.getenv("CLUTSEG_PATH"), "/clutter-segmentation/clutseg/data/test.sqlite3", sep=""))
    dbGetQuery(con, paste("select count(experiment.id) from response join experiment on experiment.response_id=response.id"))
@

\subsection{Selected Parameter Set}

The following parameter set, dubbed SPS, has been found to work well on the
validation set.

\section{Performance}

Performance of the \clutseg system has been tested on the validation set, on a
separate test set and in a live test on the PR2 robot. This section reviews
performance for a parameter set that worked well on the validation set. It also
covers observations made on a broad range of experiments.

\subsection{Performance on Validation Set}

The \clutseg system correctly located and labeled an object within error bounds
of 3 cm and 20 degrees in rotation in 90\% of the validation scenes. This
success rate is not predictive, though, as the validation set was used for
optimizing the system. Because ground truth was available, more precise data
can be presented on the validation set than for the test set. 

\subsection{Performance on Test Set}
\subsection{Performance in Live Test}

\section{Issues}

\subsection{Repetitions in texture}

The system had difficulties to recognize {\it icedtea} when shown from a
certain angle.  It turned out that {\it icedtea} has smallprint almost all over
its backside. The same set of letters repeatably appear over a large area,
which probably makes it hard to obtain informative features.

\subsection{Incomplete model}

The bottom of the template object is facing the rotating table and is invisible
to the camera from all viewpoints. This does not matter as long as instances of
these template objects are standing upright. This does matter when instances occur
in arbitrary orientation.

\subsection{Maximum projection error in RANSAC}

Generally, the system relies on the size of the consensus set to be a strong
indicator for the goodness of an estimate. It was observed, though, that in one
experiment, the model was badly aligned, yet had strong support by many
inliers. The reason for the large consensus set is that the projection error is
a measure on the image plane, and matching keypoints are designated inliers as
long as they do not exceed the maximum projection error which is a circle
induced by L2-norm on the image plane. The problem can be traced back to the
fact objects that are far away from the camera merely degenerate to a point on
the image plane. All correspondences within the circle are designated inliers.

% TODO: reformulate

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.6\textwidth]{media/ransac-max-projection-error-deficiency}
        \caption[Schematic explanation of distant pose estimate with many inliers]{Far-away pose estimate can result in many inliers}
    \end{center}
\end{figure}

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.6\textwidth]{media/ransac-max-projection-error-deficiency-sample}
        \caption[Sample of distant pose estimate with many inliers]{On a test scene, the badly aligned pose still results in many inliers}
    \end{center}
\end{figure}




