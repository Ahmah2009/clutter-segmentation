<<echo=FALSE>>=
    source("../scripts/common.R")
@

This chapter presents the experimental results of the \clutseg system, on the
validation set, on a test set and results from a live test on a PR2 robot. It
covers how the modelbase was built, how validation data, ground truth and the
test data have been obtained. The parameter set found by experimentation is is
discussed. Scenarios are presented in which the system performed well, and also
limitations and issues that became apparent during the evaluation.

\section{Data Acquisition}
\label{section:data-acquisition}

This section shows how all the data required for constructing the modelbases,
the validation set and the test set was obtained. Those three artifacts all
required raw data, which was collected in a similar fashion, using the same
hardware and software setup.

% The Robot
% Operating System running on the PR2 robot is organized as a distributed system
% of nodes, which communicate via topics. Nodes can subscribe and publish to
% topics. Data from topics can be recorded in bag files.

\subsection{Common Setup}

We used the Willow Garage PR2 robot at TUM to collect raw data.  On top of the
PR2, a Kinect RGB-D camera was mounted. A wooden board with two chessboards
attached was setup in about TODO meters distance from the camera. This board
served as a rotating table. The chessboards were printed on white DIN-A4
paper using a laser printer, and around them was enough white margin as
recommended in the documentation of method {\tt findChessboardCorners} in
OpenCV. The recording took place in an indoor laboratory environment whose
lighting conditions were roughly similar to the ones to be expected in an
application scenario. The board was manually rotated, carefully and not too
fast.  The objects must not move with respect to the fiducial markers, which
are attached to the object coordinate system.

  The PR2 robot was controlled by the Robot Operating System, which is a
distributed system where nodes communicate by publishing and subscribing to
topics. In our case, the interesting topics were the image and the 3D point
cloud, together forming the sought-after depth images. In ROS, data can be
recorded in so-called bags. For depth images, these bags quickly grow large.

  The resolution of the image was 1280x1024 pixels, and the dense point cloud
had a resolution of 640x480. The sampling rate was set to about one depth image
per second. The camera matrix needs to be recorded as well, for it is required
at least in solving the point-to-point correspondence problem in pose
estimation. 

% TODO: write chessboard, not chessboard, in order to be consistent with OpenCV, in order to be consistent with OpenCV


\subsection{Modelbases}
\label{subsection:modelbase}

% Acquiring modelbases was done in two independent steps. First, raw data
% was collected once. Second, several modelbases were built according to 
% the feature parameter specifications in the experiments. 


\tod requires multiple views from the template object. For the modelbases, we
saved about 60-70 views per object to a bag file, which has been observed to
work better than with only 40 views. For each object, around 1 Gigabyte of data
was recorded. The next preparatory step was to uncompress the bag files with a
script included in both \clutseg and \tod, then perform pose estimation and
masking for each view as described in \refSection{subsection:learning}.  This
post-processed raw data provides the masks and the object-view transformation
which can be used to create several modelbases using different feature
detectors and descriptors. The resulting models of each template object were
much smaller in size than the original raw data. They took up around 1 Megabyte
of disk space each, three magnitudes less than the original raw data. The
feature detectors are responsible for this data reduction.
In the experiments, ORB selected 394 keypoints on average per image of the
modelbase raw data.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{media/modelbase}
        \caption[2D view of template objects used in evaluation]{The four different template objects used in evaluation}
        \label{figure:modelbase}
    \end{center}
\end{figure}

\refFigure{figure:modelbase} shows the four different household objects for
which we collected raw data. They are items commonly found in German
supermarkets. These are {\it assam\_tea}, {\it haltbare\_milch}, {\it icedtea},
and {\it jacobs\_coffee}. The objects have different properties. For example,
{\it assam\_tea} is smaller than {\it icedtea}. The front of {\it icedtea}
exhibits characteristic texture. Its back, though, only shows smallprint. {\it
jacobs\_coffee} shows texture that repeats on different sides of the object.
Finally, {\it haltbare\_milch} contains characteristic texture which is
confined only to certain regions on its surface.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{media/modelbase-3D}
        \caption[Model features of a selected modelbase]{Model features of a selected modelbase}
        \label{figure:modelbase-3D}
    \end{center}
\end{figure}

\refFigure{figure:modelbase-3D} shows the model features of a selected
modelbase. From the dense 3D point cloud collected for each object, the feature
detector chooses keypoints. This results in a sparse 3D point cloud
representation for each template object.

A practical consequence of \refEquation{equation:fiducial-object} is that a
template object must not move with respect to the fiducial markers when
collecting raw data. This would be equivalent to redefining the object
coordinate system.

\subsection{Validation Set}

The validation set, which was used to optimize parameter sets, was collected
similarly to the data for the modelbases. Again, the fiducial markers were
required for computing ground truth for the objects on the validation scenes.
We recorded around 50-60 scenes, each of them showing three instances of
objects in the modelbase. Out of those, we chose 21 scenes at roughly equal
angle steps to form the validation set.

Given a scene, ground truth requires to compute the object-view transformations
$\oTvi{i}$, $\oTvi{j}$, and $\oTv{k}$ for the three objects $i$, $j$ and $k$ in
the scene. We placed object $j$ in the center between the fiducial markers, such
that 

\begin{equation}
    \oTvi{j}: \vec{p} \mapsto\ \fTv \pth{\vec{p}}
\end{equation}

In other words, object $j$ was placed on the rotating table the same way as
when raw data for modelbase had been collected. Pencil markers on the template
objects helped us remembered the correct arrangement. Imprecision was difficult
to avoid, when we placed the objects manually on the table. This introduces
some noise in the ground truth data.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{media/fiducial-object-3}
        \caption{Relation between fiducial markers and three object coordinate systems}
    \end{center}
\end{figure}

The other two objects $i$ and $k$ were placed upright next to object $i$, with
offset of $-0.15$ and $0.13$ (in metres) in x-direction of fiducial coordinates

\begin{equation}
    \oTvi{i}: \vec{p} \mapsto\ \fTv \pth{ \vec{p} - \begin{pmatrix} 0.15 \\ 0 \\ 0 \end{pmatrix} }
\end{equation}

\begin{equation}
    \oTvi{k}: \vec{p} \mapsto\ \fTv \pth{ \vec{p} + \begin{pmatrix} 0.13 \\ 0 \\ 0 \end{pmatrix} }
\end{equation}

% TODO: give average number of keypoints
<<echo=false>>=
    e_orb = dbGetQuery(con, paste(
        "select distinct avg_keypoints",
        "from response r",
        "join experiment e on r.id = e.response_id",
        "where e.name like '%orb%' and e.test_set='ias_kinect_test_grounded_21'"))
@

<<echo=false>>=
    rm(e_orb)
@

% TODO: future work might involve computation of ground truth in more complex
% configurations


% TODO: figure object-coordinate-syste

\subsection{Test Set}

The test set was made of cluttered scenes of various character. All four
objects are shown on the test scene. Some scenes showed objects in upright
position, but some objects occluding others. Other scenes showed objects in
arbitrary orientation and medium occlusion. Other, yet more difficult scenes,
showed objects in arbitrary orientation and severe occlusion.

The test set data has been collected similarly to the validation set, but
ground truth was not computed. The evaluation on the test set was therefore
restricted to visual analysis.

\section{Experiments}

This section describes the regions in parameter space that have been covered by
experiments. This knowledge is required to interpret data from multiple
experiments. It also presents a selected parameter set that has been found to
work well on the validation set.

\subsection{Searched Parameter Space}


<<echo=FALSE>>=
    # Get the number of experiments in total
    e_all = dbGetQuery(con, paste(
        "select count(e.id) as cnt,",
        "sum(r.train_runtime) as total_train_runtime,",
        "avg(r.train_runtime) as avg_train_runtime,",
        "sum(r.test_runtime) as total_test_runtime,",
        "avg(r.test_runtime) as avg_test_runtime",
        "from experiment e",
        "join response r on e.response_id = r.id"));
    # Get the number of experiments with FAST and RBRIEF
    e_fast_rbrief = dbGetQuery(con, paste(
          "select count(e.id) as cnt",
          "from experiment e",
          "join paramset p on e.paramset_id = p.id",
          "join pms_fe t on p.train_pms_fe_id = t.id",
          "join pms_fe r on p.recog_pms_fe_id = r.id",
          "where e.response_id not null",
          "and t.detector_type = 'FAST'",
          "and t.descriptor_type = 'rBRIEF'",
          "and r.detector_type = 'FAST'",
          "and r.descriptor_type = 'rBRIEF'"))
    # Get the number of experiments with ORB
    e_orb = dbGetQuery(con, paste(
          "select count(e.id) as cnt",
          "from experiment e",
          "join paramset p on e.paramset_id = p.id",
          "join pms_fe t on p.train_pms_fe_id = t.id",
          "join pms_fe r on p.recog_pms_fe_id = r.id",
          "where e.response_id not null",
          "and t.detector_type = 'ORB'",
          "and t.descriptor_type = 'ORB'",
          "and t.extractor_type = 'ORB'",
          "and r.detector_type = 'ORB'",
          "and r.descriptor_type = 'ORB'",
          "and r.extractor_type = 'ORB'"))
@

In total, \Sexpr{e_all$cnt} experiments with different parameter sets have been
conducted. In \Sexpr{e_fast_rbrief$cnt} experiments, we used FAST as a feature
detector and rBRIEF as a feature descriptor. In \Sexpr{e_orb$cnt} experiments,
we chose ORB instead. 

Altogether, the experiments took more than \Sexpr{floor(e_all$total_test_runtime / 3600)}~hours
of wall-time to compute, an average of
\Sexpr{round(e_all$avg_test_runtime / 60, 1)}~minutes per experiment.
\refTable{table:hardsoftware-parameter-optimization} lists the hardware and software
configuration. Note that the hardware is more than six years old.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{lll}
        Component & Type \\
        \hline
        Mainboard & ASUS K8V SE Deluxe \\
        CPU & AMD Athlon64 3000+ S754 \\
        RAM & 1 Gigabyte DDR-RAM & \\
        OS & Ubuntu 10.10 Maverick \\
    \end{tabular}
    \caption{Hard- and software configuration for parameter optimization}
    \label{table:hardsoftware-parameter-optimization}
  \end{center}
\end{table}

\begin{figure}
    \begin{center}
<<fig=true,png=true,echo=false,size=0.5>>=
    r_all = dbGetQuery(con, paste(
        "select succ_rate",
        "from response"));
    hist(r_all$succ_rate, xlab = "Success Rate", main = "Histogram of Success Rates")
@
    \caption{Histogram of success rates in experiments}
    \end{center}
\end{figure}

<<echo=false>>=
    rm(e_all)
    rm(e_fast_rbrief)
    rm(e_orb)
@

\subsection{Observations}

\subsection{Selected Parameter Set}

<<echo=false>>=
    sps = dbGetQuery(con, paste(
          "select e.id as id,",
          # parameters
          "tf.detector_type as detector_type,",
          "tf.descriptor_type as descriptor_type,",
          "tf.extractor_type as extractor_type,",
          "tf.n_features as n_features,",
          "tf.octaves as octaves,",
          "tf.scale_factor as scale_factor,",
          "dm.matcher_type as detect_matcher_type,",
          "dm.knn as detect_knn,",
          "dm.do_ratio_test as detect_do_ratio_test,",
          "dm.ratio_threshold as detect_ratio_threshold,",
          "dg.ransac_iterations_count as detect_ransac_iterations_count,",
          "dg.max_projection_error as detect_max_projection_error,",
          "dg.min_inliers_count as detect_min_inliers_count,",
          "rm.matcher_type as refine_matcher_type,",
          "rm.knn as refine_knn,",
          "rm.do_ratio_test as refine_do_ratio_test,",
          "rm.ratio_threshold as refine_ratio_threshold,",
          "rg.ransac_iterations_count as refine_ransac_iterations_count,",
          "rg.max_projection_error as refine_max_projection_error,",
          "rg.min_inliers_count as refine_min_inliers_count,",
          "g.accept_threshold as accept_threshold,",
          # statistics
          "r.succ_rate as succ_rate,",
          "r.value as value",
          # joins 
          "from experiment e",
          "join response r on e.response_id = r.id",
          "join paramset p on e.paramset_id = p.id",
          "join pms_fe tf on p.train_pms_fe_id = tf.id",
          "join pms_fe rf on p.recog_pms_fe_id = rf.id",
          "join pms_match dm on p.detect_pms_match_id = dm.id",
          "join pms_match rm on p.refine_pms_match_id = rm.id",
          "join pms_guess dg on p.detect_pms_guess_id = dg.id",
          "join pms_guess rg on p.refine_pms_guess_id = rg.id",
          "join pms_clutseg g on p.recog_pms_clutseg_id = g.id",
          "where tf.detector_type = 'ORB'",
          "and tf.descriptor_type = 'ORB'",
          "and tf.extractor_type = 'ORB'",
          "and rf.detector_type = tf.detector_type",
          "and rf.descriptor_type = tf.descriptor_type",
          "and rf.extractor_type = tf.extractor_type",
          "and rf.n_features = tf.n_features",
          "and rf.octaves = tf.octaves",
          "and rf.scale_factor = tf.scale_factor",
          "order by r.succ_rate desc limit 1"))
@

Here we present a parameter set, dubbed SPS, that has been found to work well
on the validation set. It contains values for all configurable system
parameters. \refTable{table:selected-parameters-features} provides values for
feature extraction. We used the same feature extraction parameters for the
modelbase and for the query scenes. We used the default values from OpenCV for
parameters {\tt octaves} and {\tt scale\_factor}.
% TODO: introduce \n_features octaves and scale\_factor in theoretical and implementation part

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{ll}
        Parameter & Value \\
        \hline
        {\tt detector\_type} & \Sexpr{sps$detector_type} \\
        {\tt descriptor\_type} & \Sexpr{sps$descriptor_type} \\
        {\tt extractor\_type} & \Sexpr{sps$extractor_type} \\
        {\tt n\_features} & \Sexpr{sps$n_features} \\
        {\tt octaves} & \Sexpr{sps$octaves} \\
        {\tt scale\_factor} & \Sexpr{sps$scale_factor} \\
    \end{tabular}
    \caption{Selected \clutseg parameters for feature extraction}
    \label{table:selected-parameters-features}
  \end{center}
\end{table}

\refTable{table:sps-match-guess} shows parameter values for the
detection and refinement stages. We used Locality Sensitive Hashing in both
stages to match the binary features produced by Oriented BRIEF.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{ll}
        Parameter & Value \\
        \hline
        {\tt detect\_matcher\_type} & \Sexpr{sps$detect_matcher_type} \\
        {\tt detect\_knn} & \Sexpr{sps$detect_knn} \\
        {\tt detect\_do\_ratio\_test} & \Sexpr{sps$detect_do_ratio_test} \\
        {\tt detect\_ratio\_threshold} & \Sexpr{sps$detect_ratio_threshold} \\
        {\tt detect\_ransac\_iterations\_count} & \Sexpr{sps$detect_ransac_iterations_count} \\
        {\tt detect\_max\_projection\_error} & \Sexpr{sps$detect_max_projection_error} \\
        {\tt detect\_min\_inliers\_count} & \Sexpr{sps$detect_min_inliers_count} \\
        {\tt refine\_matcher\_type} & \Sexpr{sps$refine_matcher_type} \\
        {\tt refine\_knn} & \Sexpr{sps$refine_knn} \\
        {\tt refine\_ransac\_iterations\_count} & \Sexpr{sps$refine_ransac_iterations_count} \\
        {\tt refine\_max\_projection\_error} & \Sexpr{sps$refine_max_projection_error} \\
        {\tt refine\_min\_inliers\_count} & \Sexpr{sps$refine_min_inliers_count} \\
        {\tt accept\_threshold} & \Sexpr{sps$accept_threshold}
    \end{tabular}
    \caption{Selected \clutseg parameters for matching and pose estimation}
    \label{table:sps-match-guess}
  \end{center}
\end{table}

\section{Performance}

Performance of the \clutseg system has been tested on the validation set, on a
separate test set and in a live test on the PR2 robot. This section reviews
performance for a parameter set that worked well on the validation set. It also
covers observations made on a broad range of experiments.

\subsection{Performance on Validation Set}

The \clutseg system correctly located and labeled an object within error bounds
of 3 cm and 20 degrees in rotation in \Sexpr{floor(sps$succ_rate * 100)}\% of
the validation scenes. It got an average scene score of \Sexpr{round(sps$value, 2)}.
The success rate is not predictive, though, as the validation set was used for
optimizing the system. Because ground truth was available, more precise data
can be presented on the validation set than for the test set. 

% TODO: talk about detection stage

\subsection{Performance on Test Set}
\subsection{Performance in Live Test}

\subsection{Oriented BRIEF}

This section discusses performance and characteristics of Oriented BRIEF. It
presents a case, where Oriented BRIEF outperformed SIFT and SURF in speed of
computation. 

We first compared SIFT, SURF and ORB in terms of speed and quantity of features
produced. We used the default parameters specified in OpenCV revision.
\refFigure{figure:sift-surf-orb-benchmark} shows three statistics as a result,
averaged over 21 images. Time was measured in terms of clock cycles. First, the
average time in seconds required to extract features per image. Second, the
average time in milliseconds required per keypoint and image. Third, the
average number of keypoints extracted per image.

\begin{figure}
    \begin{center}
<<echo=false>>=
    png("sift-surf-orb-benchmark.png", 800, 400)
    layout(matrix(c(1, 2, 3), 1, 3))

    col = c("orange", "darkgreen", "blue", "lightblue") 
    par(cex=1.2)
    par(cex.main=0.9)

    d=as.data.frame(read.table("../media/sift-surf-orb-benchmark.txt", header=TRUE))
    barplot(d$time_img, names.arg=d$extractor, main="Time per image",    ylab="time [s]",  ylim=c(0, 0.8), col=col)
    barplot(d$time_kpt, names.arg=d$extractor, main="Time per keypoint", ylab="time [ms]", ylim=c(0, 0.5), col=col)
    barplot(d$quantity, names.arg=d$extractor, main="Keypoint quantity", ylab="quantity", ylim=c(0, 2000), col=col)
    invisible(dev.off())
@
     \includegraphics[width=\textwidth]{sift-surf-orb-benchmark}
     \caption{Comparison of SIFT, SURF and ORB in times of speed and quantity}
     \label{figure:sift-surf-orb-benchmark}
    \end{center}
\end{figure}

Oriented BRIEF ran about \Sexpr{floor(d$time_img[1]/d$time_img[3])} times faster than
SIFT, and about \Sexpr{floor(d$time_img[2]/d$time_img[3])} times faster than SURF.


\section{Issues}

% \subsection{Prior Assumption about Orientation}

% many presentations of object recognition systems seem to make the prior assumption of objects appearing in upright pose.
% the performance on the test set which showed objects in arbitrary orientation was below expectations
% the model features were extracted by a scan of the object from positions on a circle
% the query features are observed from a position on a sphere around the object
% features are rotation-invariant only to rotation of objects around an axis parallel to the optical axis 
% the number of matches significantly reduced on the images with objects in arbitrary orientation

% Many presentations of object recognition systems in media and literature seem
% to make the prior assumption that objects in query scenes are oriented in
% upright position. Our test set shows scenes with objects not only in clutter,
% but also in arbitrary orientation.  This adds extra difficulty. \clutseg had
% problems with locating an object at least in TODO of TODO test scenes. When we
% collected raw data for the modelbase, we made an implicit prior assumption
% about the orientation of objects in query scenes. We scanned the template
% object from several points on a circle around the object. Yet, the query scenes
% show instances from arbitrary points on spheres around them.

% Rotation-invariant feature detectors cope with 3D rotation of objects around an
% axis parallel to the optical axis. SIFT features still operate up to 20 degrees
% of rotation around an axis parallel to the image plane \cite{Lowe1999}. % TODO: confirm this

\subsection{Incomplete model}

The bottom of the template object is facing the rotating table and is invisible
to the camera from all viewpoints. This does not matter as long as instances of
these template objects are standing upright. This does matter when instances occur
in arbitrary orientation.

\subsection{Repetitions in texture}

The system had difficulties to recognize {\it icedtea} when shown from a
certain angle.  It turned out that {\it icedtea} has smallprint almost all over
its backside. The same set of letters repeatably appear over a large area,
which probably makes it hard to obtain informative features.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.2\textwidth]{media/icedtea_smallprint}
        \caption[Repetitive texture on template object]{Repetitive texture on template object {\it icedtea}}
    \end{center}
\end{figure}

\subsection{Consensus for Distant Pose Estimates}

Generally, the system relies on the size of the consensus set to be a strong
indicator for the goodness of an estimate. We observed that in one experiment
and scene, \clutseg estimated a distance of 88 metres between {\it icedtea} and
the camera. This bad estimate was almost two orders of magnitude away from
ground truth. Yet it still had support by 14 inliers, enough in this scene to
make it the best-ranked guess.
% RANSAC and the Maximum Projection Error
% Perspective Projection and RANSAC
% RANSAC and Degenerate Projections
% Degeneration of Projections in Perspective
% => it is not a deficiency in the RANSAC paradigm
% => it is a deficiency in the error measure for fitting
% Projection error and Distant Pose Estimates
% Consensus for Distant Pose Estimates

An explanation for the large consensus set is that the projection error is a
measure on the image plane. Given a scene, consider any correspondence $c_i =
(q_i, p_i) \in \real^2 \times \real^3 $ between a query feature with
keypoint $q_i$ and a model feature with 3D point $p_i$. Let $\oTvh$ be a
pose estimate. Let $U: \real^3 \to \real^2$ denote perspective
projection for the calibrated camera. Define the projection $\hat{q}_i$ of the
model point $p_i$, aligned to the scene by $\oTvh$, as

\begin{equation}
    \hat{q}_i := U\pth{ \ \oTvh \pth{ p_i}\ }
\end{equation}

The correspondence $c_i$ is designated inlier if and only if the L2-distance
between query keypoint and the projection of the model point aligned to the
scene is less than a threshold $r$, precisely 

% TODO: include predicate that z-coordinate (depth) of T(p_i) must be greater than zero
% this is a geometrical constraint, see pnp_ransac.cpp

\begin{equation}
    c_i\ \text{ is an inlier w.r.t. }\ \oTvh\quad \Leftrightarrow \quad \normtwo{\ q_i - \hat{q}_i\ } \le r
    \label{equation:inlier-definition}
\end{equation}

Now, let us model the scenario, where a distant pose estimate was generated
for an object that is close to the camera in truth. Let $\oTv$ denote
ground truth for $\oTvh$. 
 
    % \normtwo{\oTvh\left(\vec{0}\right) - \oTv\left(\vec{0}\right)}
\begin{equation}
    e_t \pth{ \oTvh,\ \oTv } \geq l
\end{equation}

Choose $l$ large enough that the projection of the aligned model spans only one
pixel $\hat{q}$ on the digital image plane. For the $n$ correspondences for this object, this means

\begin{equation}
    \forall{i \in \brt{n}}: \hat{q}_i \approx \hat{q}
    \label{equation:same-point}
\end{equation} 

Plugging \refEquation{equation:same-point} into \refEquation{equation:inlier-definition},
we obtain an inlier criteria for the bad estimate

\begin{equation}
    \forall{i \in \brt{n}}:\quad c_i\ \text{ is an inlier w.r.t. }\ \oTvh\quad \Leftrightarrow \quad \normtwo{\ q_i - \hat{q}\ } \le r
\end{equation}

All correspondences within a circle of radius $r$ around $\hat{q}$ are
designated inliers. If this circle contains many keypoints on the query image,
the resulting consensus set of the pose estimate might grow larger than
expected. The effect can possibly be neglected if radius $r$ is small. In fact,
in the experiment exhibiting the phenomenon, the radius was set to $r = 12$
(pixels). Note that this radius is equivalent to the parameter {\tt
max\_projection\_error}.

There are at least three hypothetical solutions to this issue. One might set
parameter {\tt max\_projection\_error} to a low value.  The bad pose estimates
could be pruned away by priorly assuming the objects to be within a certain
range. Finally, radius $r$ could be replaced by a function that depends on the
estimated distance $\normtwo{\oTvh\pth{\vec{0}}}$.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{media/ransac-max-projection-error-deficiency}
        \caption[Schematic explanation of distant pose estimate with many inliers]{Far-away pose estimate can result in many inliers}
    \end{center}
\end{figure}

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.6\textwidth]{media/ransac-max-projection-error-deficiency-sample}
        \caption[Sample of distant pose estimate with many inliers]{On a test scene, the badly aligned pose still results in many inliers}
    \end{center}
\end{figure}




