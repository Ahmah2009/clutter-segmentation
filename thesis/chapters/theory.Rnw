This chapter provides the theoretical foundations for building the 3D object
recognition system as presented in \refChapter{chapter:implementation}. It
provides a review of required mathematical tools and the basics of multi-view
3D object recognition. It covers the description of images using local
features with state-of-the-art techniques, how to quickly establish
correspondences between known and unseen images, and finally how to estimate
poses in presence of noise and mismatches in the correspondences.  Also, this
chapter shows how the implemented system relates to the mathematical framework
of classifiers and estimators, which is helpful for the evaluation given in
\refChapter{chapter:evaluation}.

\section{Prerequisites in Geometry}

This section establishs mathematical tools and notation that are used
throughout this work.

% TODO: talk about pin-hole camera model assumption

\subsection{Rigid Transformations}

% TODO: reference to definition of rigid transformations

A {\it rigid transformation} is defined as combination of an orthogonal
transformation and translation in Euclidean space. A {\it proper rigid
transformation} additionally requires the orthogonal transformation to be a
rotation. In three dimensions, a proper rigid transformation can be represented
by a rotation matrix $R \in \real^3$ and a translation vector $\vec{t} \in
\real^3$.

\begin{equation}
    T: \real^3 \to \real^3, \vec{p} \mapsto R\vec{p} + \vec{t}
\end{equation}

The inverse transformation is given by
 
\begin{equation}
    T^{-1}: \real^3 \to \real^3, \vec{p} \mapsto R^{-1}(\vec{p} - \vec{t})
\end{equation}

A proper rigid transformation, abbreviated by PRT in the following, can be used
to describe a rigid motion of an object with respect to a reference coordinate
system. In physics, this is called active transformation, and explains the 
relationship between two different points in the same coordinate system.


Another interpretation is to see a PRT as a passive transformation. The PRT
transforms vector coordinates between two different coordinate system. This
explains how the very same point is seen from two different coordinate systems.
The passive interpretation is used throughout this work.

\subsection{Object-View Transformations}

Coordinates of points in 3D space can be specified with respect to any
coordinate system. In this work, two different kinds of coordinate systems are
used. The first kind is attached to the camera and is called {\it view
coordinate system}. The second kind is attached to an object or its model and
is called {\it object coordinate system}. The PRT between object coordinates
and view coordinates is called {\it object-view transformation} and is denoted
sy $\oTv$. Subsequently, the inverse PRT is called {\it view-object
transformation} and denoted by $\vTo$

\begin{equation}
  \vTo = \pth{ \oTv }^{-1}
\end{equation}

Here are two examples to show the usefulness of these definitions. To compute
the distance between camera and the object origin, use

\begin{equation}
d = \normtwo{\ \oTv \pth{ \vec{0} }\ }
\end{equation}

Compute the distance $d_{12}$ between the two objects $1$ and $2$ in a scene by

\begin{equation}
d_{12} = \normtwo{\ \oTvi{1} \pth{ \vec{0} } -\ \oTvi{2} \pth{ \vec{0}} \ }
\end{equation}

\begin{figure}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{media/coordinate-frames}
    \end{center}
    \caption{Transformation between object and view coordinate systems}
    \label{figure:coordinate-frames}
\end{figure}

Note that there exists a coordinate system for every object and view.  When
talking in general about object recognition, it is often more intuitive to talk
about the {\em pose} of an object. If precise wording is required, it is
preferred to talk about the object-view transformation that brings object
coordinates into view coordinates for a specific view-object pair.

% \subsection{Perspective-N-Point Problem}

\section{Object Recognition}

Object recognition is the problem of labeling or locating objects in a query
scene. This section describes relevant mathematics and techniques used in 3D
object recognition using local 2D features.
% TODO: provide references to other approaches, or different subxxx of Object Recognition (CAD based, etc.)
% instead of just talking about doing it
References to other approaches,
such as with true 3D features or object recognition using parametric models are
provided but, being out of scope, not covered in detail.

% \subsubsection{Applications of Object Recognition}
% Mars mission
% Flood detection
% Earthquake detection
% Industrial quality control

\subsection{Local Features}

In object recognition, the question arises on how to generate description of
objects and find candidates that match the description in the query scene.  We
can distinguish between local and global approaches. An example for a global
approach is template matching. Yet, in scenes with occlusion, local methods are
clearly preferred. Object recognition using local features is based on the
detection of keypoints in an image. There is a multitude of different methods
available, but they all aim at choosing keypoints that are repeatable for the
same object over different images. A feature consists of a keypoint and a
feature descriptor that captures information about its local image
neighborhood. Feature descriptors are often vectors and their distance in terms
of a vector norm provides a means of measuring similarity between two features. 
% TODO: cite, maybe Lowe

\subsubsection{Learning Models from a Single View}

Lowe describes an approach to object recognition using local features where
models are learnt from a single view of the template object \cite{Lowe1999}.
Given a query image, local features are extracted and matched against the model.
This approach works well, if the object in the query scene has approximately
the same orientation as in the template view. Lowe reports that using SIFT
features, discussed in \refSection{subsection:SIFT}, it is possible
to recognize objects up to a 20 degree rotation \cite{Lowe1999}. Unfortunately,
considering only a single view is insufficient if query scenes show objects in
arbitrary orientation.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.4\textwidth]{media/single-view-or}
        \caption{Recognition with models learned from a single view}
    \end{center}
\end{figure}

\subsubsection{Learning Models from Multiple Views}
\label{subsubsection:multi-view-or}

The single-view scheme can be extended to allow for recognizing objects in
arbitrary orientations. For this, a description of a template object is based
on local features extracted from multiple views of the object. Local features
from a query scene are matched against the model that incorporates features
recorded from different viewpoints.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{media/multi-view-or}
        \caption{Recognition with models learned from multiple views}
    \end{center}
\end{figure}

Notably, the strategy on how to choose the template views in the first place
has an impact on on both memory and computation time, and has been subject of
research. % TODO: cite Section

Learning models from multiple views is basis of this work. 
% ections \ref{subsection:feature-detectors} and
% \ref{subsection:feature-descriptors} show how to extract a description of the
%query image and the template views. Section \ref{subsection:feature-matching}
% presents methods for fast establishment of correspondences, and section
% \ref{subsection:pose-estimation} shows how to infer on object poses. 

\subsection{Feature Detectors}
\label{subsection:feature-detectors}

% TODO: strip section about feature detectors down to the bare essence

A feature detector designates keypoints on an image.  Some feature detector
algorithms assign orientations to keypoints. Detectors can be roughly
partitioned into three groups. First, there are {\it corner detectors}, such as
Harris and SUSAN, that select points with high curvature. Second, {\it blob
detectors} such as SURF or DoG look for points that are surrounded by all
brighter or darker pixels. Third, there are {\it region detectors}, such as
MSER, which extract features by finding image regions.

There is generally no such thing as the best feature detector. As often in
engineering, all we can do is making the best choice for the task at hand.  An
ideal feature detector though, should be {\it efficient} and it should find
features that are {\it repeatable}, {\it informative}, {\it local}, and {\it
accurate} \cite{Tuytelaars2007}.

Repeatability describes the chances that the same point on an object shown from
two different viewpoints and viewing conditions is detected on both images.
Thus, repeatability is very important when trying to find correspondences
between two images. A feature detector designates keypoints after examining the
local image neighborhood. These neighborhoods can for example be deformed by
noise, image discretization, a change of viewpoint or a change in lighting
conditions \cite{Tuytelaars2007}. These deformations have to be addressed by a
feature detector to achieve repeatability.

An informative feature belongs to an image neighborhood that shows high
variation in intensity. If not, the features would be difficult to distinguish.
For example, we would not like to have features detected on non-textured
background or areas that carry little information.

A local feature only carries information about a small neighborhood. That
ensures that it is still repeatable despite of occlusion. A small neighborhood
also leads to less informative features, enforcing a trade-off between locality
and informativeness.

\subsection{Feature Descriptors}
\label{subsection:feature-descriptors}

A feature descriptor generally computes a vector that describes the local
neighborhood of a keypoint. Ideally, two corresponding keypoints should have
very similar descriptor vectors, such that correspondences be found by
nearest neighbor search. A feature descriptor that allows for efficient
matching is BRIEF \cite{Calonder2010}. 

\begin{table}
    \begin{center}
        \begin{tabular}{lcccc}
            Detector & Multi-scale & Oriented Keypoints \\
            \hline
            SIFT & yes & yes \\
            SURF & yes & yes \\
            FAST & no & no \\
            ORB & yes & yes 
        \end{tabular}
    \end{center}
    \caption{Properties of selected feature detectors}
\end{table}

\subsubsection{SIFT}
\label{subsection:SIFT}

The scale-invariant feature transform became popular in object recognition.  It
is desirable that images of objects at different scales produce more or less
the same features. SIFT achieves this by selecting keypoints from a scale-space
pyramid in order to generate scale-invariant features. SIFT is both a feature
detector and descriptor. Lowe further described a system that lets
correspondences vote in Hough space, which is dimensioned by location,
orientation and scale \cite{Lowe1999}. When bins contain more than a predefined
number of votes, the object is said to be recognized at this pose. SIFT has
also been successfully applied to panorama stitching \cite{Brown2006}. 

\subsubsection{Oriented BRIEF}

{\it Oriented BRIEF} or in short {\it ORB} is a combination of the FAST feature
detector and the BRIEF feature descriptor, plus modifications that render the
extracted features orientation-invariant. It was designed to outperform SIFT
and SURF at least in terms of computation time. An implementation is already
available in the development branch (trunk) of OpenCV, a corresponding paper is
expected to be published at ICCV 2011.

FAST is a feature detector based on analyzing Bresenham circles around
candidate keypoints. Generated features are neither orientation nor rotation
invariant.

BRIEF is a binary feature descriptor, which is sensitive to rotation. It
computes a bit string for the image neighborhood of a given keypoint. Each bit
stores the result of the comparison of pixel intensities between two 2D points
in the neighborhood, which are called {\it test locations}. The test locations
were randomly sampled from a 2D Gaussian distribution and together form a {\it
test pattern}. Finding nearest neighbors of binary descriptors according to
Hamming distance is faster than for real-valued features using the L2 norm.

% TODO: include image for test locations from orb\_patterns in opencv trunk

ORB computes image moments to define orientation for keypoints detected by
FAST. Image moments of order $(p + q)$ are defined for a 2D digital image $f$
of dimension $MxN$ as

\begin{equation}
    m_{pq} = \sum_{x=0}^{M-1} \sum_{y=0}^{N-1} x^p y^q f(x, y)
\end{equation}

ORB computes moments $m_{01}$, and $m_{10}$ for neighborhoods $f$ of the
keypoints and designates orientations $\alpha$ as

\begin{equation}
    \alpha = \arctan \frac{m_{01}}{m_{10}}
\end{equation}

Consider the vector $\pth{ m_{01}, m_{10} }^T$. The angle of the vector
in becomes the orientation of the keypoint. If $f$ were a probability
distribution, then this vector would contain the means of the marginal
distributions of x and y, respectively.

% TODO: verify correlation problem and relation to descriptor entropy

A problem arises, when rotating the BRIEF test pattern according to the FAST
keypoint orientation. There is correlation between the image moments and the
BRIEF vectors obtained from rotated test patterns. Such correlation reduces
entropy of the BRIEF descriptor. The solution is to compute test patterns that
have only little correlation.

A more detailed theoretical treatment of ORB should be deferred until the
publication of the corresponding authoritative paper. The description about ORB
given here was based on knowledge extracted from a presentation at ICRA 2011 in
Shanghai, the ORB implementation from OpenCV trunk, and inference from FAST and
BRIEF. It is thus to be handled with care.

\subsection{Feature Matching}
\label{subsection:feature-matching}

% TODO: speed-up must be backed up by a benchmark in the evaluation section

In object recognition using local features, correspondences between known views
of the object and the query image can be established by matching features from
the query images to the features from the model. Computation time depends on
the size of the descriptor vector, the nearest neighbor algorithm and the used
distance metric. Great speed-ups can be achieved by weakening the requirements
and allowing for approximate nearest neighbors. Computing the Hamming distance
between binary features is faster than the L2-norm for real-valued feature
descriptors \cite{Calonder2010}. This holds especially true with AMD's and
Intel's {\tt POPCNT} instruction. % TODO: cite

% TODO: can also be adapted to match binary features such as BRIEF. 

\subsubsection{Locality Sensitive Hashing}

% TODO: consistent use of uppercase and lowercase in algorithm names

SIFT, ORB and other feature detectors and descriptors are used to describe an
image in terms of local features. Nearest neighbor matching can then find
correspondences between two feature sets. Locality Sensitive Hashing (LSH)
quickly finds nearest neighbors, and trades correctness in favor of a reduction
in computation time. Not always does it return the absolute nearest neighbor.
Locality Sensitive Hashing is a randomized algorithm that performs multiple
scalar projections of a high-dimensional vector and reasons that the scalar
projections are similar for similar input vectors. It can be extended to find
nearest neighbors for binary features. LSH has applications in object
recognition, image retrieval, music rerieval, identification of duplicates
\cite{Slaney2008}.

\subsection{Pose Estimation}
\label{subsection:pose-estimation}

% TODO: relation to PnP problem
% TODO: I will not show how to solve this problem


This section shows that estimating the pose of an object can be reduced to the
perspective-$n$-point (PnP) problem, which can be solved by RANSAC in the
presence of outliers. There exist many different terms for the pose estimation
problem in literature. 

% Estimating the pose of an object in a scene is nothing more than estimating the
% object-view transformation, as the latter uniquely defines where each 3D point
% belonging to the object is located with respect to the view coordinate frame,
% i.e. the camera.

% \subsubsection{Perspective-n-Point Problem}

% First, observe that the {\it Location Determination Problem}, as stated in
% \cite{Fischler1981}, is equivalent to the problem of pose estimation. Given

\subsubsection{RANSAC}

RANSAC stands for {\it RANdom SAmple Consensus} and is a randomized algorithm
that fits a parametrized model to given data, which is robust to outliers.  The
pose estimation problem in this context can be reduced to the location
determination problem or the equivalent PnP problem \cite{Fischler1981}.
Generally, this algorithm randomly chooses a small set of data points that
fully determine the model parameters.  Then it forms a consensus set of {\it
inliers}, that is, all data points that are consistent with the model up to a
certain margin of error. RANSAC constructs a fixed number of such consensus
sets and then chooses the model with the largest consensus to compute the final
estimate.

Pose estimation using RANSAC can be categorized as a method of {\it pose
consistency} or {\it alignment}, where first a pose is hypothesized, and
then the hypothesized pose is verified for support by the remaining data
\cite{Forsyth2003}.

% There are six degrees of freedom for an object to be located and oriented in
% space, three for translation and three for orientation. 
% TODO: how many projected points do we need
% Three 2D points in perspective projection are INsufficient to fully determine a pose.
% If there were no noise in the set of 2D correspondences, we would be able to
% find poses of an object by simply taking any three correspondences belonging to
% this object. Unfortunately, image noise, discretization effects and approximate
% nearest neighbors asks for a more sophisticated approach that incorporates
% strategies to deal with noise and inconsistency. RANSAC iteratively selects
% three correspondences to compute pose estimates. The 3D model of the template
% object aligned to the estimated pose is then projected on the query image plane
% to compute the projection error and the consensus set. The size of the
% resulting consensus sets plays an important role in this work.

\section{Machine Learning}

This section shows how object recognition relates to machine learning,
especially to classification and estimation theory. It also covers the basics
of a machine learning experiment, which are required for optimizing parameters
and understanding the strengths and weaknesses of an object recognition system.
When evaluating the system, a measure of {\it goodness} needs to be defined.

\subsection{Classification}

The purpose of this section is to show the difficulties in formulating the
problem statement in terms of classification.  Assume, there were no interest
in the pose of an object. Assume that objects are unique on an image. Then
object recognition can be seen as a {\it multi-label classification} problem,
as described in \cite{Tsoumakas2007}.  An image is described by a set of
labels, and symmetrically, the classification result is a set of labels that
predict which objects are on the image.  Receiver operating characteristics
(ROC) can be used to evaluate standard two-class classifiers
\cite{Fawcett2006}. Standard measures of goodness are the true and false
positive rates that can be computed from confusion matrices. In case of
multi-label classification, it is possible to define similar metrics
\cite{Tsoumakas2007}.  Unfortunately, even if object pose were not an issue,
assuming objects to appear uniquely on an image might be unrealistic. For
example, a shopping bag as in the application scenario of this work is best
represented by a multiset, and not by a multi-label set.  Given a finite set of
labels, the number of multi-label sets is finite, the number of multisets is
not. A problem with infinite number of decisions is more a problem of
estimation than classification \cite{Melsa1978}. For example, trying to define
a specificity metric on it is difficult, since the number of negatives in a
multi-set is generally infinite.

\subsection{Estimation}

Estimation can be regarded as the continuous extension of classification
\cite{Melsa1978}. Clearly, if it is known which objects are on a scene, object
recognition can be seen as an estimation problem. For each object on the scene,
it is the task of the object recognition system to estimate the six degrees of
freedom. The squared or absolute error of an estimate are possible measures of
goodness that are generally available for estimators.

\subsection{Recognition}

The recognition problem, where objects have to be both correctly labeled and
its location determined therefore neither fits a pure classification nor a pure
estimation problem. In this work, measures of ``goodness'' see the object
recognition system as a combination of classifier and estimator. Estimation
``goodness'' is conditioned to the classifier making the right choice, and
scenes that contain multiple instances of the same object are not considered.

\subsection{Parameter Optimization}

In the presented system, the algorithms for detecting, describing and matching
features are parametrized, as well as the pose estimation and the final
decision process. This leads to the question on how to find a parameter set
such that the system performs well in the application scenario.

\subsubsection{Validation Set}

Choosing parameters for parametrized algorithms is a very general problem in
computer science. Solutions for these problems include adaptive methods that
estimate the parameters directly from the data. For example, in adaptive
numerical quadrature, grid points are selected according to an error estimate
gained directly from function samples. Optimization techniques find a parameter
set that optimizes performance on a validation set, where ground truth is
avaible. This optimization technique is well-known in machine learning.

\subsubsection{Test Set}

It is interesting to estimate the error which a classifier or estimator is
expected to make when working on unseen instances. This is the generalization
error which cannot be estimated on the validation set. Instead a separate test
set is required to estimate the generalization error \cite{Alpaydin2010}.
% , and is related to the problem of assuming appropriate priors
% in Bayesian inference.

