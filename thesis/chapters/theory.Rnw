% \chapter{Theory}
% \label{chapter:theory}

This chapter provides the theoretical foundations for building the 3-d object
recognition system, as will be presented in chapter
\ref{chapter:implementation}. It provides the basics of multi-view 3-d object
recognition, and a short review of required mathematical tools. It covers the
description of images using local features with state-of-the art techniques,
how to quickly establish correspondences between known and unseen images, and
finally how to estimate poses under uncertainty and noise using. Also, this
chapter shows how the implemented system relates to the mathematical framework
of classifiers and estimators, which is helpful for the evaluation given in
chapter \ref{chapter:evaluation}.

\section{Object Recognition}

Object recognition is the problem of finding or labeling objects in a query
scene. This section describes relevant mathematics and techniques used in
multi-view 3-d object recognition using local 2-d features. References to other
approaches, such as with true 3-d features or object recognition using parametric
models are provided but, being out of scope, not covered in detail.

% \subsubsection{Applications of Object Recognition}
% Mars mission
% Flood detection
% Earthquake detection
% Industrial quality control

\subsection{Multi-View 3-d Object Recognition}

In multi-view 3-d object recognition, we recognize an object in a query scene
by matching it against multiple views of the template object, which have been
recorded from different viewpoints.

It is helpful to first consider the case of single-view 3-d object recognition,
where a description of a template object is extracted from a single view. An example
for such an approach can be found in \cite{Lowe1999}. This approach works well 

\begin{figure}
    \includegraphics[width=0.5\textwidth]{media/single-view-or}
    \caption{Single-view 3-d object recognition scheme}
\end{figure}


\subsection{Local Features}

Object recognition using local features is based on the detection of keypoints
in an image. There is a multitude of different methods available, but they all
aim at choosing keypoints that are repeatable for the same object over
different images. A feature consists of a keypoint and a feature descriptor
captures information about the keypoint and the local image neighborhood.  The
distance between two feature descriptors provides a means of measuring
similarity between two features. 

\subsubsection{Feature Detectors}

A feature detector is an algorithm that designates keypoints on an image.  Some
feature detectors assign orientations to keypoints. Detectors can be roughly
partitioned into three groups. First, there are {\it corner detectors}, such as
Harris and SUSAN, that select points with high curvature. Second, {\it blob
detectors} such as SURF or DoG look for points that are surrounded by all
brighter or darker pixels.  Third, there are {\it region detectors}, such as
MSER, which extract features by finding image regions.

There is generally no such thing as the best feature detector. As often in
engineering, all we can do is making the best choice for the task at hand.  An
ideal feature detector though, should be {\it efficient} and it should find
features that are {\it repeatable}, {\it informative}, {\it local}, and {\it
accurate} \cite{Tuytelaars2007}.

Repeatability describes the chances that the same point on an object shown from
two different viewpoints and viewing conditions is detected on both images.
Thus, repeatability is very important when trying to find correspondences
between two images. A feature detector designates keypoints after examining the
local image neighborhood. These neighborhoods can for example be deformed by
noise, image discretization, a change of viewpoint or a change in lighting
conditions \cite{Tuytelaars2007}. These deformations have to be addressed by a
feature detector to achieve repeatability.

An informative feature belongs to an image neighborhood that shows high
variation in intensity. If not, the features would be difficult to distinguish.
For example, we would not like to have features detected on non-textured
background or areas that carry little information.

A local feature only carries information about a small neighborhood. That
ensures that it is still repeatable despite of occlusion. A small neighborhood
also leads to less informative features, enforcing a trade-off between locality
and informativeness.


\subsubsection{Feature Descriptors}

A feature descriptor generally computes a vector that describes the local
neighborhood of a keypoint. Ideally, two corresponding keypoints should have
very similar descriptor vectors, such that correspondences be found by
nearest neighbor search. A feature descriptor that allows for efficient
matching is BRIEF \cite{Calonder2010}. 

\begin{table}
    \begin{center}
        \begin{tabular}{lcccc}
            Detector & Multi-scale & Oriented Keypoints \\
            \hline
            SIFT & yes & yes \\
            SURF & yes & yes \\
            FAST & no & no \\
            ORB & yes & yes 
        \end{tabular}
    \end{center}
    \caption{Properties of selected feature detectors}
\end{table}

\subsection{Feature Matching}

In multi-view object recognition using local features, correspondences between
known views of the object and the query image can be established by matching
features from the query images to the features from the model. Computation time
depends on the size of the descriptor vector, the nearest neighbor algorithm
and the used distance metric. Great speed-ups can be achieved by weakening the
requirements and allowing for approximate nearest neighbors. Computing the
distance between binary features is faster than with real-valued feature
descriptors.

\subsection{SIFT}

The scale-invariant feature transform became popular in object recognition.  It
is desirable that images of objects at different scales produce more or less
the same features. SIFT achieves this by selecting keypoints from a scale-space
pyramid in order to generate scale-invariant features. SIFT is both a feature
detector and descriptor. Lowe further described a system that lets
correspondences vote in a Hough space, which is dimensioned by location,
orientation and scale \cite{Lowe1999}. When bins contain more than a predefined
number of votes, the object is said to be recognized at this pose.  SIFT was
successfully applied to TODO.

\subsection{Oriented BRIEF}

{\it Oriented BRIEF} or in short {\it ORB} is a combination of the FAST feature
detector and the BRIEF feature descriptor, plus modifications that render the
extracted features orientation-invariant. It was designed to outperform SIFT
and SURF at least in terms of computation time. An implementation is already
available in the development branch (trunk) of OpenCV, a corresponding paper is
expected to be published at ICCV 2011.

FAST is a feature detector based on analyzing Bresenham circles around
candidate keypoints. Generated features are neither orientation nor rotation
invariant.

BRIEF is a binary feature descriptor, which is sensitive to rotation. It
computes a bit string for the image neighborhood of a given keypoint. Each bit
stores the result of the comparison of pixel intensities between two 2-d points
in the neighborhood, which are called {\it test locations}. The test locations
were randomly sampled from a 2-d Gaussian distribution and together form a {\it
test pattern}. Finding nearest neighbors of binary descriptors according to
Hamming distance is faster than for real-valued features using the L2 norm.

% TODO: include image for test locations from orb\_patterns in opencv trunk

ORB computes image moments to define orientation for keypoints detected by
FAST. Image moments of order $(p + q)$ are defined for a 2-d digital image $f$
of dimension $MxN$ as

\begin{equation}
    m_{pq} = \sum_{x=0}^{M-1} \sum_{y=0}^{N-1} x^p y^q f(x, y)
\end{equation}

ORB computes moments $m_{01}$, and $m_{10}$ for neighborhoods $f$ of the
keypoints and designates orientations $\alpha$ as

\begin{equation}
    \alpha = \arctan \frac{m_{01}}{m_{10}}
\end{equation}

Consider the vector $\left( m_{01}, m_{10}\right)^T$. The angle of the vector
becomes the orientation of the keypoint. If $f$ were a probability
distribution, then this vector would contain the means of the marginal
distributions of x and y, respectively.

A problem arises, when rotating the BRIEF test pattern according to the FAST
keypoint orientation. There is correlation between the image moments and the
BRIEF vectors obtained from rotated test patterns. Such correlation reduces
entropy of the BRIEF descriptor. The solution is to compute test patterns that
have only little correlation.

\subsection{Locality Sensitive Hashing}

% TODO: consistent use of uppercase and lowercase in algorithm names

SIFT, ORB and other feature detectors and descriptors are used to describe an
image in terms of local features. Nearest neighbor matching can then find
correspondences between two feature sets. Locality Sensitive Hashing (LSH)
quickly finds nearest neighbors, and trades correctness in favor of a reduction
in computation time. Not always does it return the absolute nearest neighbor.
Locality Sensitive Hashing is a randomized algorithm that performs multiple
scalar projections of a high-dimensional vector and reasons that the scalar
projections are similar for similar input vectors. It can be extended to find
nearest neighbors for binary features. LSH has applications in object
recognition, image retrieval, music rerieval, identification of duplicates
\cite{Slaney2008}.

\section{Pose Estimation}

\subsection{Rigid Transformations}

A {\it rigid transformation} is defined as combination of an orthogonal
transformation and translation in Euclidean space. A {\it proper rigid
transformation} requires the orthogonal transformation to be a rotation. In
three dimensions, a proper rigid transformation can be represented by a
rotation matrix $R \in \mathbb{R}^3$ and a translation vector $t \in
\mathbb{R}^3$.

\begin{equation}
    T: \mathbb{R}^3 \to \mathbb{R}^3, v \mapsto Rv + t
\end{equation}

A proper rigid transformation can be used to describe a rigid motion of an
object. It can also be used to transform the coordinate systems, whilst objects
in the scene remain unchanged with respect to a fixed world frame. The latter
interpretation is prevalent in this work.


\subsection{Hough Transform}

\subsection{RANSAC}

RANSAC stands for {\it RANdom SAmple Consensus} and is a randomized algorithm
that fits a parametrized model to given data. It is robust to outliers, and
therefore a replacement for least-squares fitting in applications that have to
deal with lots of noise. RANSAC randomly chooses the minimal set of data points
that fully determine the model parameters. Then it forms a consensus set out of
all data points that are consistent with the model up to a certain margin of
error. RANSAC constructs a fixed number of such consensus sets and then chooses 
the model with the largest consensus as an estimate.

One application of RANSAC is pose estimation. There are six degrees of freedom
for an object to be located and oriented in space, three for translation and
three for orientation. Three points in a 2-d perspective projection are
necessary and sufficient to determine a pose. If there were no noise in the set
of 2-d correspondences, we would be able to find poses of an object by simply
taking any three correspondences belonging to this object. Unfortunately, image
noise, discretization effects and approximate nearest neighbors asks for a more
sophisticated approach that incorporates strategies to deal with noise and
inconsistency. RANSAC iteratively selects three correspondences to compute pose
estimates. The 3-d model of the template object at the estimated pose is then
back-projected on the query image to compute the projection error and the
consensus set.

\section{Machine Learning}

\subsection{Classification}

\subsection{Estimation}

\subsection{Recognition}

\subsection{Parameter Optimization}


