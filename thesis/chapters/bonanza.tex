

How tod\_training works

This is a summary of how tod\_training works, describing input, steps,
algorithms and the output of the training process.

See also http://answers.ros.org/question/289/does-tod\_training-in-object\_recognition-require

See also http://answers.ros.org/question/505/purpose-of-pose-estimations-in-tod\_detecting

See also http://answers.ros.org/question/504/physical-setup-for-collecting-tod\_training-data

% TODO: Learning NOT training

% TODO: replace "training subject" with "template"
% TODO: replace "training" with "learning"
% TODO: find name for "estimated pose" 
% TODO: find name for "guessed pose" 
% TODO: can we say "guess" or better "estimate" or neither?
\begin{description}
    \item[training]the process of creating a training base
    \item[training base] the database that contains the learned classifier for recognition
    \item[training pipeline] the different stages involved in the training process
    \item[training subject] the object to learn to be recognized by the system
    \item[view] shows a training subject in a certain pose
    \item[training image] a per-view 2d image that shows the training subject in a specific pose
    \item[training cloud] a per-view 3d point cloud sampled from the training subject
    \item[estimated pose] an estimate of translation and rotation of the training subject in a view
    \item[masking] extracting the region of interest in the training image
    \item[feature detection] the selection of keypoints in an image
    \item[feature extraction] the generation of feature descriptors for keypoints
    \item[keypoint] a (hopefully) characteristic point in an image
    \item[keypoint descriptor] a vector that describes the characteristics of a keypoint
    \item[training keypoint] a keypoint in a training image
    \item[query image] a test image on which objects shall be detected
    \item[query keypoint] a keypoint in the query image
    \item[recognition] the process of detecting objects in an image
    \item[match] a pair of a query keypoint and a training keypoint with similar descriptors
    \item[guess] an estimate that an object is shown on the query image, including pose
    \item[guessed pose] the fitted pose estimate for an object detected in a query image
    \item[inlier] A matching query keypoint that supports the guessed pose.
\end{description}

\section*{Overview over training pipeline}

\includegraphics[width=\textwidth]{../doc/tod_training}

Yellow boxes represent processes/steps/actions and grey boxes represent data
artifacts used by or created in the different steps.

\section*{Input data}

The tutorial http://www.ros.org/wiki/tod\_training/Tutorials/BaseCreation
provides links to the training bags found in
http://vault.willowgarage.com/wgdata1/vol1/tod\_kinect\_bags/training/. These
training bags contain the following information

camera\_info     sensor\_msgs/CameraInfo

image               sensor\_msgs/Image

points2         sensor\_msgs/PointCloud2

(image\_mono sensor\_msgs/Image)

(tf                 tf/tfMessage)

According to the tutorial the topics camera\_info, image and points2 are
absolutely necessary and the training subjects or the rotating platform they
are placed upon have fiducial markers rigidly attached to them.

\section*{Training base preparation}

\subsection*{Bag dumping}

Before starting the actual training, it is necessary
to extract information from the compressed bag files and bring this data into a
certain directory layout. The responsible script is dump\_all.py which
delegates the preparation work to bag\_dumper.cpp. It uncompresses the
obligatory point clouds, images and camera information from the recorded bag
files to the directory which is referred to as the training base. The topic
names can be remapped via command-line. Note: at the time of writing, the bag
dumping tool was broken, see 5082 for a workaround.



\subsection*{Training process}

The training process is triggered by train\_all.sh which runs a training
pipeline for each training subject. We can derive the four steps from
train\_object.sh. The different stages in the pipeline pass results as files.
The four steps are:

\begin{enumerate}
    \item Pose estimation (apps/pose\_estimator.cpp)
    \item Masking (apps/masker.cpp)
    \item Feature detection and extraction  (apps/detector.cpp)
    \item 2d-3d point mapping  (apps/f3d\_creator.cpp)
\end{enumerate}

It is crucial to find out what every single of these steps does, which input
they require and what output they produce. As the results are implicitly passed
between the different training stages, this is a little bit more complicated.


\subsubsection*{Pose estimation}

The goal of the pose estimation step is to find the pose (6d) of the training
subject for each training image and save this pose in a yaml file that
corresponds to a training image.

Input
\begin{itemize}
    \item a training image
    \item a description about the fiducial marker
    \item camera information
\end{itemize}

Output
\begin{itemize}
    \item pose estimation for the training image
\end{itemize}

The pose estimation is performed by FiducialPoseEstimator (see pose.h and
pose.cpp) which takes camera information and the training image (which is
converted into a gray-level image) and returns an estimated pose. The list of
images for the training subject is retrieved from a directory listing (see
Opts.cpp), the description of the fiducial marker is read from fiducial.yaml.
The estimated pose is 6-dimensional and consists of a translation and a
rotation vector. You can visualize the estimated pose by enabling the verbosity
option.

If the pose cannot be retrieved from the original image because the corners
of the chessboard could not be detected, there is a fallback to an image that
has been dithered and converted into grayscale using imagemagick's monochrome
operator.

\subsubsection*{Masking}

The masking step extracts the region of interest. The default method used in
tod\_training is based on point cloud segmentation, but makes use of the pose
estimation from the previous step. It reads in the pose information from the
.pose.yaml files generated by apps/pose\_estimator.cpp. Have a look at
apps/masker.cpp.

Input
\begin{itemize}
    \item a point cloud
    \item estimated pose
    \item camera information
\end{itemize}

Output
\begin{itemize}
    \item a black\&white mask
\end{itemize}

The point cloud segmentation (see pointcloud\_segmentation.cpp) works as
follows: The pose estimated from the fiducial markers becomes the center point
of a rectangular box that is used to extract the 3-dimensional region of
interest. The box shape is hard-coded and cannot be altered without changing
the source code. After retrieving all those points in the point cloud that fall
into this box, outliers are removed (via RadiusOutlierRemoval) that have not
enough neighbors within a certain radius. Using OpenCV the segmented cloud in
the 3-dimensional region of interest is projected (Perspective projection) on a
2-dimensional image which then becomes the black\&white mask that masks the
2-dimensional stereo image. This perspective projection actually requires the
camera pose that is retrieved from the very first step in the training
pipeline. Check out masker.cpp which is giving a clue on where the pose
information enters the stage:

       f2d.camera.pose = pose\_est.estimatePose(Mat());

Note that at several places in the pipeline the pose information is retrieved
by calling estimatePose but there is no estimation done. Instead, the pose
information is read from the generated .pose.yaml files and then returned by an
instance of KnownPoseEstimator which is a kind of fake estimator that only
seems to exist for compliance to interfaces.

The produced black\&white mask is saved to a .mask.png file such that it can be
read in the next stage of the training pipeline.


\subsubsection*{Feature detection and extraction}

This step takes the 2-dimensional stereo image, reduces it to the region of
interest and then generates a feature description according to one of a bunch
of available algorithms. Unfortunately, this stage in the pipeline is full of
misnomers or at least misleading terms and therefore difficult to comprehend.
Furthermore, I cannot understand why such a simple design pattern which is
hidden behind the implementation causes such a mess in the implementation
itself. Maybe it's just my ignorance regarding C++ but apart from that there's
no real necessity to make things look more complex than they are.

Input
\begin{itemize}
    \item 2d region of interest as a black\&white mask
    \item 2d gray-scale image
    \item feature extractor configuration
\end{itemize}

Output
\begin{itemize}
    \item a feature description for the 2d gray-scale image
\end{itemize}


Looking into apps/detector.cpp, we can see that it first reads the stereo image
as gray-scale. The color information is not included into the feature
description (which would probably be a bad idea anyway since color information
is very much subject to illumination). Then, a feature detector and a feature
extractor is built according to the parameters specified in
features.config.yaml. This configuration file is compulsory. The feature
extractor populates an instance of Features2d. The feature description will be
stored in a per-view .features.yaml.gz file and the contents are
human-readable. Also, the pose information is read again but it is not really
necessary for the detection process. Instead, the pose information is just
duplicated and stored into an instance of Features2d and finally also ends up
in the .features.yaml.gz file. Evidence for this can be found both in
apps/detector.cpp and in feature\_extraction.cpp where the camera instance of
the Feature2d instance is never used (though the interface would allow to do
so). * Thus, the detector does not use pose information for feature detection
and extraction!

So basically, there are two sub-tasks in this stage. Detection of features and
extraction of features. The detection process finds keypoints in the image and
the extraction process creates a feature description out of these keypoints.

There are several algorithms available, for a up-to-date listing see
feature\_extraction.cpp. At the time of writing, the available detectors
include: FAST, DynamicFAST, SURF, DynamicSURF, STAR, DynamicSTAR, SIFT, MSER
and GFTT. % no longer up-to-date

For feature extraction, tod\_training seems to support: ORB, multi-scale,
sequential, rBRIEF. I cannot say much more at the moment. % no longer up-to-date

\subsubsection*{2d-3d point mapping}

This stage maps the 2d training keypoints to 3d points by projecting the
training cloud onto the 2d image plane and pairing each keypoint with the 3d
point whose projection is closest to that keypoint.

\subsubsection*{Key data structures}

There are several data structures that appear all over the source code in
tod\_training. There are Features2d, Camera, PoseRT.

The Features2d instance has the following members
\begin{itemize}
    \item keypoints
    \item descriptors
    \item image
    \item image\_name
    \item mask\_name
    \item camera (including camera.pose!)
\end{itemize}

Which of those members are currently initialized or not seems to differ
depending on the context where those Feature2d instances are used. Also,
deriving from comments in the source, the developers are not completely sure
about the responsibilities.

The Features2d object has a camera and the camera again stores the pose object,
which is a little bit confusing since intuitively the pose belongs to the
object when seen from the camera coordinate system. Don't get confused. The
camera instance just stores the pose information that is necessary to transform
points between the object coordinate system (draw axes on the objects will
produce this system) and the camera coordinate system (given by the camera
where z denotes what we commonly refer to as depth in space). Have a look at
this picture:

\includegraphics[width=0.7\textwidth]{../doc/pose.png}

\subsubsection*{Output}

The training results are stored in files in a directory which I call the
training base. This training base contains one subdirectory per object.

\begin{verbatim}
config.txt                  List of all objects in the training base.
config.yaml                     Configuration used by tod_detecting
features.config.yaml                Configuration used by tod_training
fiducial.yml                    Description of the fiducial marker.
...
fat_free_milk/                  One folder per object
    camera.yml              Camera information
           ...
           image_00052.png          full color image
image_00052.png.f3d.yaml.gz compressed feature description incl. pose
image_00052.features.yaml.gz    compressed feature description incl. pose
image_00052.png.mask.png               black and white mask   
image_00052.png.pose.yaml              pose information
...
...
\end{verbatim}

\subsection*{How tod\_detecting works}

This is a summary of how tod\_detecting works, which input it requires, which
steps it performs, which algorithms it employs and what the results look like.
The detection of objects in a query image can be roughly separated into four
different subtasks: Reading the training base, extracting features in the query
image, create a matcher.

Some of the files in the training base are no longer of interest, such as the
mask (.mask.png files are not read by tod\_detecting). Also, the png images in
the training base are (as expected) not required by the detection process. All
data the recognizer needs is included in the feature descriptors, the cloud
files and the pose estimations.

See also http://answers.ros.org/question/289/does-tod\_training-in-object\_recognition-require
See also http://answers.ros.org/question/505/purpose-of-pose-estimations-in-tod\_detecting
For collecting training bags, see http://answers.ros.org/question/504/physical-setup-for-collecting-tod\_training-data
and http://answers.ros.org/question/355/recording-a-tod\_training-bag, and http://www.ros.org/wiki/tod\_training/Tutorials/BaseCreation?action=recall\&rev=17.
(this is an old version of the tutorial which includes some information the old
one does not have, Alexander Shikhov pulled this out of the ashes, see for more
information on fiducial markers)

Summary of steps included in recognition process
\begin{enumerate}
    \item Detect and extract features
    \item Matching keypoints
    \item Guessing
\end{enumerate}


This description is about SVN revision 50321. Most of tod\_detecting has changed
since the time of writing, and especially the part about the guessing has been
radically altered.

\subsubsection*{Overview over recognition process}

Yellow boxes represent processes/steps/actions and grey boxes represent data
artifacts used by or created in the different steps.

\includegraphics[width=0.8\textwidth]{../doc/tod_detecting_kinect}

\subsubsection*·{Reading training base}

The subjects in the training base MUST be listed in config.txt, subjects not
listed in this file are simply ignored by tod\_detecting. The different training
images for a specific subject are retrieved by looking into sub-folders of the
training base. Such a folder belongs to the training base if it contains a file
called camera.yaml. See recognizer.cpp and Loader.cpp.

For each object, this process consists of
\begin{enumerate}
    \item Loading the pose information for each view
    \item Selecting some of the views based on whether the angles between two successive views exceeds some threshold
    \item For the selected views, load the Feature3d instances from the .f3d.yaml.gz files which also includes the point cloud belonging to this view
\end{enumerate}


\subsubsection*{Detect and extract features}

According to the parameters given in config.yaml, tod\_detecting detects and
extracts features from the query image and stores them in a Features2d
instance. When comparing recognizer.cpp in tod\_detecting and detector.cpp in
tod\_training, we can see that the feature detection and extraction process is
the same, and the code meets expectations that the data prior to matching has
been collected in a similar manner. A working example configuration for feature
detection and extraction is:

detector\_type:   DynamicFAST

descriptor\_type: rBRIEF

extractor\_type:  multi-scale

Using this sample configuration, the classes DynamicAdaptedFeatureDetector,
RBriefFastAdjuster and RBriefDescriptorExtractor, MultiscaleExtractor all come
into play. The feature detection and extraction step does not use pose
information, as both code analysis and GNU debugger show. This is also
symmetrical to what we find in tod\_training.


\subsubsection*{Matching}

Matching is about finding good matches between keypoints in the query and the
training images. The matching is unscrupulously performed between the one query
image and the training images of all training subjects. The result of the
matching is therefore a set of matches between keypoints in the query image and
any training image in the training base.

There are different matching algorithms available, all of them implemented in
OpenCV: FLANN, BF, BF-BINARY,  LSH-BINARY as can be seen in Matcher.cpp. The
parameters can be set in config.yaml and a working example configuration for
the matching process is matcher type:

LSH-BINARY

ratioThreshold: 0.8

knn:  3

doRatioTest: 1

This sample configuration will involve LshMatcher and RatioTestMatcher to
perform the matching.

There are two brute force matchers from OpenCV available that use Manhattan
(BF) or Hamming (BF-BINARY) distance. LSH-BINARY implements  locality sensitive
matching and it is also possible to use a FLANN-based matcher.

The source code is quite chaotic, especially the naming is dubious seen from an
OOP perspective.  One reason is that the tod::Matcher keeps a large parallel
array instead of keeping one list of records. Also, there is a lot of (call it
awesome or obfuscating) book-keeping code involved that deals with different
indices and mapping between those.

The matching process tries to find matches between keypoint descriptors in the
query image and the train images. OpenCV's DescriptorMatcher is used to - find
the k best matches for each keypoint descriptor in the query image and the
keypoint descriptors of the training images. It's performed using some
k-nearest-neighbors algorithm in OpenCV. A cv::DMatch describes such a match
between a query keypoint descriptor and a training keypoint descriptor. The
Matcher is responsible for keeping track of which match belongs to which object
and corresponds to which training image. One query keypoint can find many
matching template keypoints. 


Matcher parameters

See
\begin{description}
    \item[matcher\_type] Specifies which matcher implementation to use. Choose
        between FLANN, BF, BF-BINARY, LSH-BINARY.  See Matcher.cpp
    \item[ratio\_threshold] Used by RatioTestMatcher. See SIFT paper from Lowe 99. See Matcher.cpp
    \item[knn] defines the number of nearest neighbors to find for each query
        keypoint. Equal to parameter k in OpenCV's knnMatch. Must be greater than 1.
        Setting knn = 30 (was 3) produced less positives on ias\_kinect\_test\_all, and
        knn = 100 even less. See Matcher.cpp
    \item[do\_ratio\_test] If true, some pruning and a ratio test is performed.
        See SIFT paper from Lowe 99.  Turning off ratio test lead to far more
        guesses and positives on ias\_kinect\_test\_all.
\end{description}


\subsubsection*{Guessing}

Guessing is the process of actually predicting which training subject can be
found in which pose on the query image. There are two different modes, you can
either use TODRecognizer or KinectRecognizer in Recognizer.cpp and according to
information from Alexander Shikhov, the recognizers differ in performance when
used either with a 5 megapixel Prosilica camera or with a Kinect camera.
Experiments have shown that TODRecognizer and KinectRecognizer perform pretty
much the same in SVN revision 50321, yet KinectRecognizer produces less false
positives on the TOD kinect data set. In SVN revision 50846, KinectRecognizer
has not improved in performance.

\begin{tabular}{ l r c c l }
  Recognizer & SVN revision & Uses Pose & Offers 3d-3d RANSAC & Comments \\
  TODRecognizer & 50321 & No & No & Just uses clustering of keypoints and then RANSAC. \\
  TODRecognizer & 50846 & Yes & Yes & Operates the same way as KinectRecognizer but uses clustering first. See KinectRecognizer::match, TODRecognizer::match and GuessGenerator::calculateGuesses method. \\
  KinectRecognizer & 50321 & Yes & No & Uses 2d-3d model fitting using RANSAC. \\
  KinectRecognizer & 50846 & Yes & Yes & Offers 3d-3d model fitting using RANSAC which is automatically used if a query point cloud is provided, otherwise uses 2d-3d model fitting using RANSAC, just like in older revisions. \\
\end{tabular}


Since TODRecognizer seems to be mostly neglected by the developers of tod\_*,
the following part will only describe KinectRecognizer in 50648.

The Recognizer uses the matcher to find correspondences between keypoint
descriptors in the query image and training images. A tod::Recognizer uses a
tod::Matcher which uses a cv::DescriptorMatcher to first collect a set of
cv::DMatch objects. This set provides the foundation for the whole further
process. A match connects a keypoint in the query image with a matching
keypoint in one of the training images. As such, the matches describe
similarities between the query image and the training images. The following
figure illustrates this relation:

\includegraphics[width=0.8\textwidth]{../doc/dmatch}

The result of the matching is a set of matches. This set contains matches that
link query keypoints with keypoints from different training images. Let this
set of matches be called M.

The idea of the guessing process is now to generate guesses out of these
matches. The simplest solution with questionable performance would probably
consist of simple voting. tod\_detecting takes another approach.

KinectRecognizer iterates over each training subject and looks for possible
occurrences on the query image. Let MS  be the set of all matches on training
images of subject S. KinectRecognizer uses RANSAC to find a pose that minimizes
error between the reprojection of matching 3d points in the training views of S
and the query keypoints. If RANSAC produces enough inliers (i.e. above a
certain threshold referred to as min\_inliers\_count or minInliersCount in the
sources), the algorithm concludes that an instance of training subject S has
been found in the scene. In each training view, there is a one-to-one mapping
between the training keypoints and corresponding 3d points (f3d\_creator). As
such, a match does not only create a correspondence between a query keypoint
and a keypoint in the training image but also links a query keypoint with a 3d
point from the training view. All matches from MS are considered at once when
producing a new guess candidate. The corresponding 3d points are merged into a
common coordinate system according to the pose information gathered in the
training process (pose\_estimator.cpp). We can think of those 3d points forming
a partial 3d model of the training subject that is then being fit to the 2d
projection of the subject in the training image by RANSAC.

Unfortunately, both KinectRecognizer and TODRecognizer use the camera matrix
and the camera distortion matrix used for guessing the pose are taken from the
training images instead of the query image. This is a conceptual mistake that
has to be addressed by filling in the information of the camera used in
testing.


{\definecolor{gray}{rgb}{0.6,0.6,0.6} \color{gray}

TODRecognizer   obsolete

On a per-subject basis, TODRecognizer performs agglomerative hierarchical
clustering of test image keypoints that have been matched with the specific
training subject. At the beginning of the clustering a cluster is created
(remember every keypoint has a keypoint descriptor) for every keypoint in the
test image that found a match in one of the training images. A Cluster consists
of a set of keypoints. The result of the clustering process is a 2-dimensional
Iliffe vector that shows the association between clusters (first index) and the
keypoints (as a list of indices). In order to visualize these clusters, call

drawClusters(test.image.clone(), points, clusterIndices);

where points refers to the test image keypoints mentioned above and
clusterIndices to the Iliffe vector mentioned above.

Then, also on a per-subject basis, a GuessGenerator makes guesses based on the
obtained clusters, the test image and its keypoints and the matches between
keypoints of the test image and the specific subject's training images (often
referred to as objectMatches in the source code). The guesses are
post-processed and filtered according to a criterion based on the standard
deviation of point clouds and some OverlappingFilter I haven't looked into yet.

The GuessGenerator constructs a Features2d and a Features3d instance using the
supplied test image and its keypoints. It remains a puzzle why these Features2d
instances are always constructed on-the-fly and then split into their members
in a manner that appears quite random to me. The number of parameters for each
method is too large. 

--- This part only applies to TODRecognizer in some SVN revision different from
50321. Guesses are calculated separately for each cluster. Multiple guesses can
be generated from one cluster.

For each cluster, every test image keypoint belonging to this cluster is
examined. 2d-to-3d-mode is enabled (I don't know what this is at the time of
writing). At this time, the per-subject-view Features3d instance becomes
important. Citing the documentation of Features3d:

'Encapsulates the relationship between a cloud and a 2d keypoints'                      

'The cloud should have a one to one correspondence with the features.'

For each of the test image keypoints in the cluster, the 3d point from the
training subject's view that corresponds to this keypoint is selected and
transformed using the view-specific inverted (see Tools) pose estimation. So, a
list of 2d keypoints and corresponding 3d keypoints is obtained. Now, RANSAC
(using vslam's solvePnPRansac, which again uses OpenCV's solvePnP, parallelized
with Intel Threading Building Blocks). The documentation of solvePnP says
'Finds the object pose from the 3D-2D point correspondences' and that's what
happening in tod\_detecting.

Note: it seems that recent revisions (e.g. 50470) (also) implement 3d to 3d
matching via RANSAC

The GuessGenerator makes use of the pose estimation obtained in the training
pipeline!

A guess is made if the number of inliers produced by solvePnP exceeds a
threshold and when solvePnP produced a valid pose estimation. If solvePnP fails
to produce a good-quality matching where quality is measured in terms of the
number of inliers, then there will no more guesses be generated for this
cluster. If a guess is made, the remaining keypoints that correspond to
outliers are considered for the next guess.  ---

tod\_detecting uses four methods to distinguish between good and bad guesses?
clusters with not enough keypoints are discarded right away
(min\_cluster\_size, TODRecognizer only) measuring number of inliers produced
by solvePnP (min\_inliers\_count)

filter based on overlapping (TODRecognizer only)

filter based on standard deviation (TODRecognizer only)

The guesses are post-processed and solvePnPRansac is again used to estimate
pose of the object in the test image, but this time only inliers from the
previous solvePnP run are taken into account. The GuessClarifier does not
filter guesses. This seems to be just some optimization.

In SVN revision 50321 we have another version of TODRecognizer. It does not
care about the pose estimations at all.
}

Guess parameters

\begin{description}
    \item[minInliersCount] If number of inliers produced by RANSAC when trying
    to fit a pose is less than minInliersCount, then no guess will be made
    (reject). I expect that increasing minInliersCount will push the classifier to
    the lower left in ROC space and will make false positives less likely.  Must be
    greater than zero. Having set minInliersCount = 1 (was 5) produces hundreds of
    guesses for ias\_kinect\_test\_all. Having set minInliersCount = 10 will
    produce only a few positives (on ias\_kinect\_test\_all). See GuessGenerator.cpp
    \item[ransacIterationsCount] Maximum number of iterations that RANSAC is
    allowed to perform when finding a pose that fits to the keypoints.  Setting
    ransacIterationsCount = 200 (was 100) leads to more positives. Setting
    ransacIterationsCount = 1000 leads to even more positives (50\% true positives
    instead of 20\%, can say nothing about false positives) and more guesses per
    subject, but also starts to slow down recognition (on ias\_kinect\_test\_all).
    See GuessGenerator.cpp pnp\_ransac.cpp
    \item[maxProjectionError] Defines a radius such that RANSAC can distinguish
    between inliers and outliers. When RANSAC finds a hypothetical pose, it uses
    this pose to project 3d points. If the distance between a projected point and
    its corresponding test keypoint is smaller than maxProjectionError.
    Setting maxProjectionError = 20 (was 6) leads to way more guesses and
    positives. Setting maxProjectionError = 1 (on ias\_kinect\_test\_all) results in
    almost zero positives.  See GuessGenerator.cpp pnp\_ransac.cpp
    \item[descriptorDistanceThreshold] Not used at all in whole stack
    object\_recognition, seems to be obsolete, see ticket 5096. See GuessGenerator.cpp
    \item[minStddevFactor] Only used by TODRecognizer, see GuessGenerator.cpp
    Recognizer.cpp.
    \item[minClusterSize] Only used by TODRecognizer. See GuessGenerator.cpp.
\end{description}

When using KinectRecognizer we have the following influential degrees of
freedom for the guessing process: minInliersCount, ransacIterationsCount,
maxProjectionError. The parameter configuration that has been used to create
the training base and detect keypoints and descriptors also influences
performance.



\subsubsection*{Key data structures}
Guess
A guess consists of
\begin{itemize}
    \item the aligned pose of the object in the query image
    \item a standard deviation value
    \item the query image
    \item the recognized textured object
    \item camera calibration and distortion matrix
    \item 2d keypoints on the query image that found a match in a training image
    \item 3d keypoints corresponding to the 2d keypoints in training images that were matched with the query image
    \item inlier points from RANSAC
\end{itemize}

Features2d, Features3d


