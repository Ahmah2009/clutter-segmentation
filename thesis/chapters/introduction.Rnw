\section{Intended Purpose}

Object recognition is one of the fundamental tasks in computer vision. One of
its applications is in robotics. Especially in household environments it is
often not only necessary for a robot to merely detect the presence of an
object, but also to locate this object in order to grasp it and make further
decisions on a semantic level.

A particular application addressed by this work is a robot in a household
environment who is given the task to go shopping, and which empties the
shopping bag onto a table, and then plans to move the bought items to
appropriate storage facilities. The shopping items on the table form a
cluttered scene. Most of the shopping items are rigid bodies and can be
distinguished by characteristic texture, made of pictorial descriptions of the
product, labels, and company logos. Recognition of these shopping items is
rendered difficult by high levels of occlusion and the presence of many such
textured objects in one single scene. The objects might be arbitrarily oriented
and prior assumptions about their principal axes are likely to be invalid.

The system presented in this work equips a PR2 robot with the ability to locate
one object in the cluttered scene and provide sufficient information to a
grasping pipeline, such that the clutter can be resolved by removing items
one-by-one from the scene. Although, in the shopping scenario, the robot might
have prior knowledge about which objects to locate in the scene, this
information is not used in this work, in order to develop a system that is
general enough to be applied to other application scenarios.

\section{Problem Statement}
\label{section:problem-statement}

The challenge is therefore to correctly identify a rigid textured object and
determine its pose in a scene with clutter and high occlusion, and subsequently
provide enough information to allow the robot grasp the object and remove it
from the cluttered scene.

\section{Related Work}

Literature describes many approaches to object recognition. They differ in
scope, targeted application and techniques. Those using local features have
been especially relevant to this work. 

Tuytelaars and Mikolajczyk give a qualitative review of local feature detectors
and descriptors \cite{Tuytelaars2007}, and provide guidance in making an
appropriate choice between a multitude of available detectors and descriptors.
Lowe's work on the scale-invariant feature transform explained the principles
of the scale-space pyramid and rotation invariance of features \cite{Lowe1999}.
Rosten and Drummond introduce the FAST feature detector \cite{Rosten2006}.
% while Agrawal et al. presented the CenSurE feature detector \cite{Agrawal2008},
% both designed to be efficient.
Calonder et al. describe BRIEF, a feature descriptor which is simple to compute
and efficient to match \cite{Calonder2010}. Oriented BRIEF, derived from FAST
and BRIEF, is the feature detector and descriptor of choice in this work,
although literature had not been present at the time of writing.  Matching
observed features with model features is a common sub-task in object
recognition. 
% For this, Muja described the FLANN library \cite{Muja2009}.
Gionis et al. introduce, and Slaney and Casey summarize Locality Sensitive
Hashing \cite{Gionis1999, Slaney2008}, which finds approximate
nearest neighbors.
% Lowe showed how to recognize
% objects up to a 20 degree rotation using the scale-invariant feature transform
% (SIFT), making use of the Hough Transform where correspondences vote for the
% pose of an object \cite{Lowe1999}.
% Hough voting has also been used in a system presented by Drost et
% al. \cite{Drost2010}.
Fischler and Bolles explain RANSAC and deal with the perspective-n-point
problem in an excellently written paper \cite{Fischler1981}.  Dogar and
Srinivasa recently spent research on robustly grasping objects in a cluttered
scene \cite{Dogar2010}. Forsyth and Ponce's work on computer vision served as a
reference and influenced the terminology in this work \cite{Forsyth2003}. The
same applies to Gonzalez and Woods in the field of image processing
\cite{Gonzalez2010}.  Alpaydin's excellent introduction to machine learning
\cite{Alpaydin2010} has influenced experiment design and parameter
optimization. Fawcett \cite{Fawcett2006} published a detailed guide on how to
evaluate the performance of classifiers through analysis of receiver operating
characteristics. Melsa and Cohn \cite{Melsa1978} cover decision and estimation
theory.

% Haltakov and Pangercic have developed the ODUFinder library for the Robot
% Operating System to detect both non-textured objects and textured objects,
% the latter by using SIFT and vocabulary trees.

\section{Selected Approach}

Our system was built on top of the existing Textured Object Detection ({\it
tod}) package for the Robot Operating System, targeting the PR2 robot. It
describes objects by a set of local 2-d features extracted from multiple views
to account for instances appearing in arbitrary orientations. Oriented BRIEF, a
feature detector and binary feature descriptor, is used to generate local 2-d
features for both model and query scene. Local Sensitive Hashing, an
approximate nearest neighbor search algorithm, finds correspondences between
features of the models and the query scene. RANSAC is used to solve the
perspective-n-point problem to generate a set of pose estimates. The pose
estimate with the largest consensus set returned by RANSAC is then refined. The
refinement involves matching features against a particular model and repeating
the pose estimation process.

Larger consensus sets are preferred because they make robotic grasping easier.
The system heavily exploits the requirement that a robot needs only to locate
one object in the scene. An experiment runner conveniently allows to test
different parameter configurations against a validation set, and such supports
parameter optimization.

