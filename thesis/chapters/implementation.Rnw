This chapter describes our \clutseg system for recognizing an object in a
cluttered scene. It takes a query image as input. As a result it shall detect the
presence of an object, find its pose in the query scene, and mark points on the
object.

\clutseg is based on the existing Textured Object Detection (\tod) Library,
which we discuss in detail.

\section{Software Dependencies}

\clutseg depends on many other open-source libraries. Of these, we briefly
explain (\refFigure{figure:clutseg-deps}): \opencv for operating on images,
\pcl for dealing with 3D data, \ros for interfacing with the robot, \sqlite for
storing and retrieving experimental data, and \rproject for analysing data.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{clutseg-deps}
        \caption[The software dependencies of \clutseg]{\clutseg depends (amongst others) on \tod, \ros,
        \pcl, \opencv, \vslam, \rproject, and \sqlite.}
        \label{figure:clutseg-deps}
    \end{center}
\end{figure}

{\it OpenCV} is an open-source library for computer vision \cite{Bradski2008}.
It supports the representation, the input and the output of images as well as
many image processing operations. It offers methods for camera calibration and
feature detection, which are used by \clutseg and \tod.

The {\it Robot Operating System} (\ros) is an open-source framework for the
development of robot applications, having its roots in the STAIR project at
Stanford University \cite{Quigley2009}. In \ros, distributed nodes communicate
by publishing and subscribing to topics. In our case, the interesting topics
were the image and the 3D point cloud, together forming the sought-after depth
images. These images and other data can be recorded in so-called bags. \ros
contains the \vslam package, which is only used by \tod because it contains a
RANSAC implementation for solving the PnP problem.

The {\it Point Cloud Library} (\pcl) is concerned with processing 3D data
\cite{Rusu2011}. A point cloud is a set of points in n-dimensional space. For
example, the output of the Kinect camera can be represented by a point cloud
where each point is described by its Cartesian coordinates with respect to the
camera and by its observed colour.

{\it SQLite} is a reliable file-based relational database that is well-suited
for local applications. It is thoroughly tested with over a million tests, and
hassle-free to use. It provides convenient command-line tools that allow for
exporting data quickly into human-readable formats.

{\it R} is an open-source environment for statistical computing and graphics.
It excels in good documentation. R supports many different sources of data, and
in particular also a \sqlite database.

\section{Textured Object Detection Library}

In this section we analyse the software underlying the \clutseg system. We
explain how we modified it and improved it to better fulfil our requirements.
We describe how to extract models of template objects from multiple views, and
cover the elementary pipeline that later recognizes instances in query scenes.

The Robot Operating System contains the packages {\it tod}, {\it tod\_training}
and {\it tod\_detecting}. The system formed by these three \ros packages is
called \tod for easier reference \footnote{German readers will note the
unfortunate choice in abbreviating ``textured object detection''.}. Throughout
this work, \tod has been used in Subversion revision 50479, and modifications
are explicitly mentioned in the following. At this revision, \tod was
experimental, unstable, and not covered by automatic tests. The system
participated in the {\it Solutions in Perception Challenge} contest at ICRA
2011 in Shanghai for comparison.

\tod offers two modes. One targets the Kinect RGB-D camera. The second mode is
optimized for working better with a Prosilica MP 5 camera \cite{Shishkov2011}.
While the latter clusters correspondences prior to pose estimation, the former
does not. \tod recognizes objects solely by intensity and depth information;
colour information is ignored. \clutseg uses \tod in the Kinect mode.

\subsection{Learning}
\label{subsection:learning}

In general, an object recognition system needs models of the objects which it
shall be able to recognize. In \tod, a model is a set of model features learnt
from multiple views of the template object. Each of the views is processed in a
pipeline (\refFigure{figure:tod-training}). Afterwards, the resulting model
features from selected views are unified, and together form the model of the
object.

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{../doc/tod_training}
        \caption[The pipeline of \tod used for learning models]{The pipeline of \tod used for learning models. The rectangular
nodes represent processes, the slanted nodes represent data.}
        \label{figure:tod-training}
   \end{center}
\end{figure}

\subsubsection{Model Features}

Given a single view, let $\vec{p}$ be the 3D object coordinates of a point on
the template object. Let $\vec{p'}$ be the projection of $\vec{p}$ on the image
plane, and $\vec{d}$ the descriptor vector that describes the local image
neighbourhood of point $\vec{p'}$. We call $(\vec{p}, \vec{d})$ a {\it model
feature}.

\tod conceptually generates a model feature in three steps. First, a feature
detector selects a keypoint on the image plane.  Second, a feature descriptor
vector is computed, which describes the neighbourhood of the keypoint. Third,
the descriptor vector is associated with the object coordinates of the 3D point
that corresponds to the keypoint. The keypoint itself is of no further
interest.

\subsubsection{Model Extraction}

\tod offers a toolchain that extracts model features from a template object on
a rotating table. The template object is either manually or automatically
rotated, and a depth camera takes images from multiple views. The latter
consist of a depth image and a greyscale image each. The views are the basic
units processed in a pipeline that comprises three stages for each view: First,
the computation of the object-camera transformation from the image.  Second, the
segmentation of the region of interest that shows the object on the image.
Third, the extraction of local 2D features from the greyscale image, followed
by the construction of model features from the region of interest.

\subsubsection{Camera Calibration}

Given a view, the first step is to obtain the object-camera transformation by
estimating the extrinsic parameters of the camera. This is required for every
view because the object pose changes relative to the camera, and conversely,
the camera pose changes relative to the object. The camera calibration is based
on chessboard-like fiducial markers that are glued onto the rotating table on
two opposite sides (\refFigure{figure:fiducial-object}). \opencv offers camera
calibration methods that help to find the chessboard corners and then, based on
these corners, estimate the extrinsic parameters by solving the
perspective-n-point problem. 

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{fiducial-object}
        \caption[The fiducial coordinate system and the object coordinate system]
                {The fiducial coordinate system and the object coordinate system are defined to be the same.}
        \label{figure:fiducial-object}
    \end{center}
\end{figure}

There is a subtlety in the relationship between the fiducial markers and the
object coordinate system. The camera calibration computes the PRT $\fcT$
between the {\it fiducial coordinate system} ($f$) attached to the fiducial markers
and the camera coordinate system. The origin in fiducial coordinates is defined
halfway between the two chessboards. Yet,
we need the object-camera transformation $\ocT$. The solution here consists in
just {\it defining} the object coordinate system to be the same as the
coordinate system attached to the fiducial markers, that is
\begin{equation}
    \ocT: \real^3 \to \real^3, \vec{p} \mapsto\ \fcT(\vec{p})
    \label{equation:fiducial-object}
\end{equation}

As long as one of the two chessboards is detected, the transformation can be
estimated, which proves useful when the template object occludes the chessboard
further away from the camera. The chessboards were sometimes not detected in
the image at all. In such cases, we used dithered binary images as a fallback.
This improved reliability. We singled out 23 cases of the original images where
camera calibration failed. Some tested image processing operations work better
than others on these 23 images (\refTable{table:camera-calib-fallback}). Three
preprocessed images are shown in \refFigure{figure:camera-calib-fallback}.

\begin{table}[h!]
    \begin{tabular}{llrr}
        Pre-processing method & Imagemagick command & Success \\
        \hline
        Dithering to Binary & {\tt convert -monochrome \$1 \$2} & 23/23 \\
        Colour Reduction & {\tt convert -colors 2 \$1 \$2} & 23/23 \\
        Local Adaptive Thresh. & {\tt convert \$1 -lat 25x25 -threshold 50\% \$2} & 23/23 \\
        Thresholding & {\tt convert \$1 -threshold 50\% \$2} & 19/23 \\
        Sharpening & {\tt convert -sharpen 0x12 \$1 \$2} & 2/23 \\
        Closing & {\tt convert -morphology Close Disk \$1 \$2} & 1/23 \\
        Opening & {\tt convert -morphology Open Disk \$1 \$2} & 0/23 \\
        Median Filtering & {\tt convert -median 2 \$1 \$2} & 0/23 \\
    \end{tabular}
    \caption[Various image processing operations for more reliable camera calibration]{Various image processing operations
have been applied to the set of 23 images where detecting the chessboards initially failed. When converting these images to dithered binary images,
the chessboard was detected on all 23 of them.}
    \label{table:camera-calib-fallback}
\end{table}

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=\textwidth]{pose-pre-process}
        \caption[An original and three preprocessed images for camera calibration]{From left to right: (1)
Original image where pose estimation failed; after (2) Dithering to Binary;
after (3) Colour Reduction; after (4) Local Adaptive Thresholding. On images 2,
3, and 4, the extrinsic parameters were successfully estimated.}
        \label{figure:camera-calib-fallback}
    \end{center}
\end{figure}

\subsubsection{Segmentation of the Region of Interest}

The model must only includes features that belong to the template object. Hence,
the extraction of model features must be confined to the region in the image
that shows the object but no background. 

\tod segments the 3D point cloud of the template object using a box-shaped
pass-through filter of a predefined size at the object origin $\ocT
\pth{\vec{0}}$.  Afterwards, a radius outlier search removes all points with
less than 20 neighbours within 2 cm. \tod projects the segmented point cloud
onto the image plane. The set of projected points form a mask that is supposed
to be aligned as close as possible to the true region of interest.

% Due to some bug in the underlying software, the estimate for the region of interest
% sometimes included parts of the chessboards next to the template objects. These
% undesired parts in the mask were observed to be separate connected components.
% Hence, we fixed the mask by opening the image with a disk of radius $10.3$,
% removing all but the largest connected component, and dilating the mask with a
% disk of radius $2.3$ to compensate for the initial opening step.

% \begin{figure}[h!]
%   \begin{center}
%       \includegraphics[width=0.35\textwidth]{disk-4-3}
%       \caption[Disk for opening]{
%           }
%       \end{center}
% \end{figure}


% probably should not include a picture from other database?
% \begin{figure}[h!]
%    \begin{center}
%        \includegraphics[width=0.75\textwidth]{mask-post-process}
%        \caption[Post-processed image mask]{
%            Mask before and after correction. The latter is close
%            to the true region of interest.}
%        \end{center}
% \end{figure}

\subsubsection{Extraction of Model Features}


The final step in learning requires the construction of a set of model features
from the depth image. Local 2D features are extracted using any one of the
feature detectors and descriptors provided in \opencv. Each of these features
is described by a keypoint $\vec{p'}$ on the image plane and a descriptor
vector $\vec{d}$. A camera model, whose intrinsic parameters are known,
uniquely associates the keypoint $\vec{p'}$ with a 3D point, described in camera 
coordinates $\vec{p_v}$ in the scene. Applying the camera-object
transformation, the object coordinates $\vec{p}$ are obtained by $\vec{p} =\
\coT \pth{\vec{p_v}}$. This yields a model feature $(\vec{p}, \vec{d})$.
\tod stores all learned models in a {\it modelbase}.

In the actual implementation, \tod applies the inverse object-camera
transformation just before calling RANSAC for pose estimation, but it is
equivalent, conceptually simpler, and computationally faster to do this only
once while extracting features.

\subsection{Recognition}
\label{subsection:recognition}

This section shows how \tod can be used to recognize instances of template
objects in query scenes, making use of the models learned in the previous
section (\refFigure{figure:recognition-pipeline}). The recognition process in
\tod can be split into three parts: First, the extraction of query features.
Second, the matching of query features and model features to obtain a set of
correspondences between query image and models. Third, the estimation of object
poses based on this set of correspondences.

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{../doc/tod_detecting_kinect}
        \caption[The recognition pipeline of \tod]{\tod recognition pipeline (Kinect mode).}
        \label{figure:recognition-pipeline}
   \end{center}
\end{figure}

\subsubsection{Query Features}

Let $\vec{p}$ be a 2D point on a query image, and $\vec{d}$ its descriptor
vector. Then we call $(\vec{p}, \vec{d})$ a {\it query feature}.  Hence, such a
query feature is nothing but a keypoint-descriptor pair produced by SIFT, SURF
or ORB, or any other feature extractor, and we introduce this term to clearly
tell features from the query and the model apart. Note that query features do
not include 3D information.

\subsubsection{Extraction of Query Features}

Similarly to the extraction of model features, \tod uses one of the \opencv
feature extractors to find a set of query features in the image. It is
reasonable to select the same feature extractor as in the model extraction.
Both query feature and model feature extraction can be configured, so \tod
allows to try different feature detectors with configurable parameters.  \tod
also provides rBRIEF, an experimental modified version of BRIEF, which we used
in some experiments but we then replaced in favour of ORB from \opencv.

\subsubsection{Feature Matching}
\label{subsubsection:tod-feature-matching}

Since the image is not segmented, these query features emanate from different
textured objects, or from some textured background. Matching these features
shall find correspondences between these query features and model features. Let
$C_i$ be the set of correspondences where a query feature has been matched with
a model feature from the $i$-th template object. Finally, let $C = (C_1, \dots,
C_n)$ denote the partition of all correspondences for the query scene, where
$n$ is the number of template objects described in the modelbase.  \tod
integrates different nearest-neighbour algorithms. \tod offers a brute-force
matching algorithm which we use in
\refSection{subsection:performance-validation-set} for comparison.  We use
Locally Sensitive Hashing in \tod for matching.

\tod finds up to $k$ nearest neighbours for each query feature. The maximum
number $k$ of correspondences per query feature can be controlled by parameter
{\tt knn}.  \tod implements the ratio test, as described in \cite{Lowe2004}.
If a model feature is matched with multiple query features, only the
correspondence with the minimum distance is retained. This way, \tod ensures
that the mapping from query features to model features is injective.
% TODO: injectivity is not a word

\subsubsection{Guess}

The hypotheses \tod generates for a given query scene are called {\it guesses}.
A guess is a triple $(i,\ \hat{T},\ S)$, where $i$ denotes the template object
which \tod believes to see on the scene, $\hat{T}$ the estimated pose of the
object in terms of an object-camera transformation and $S \subseteq C_i$ the
consensus set of inliers. 

\subsubsection{Pose Estimation}
\label{subsubsection:tod-pose-estimation}

Given the correspondences $C$, \tod generates a set of guesses. The
correspondences are treated separately for each template object, that is \tod
looks at each correspondence set $C_i$. For the $i$-th template object, RANSAC
is called iteratively. If RANSAC returns with a pose estimate $\hat{T}_{ij}$ in
the $j$-th iteration and the size of the consensus set $S_{ij}$ is greater or
equal to a parameter {\tt min\_inliers\_count}, then \tod infers that there is
an instance of the template object at the estimated pose and generates a guess
$(i, \hat{T}_{ij}, S_{ij})$. Note that the inlier sets $S_{ij}$ are disjoint
because the RANSAC call in the $j$-th iteration is told to operate only on the
set $C_i - \bigcup_{k=1}^{j-1} S_{ik}$, and $C$ forms a partition. If not, then
iteration is stopped, and \tod continues with the correspondence set for the
next template object, if any. This means that the inlier sets of two different
guesses never share a correspondence. 

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{inlier-criteria.png}
        \caption[A model aligned to the query scene]{A model aligned to the
            query scene. The query features ($\times$) correspond to model features
            ($\bullet$).  The correspondence belonging to the lower right query feature is
            consistent with the pose estimate because the aligned model point projected
            onto the image plane ($\triangle$) is within a certain distance of the query
            feature; this correspondence is said to be an {\it inlier}, or equivalently, it
            is said to belong to the consensus set for the aligned model. The other
            correspondence is designated {\it $outlier$}.}
        \label{figure:inlier-criteria}
   \end{center}
\end{figure}


The \ros package {\tt posest} in stack {\tt vslam} contains an implementation of
RANSAC for the PNP problem, which is used by \tod. It is roughly similar to the
RANSAC/LD algorithm presented in \cite{Fischler1981}. It solves the P4P
problem.  The largest consensus set found is fed to the PNP-solver in \opencv
({\tt cv::solvePnP}), which then computes the final pose estimate.

\refFigure{figure:inlier-criteria} shows a model aligned to the scene.
Altogether, for a given query scene, \tod takes the query image and generates
an unordered set of guesses, which is the output of the \tod recognition
process.

\section{Clutter Segmentation Tool}

This section discusses our implementation of the \clutseg object recognition
system on top of \tod. We introduce three core concepts that enable \clutseg to
recognize the next object that is to be resolved from the clutter: First, a
guess ranking helps to discard guesses with low confidence. Second, guess
refinement aims at spending more computational resources on specifically
improving pose estimates for guesses with high confidence value, and returning
only the one with the highest confidence value. Third, guess rejection reduces
the probability of settling on the wrong guess.  Finally, this section covers
the tools that help to find good parameters for the clutseg system, and help
collect statistics to evaluate its performance.

\subsection{Guess Ranking}

A core idea of \clutseg is that we can have more confidence in a guess, when it
is supported by a large consensus set of inliers. It remains an assumption
throughout this chapter that more inliers are positively correlated with the
probability of the guess being correct. Measures for correctness are defined in
\refSection{subsection:measured-statistics}.

A {\it \gls{guess ranking}} is a function $r$ that assigns a real-valued score to a
guess $(i, \hat{T}, S)$. Different rankings can be defined and plugged into
\clutseg, such as a ranking based on proximity to the camera, or a combination
of multiple guess rankings. Yet, the ranking of interest is a guess ranking
$r_{S}$ that assigns scores to guesses according to the number of inliers
\begin{equation}
    r_S: (i, \hat{T}, S) \mapsto \left|S\right|
\end{equation}

The number of inliers is not only interesting because of the assumed correlation
with probability of being correct. It also simplifies the robotic task of grasping 
an object by supplying more 3D points.

\subsection{Guess Refinement}
\label{subsection:guess-refinement}

Given a guess, the idea behind {\it \gls{guess refinement}} is to spend additional
computation resources on selected guesses to reduce the expected error in pose
estimates. Guess refinement in \clutseg takes one of the guesses $(i, \hat{T},
S)$. \clutseg matches the query features only against the model features of the
$i$-th template object, as if the modelbase contained only one model. The pose
estimation using RANSAC leads to a new set of guesses, for instances of the
$i$-th template object only. The best guess according to the chosen guess
ranking is designated the {\it refined guess}.

Guess refinement aims at finding more matches between query features and the
single object whose pose is to be refined. By ignoring other models only
correspondences between the query scene and one model are found. Such an
approach is optimistic, and possibly dangerous, especially in cases where the
guess to refine is a false positive.  The validity of such an approach has at
least to be empirically verified.

\subsection{Guess Rejection}

A technique denoted as {\it \gls{guess rejection}} reduces the chance of returning an
incorrect guess. \clutseg rejects guesses whose guess ranking is smaller than a
configurable parameter value {\tt accept\_threshold}. How guess rejection
exactly works and in which scenarios it is useful is explained in the following
section.

\subsection{Recognition}

This subsection describes the full recognition process. It puts the ideas of
guess ranking, guess refinement and guess rejection into context.

Given a query scene, the \clutseg recognition process first calls \tod once to
produce a set of initial guesses $D$ for all objects in the modelbase. A
ranking defines a total order on $D$. The inlier ranking $r_S$ is used for this
by default. \clutseg chooses the first ranked guess in $D$ and computes the
refined guess. If the ranking of the refined guess is greater or equal than
{\tt accept\_threshold}, it is accepted and returned as the result of the
recognition process. In case the refined guess is rejected or no refined guess
was made at all, the algorithm proceeds to the next guess in $D$, and repeats
the refinement step. This iteration is performed until either a refined guess
is accepted, or \clutseg declares the scene to be empty because none of the initial
guesses in $D$ lead to a refined guess.

This scheme was built with the intention to avoid the problem of finding a good
operating point for \tod in ROC space. It is sufficient that \tod returns a set
$D$ where the high-ranked guesses are true positives. For one of the
high-ranked guesses, there are two cases to consider. If the high-ranked guess
is a true positive, then the refinement step should either confirm this guess,
or improve it, respectively. If the high-ranked guess is a false positive
against expectations, then the refinement step might still lead to a guess.  We
expect this refined guess to have low ranking, and therefore to be rejected; in
such a case, other high-ranked guesses are examined.

This two-staged recognition process requires the query features to be extracted
once.  Matching must be done at least twice. In the refinement step, there are
fewer features to match than in the initial step, the {\it detection step}.

\section{Parameter Optimization}
\label{section:parameter-optimization}. 

This section reviews the parameters of the \clutseg system, discusses their
nature and explains the approach that has been used to find a reasonable
parameter set. It introduces statistics that measure the performance of the
\clutseg system on sets of query scenes where ground truth is available.

\subsection{Parameter Space}
\label{subsection:parameter-space}

The \clutseg system has a fair amount of parameters. Many of these parameters
have already been introduced in the previous chapters, others still need to be
described. Table~\ref{table:parameters} shows an overview of all configurable
system parameters, except the parameters for feature extraction, which depend
on the choice of feature extractor, and which are treated separately.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{llll}
        Parameter & Range \\
        \hline
        {\tt detect\_matcher\_type} & categoric \\
        {\tt detect\_knn} & integer \\
        {\tt detect\_do\_ratio\_test} & boolean \\
        {\tt detect\_ratio\_threshold} & real \\
        {\tt detect\_ransac\_iterations\_count} & integer \\
        {\tt detect\_max\_projection\_error} & real \\
        {\tt detect\_min\_inliers\_count} & integer \\
        {\tt refine\_matcher\_type} & categoric \\
        {\tt refine\_knn} & integer \\
        {\tt refine\_ransac\_iterations\_count} & integer \\
        {\tt refine\_max\_projection\_error} & real \\
        {\tt refine\_min\_inliers\_count} & integer \\
        {\tt accept\_threshold} &  real \\
    \end{tabular}
    \caption{\clutseg parameters for matching and pose estimation}
    \label{table:parameters}
  \end{center}
\end{table}

Even if we only consider five different values for each of the ten of these
parameters, we obtain $5^{10} \approx 10^6$ different parameter sets.  Assuming
we would like to test two different matchers and three different feature
detectors. This would result in around $6 \cdot 10^6$ parameter sets. A full
factorial design for finding optimal parameters, as recommended in
\cite{Alpaydin2010}, becomes difficult. The pragmatic approach chosen in this
work is based on three ideas: First, we make an a-priori choice in algorithms.
Second, tools were developed together with \clutseg that support visual
inspection of experiment results in order to support an intuitive analysis.
Third, the exploration of parameter space is done selectively, and parameter
sets in promising regions are more thoroughly investigated. The danger of such
an approach is the chance to only find a local optimum instead of the targeted
global optimum, and the drawback is that results have to be formulated with
even more care since they are not supported by a full grid of samples in
parameter space.

\subsubsection{Feature Extraction Parameters}

Feature extraction parameters control the extraction of local 2D features.  A
choice needs to be made for the feature detector and the feature descriptor.
Feature detectors again need to be configured. The requirements described in
\refChapter{chapter:introduction} call for a feature detector that produces
features in good quantities, and it shall be reasonably fast. Hence, we chose
ORB, which takes three main parameters. The number of levels in the scale-space
pyramid is denoted by {\tt octaves}, the magnification factor between two
levels is called {\tt scale\_factor}.  Parameter {\tt n\_features} is a hint
for ORB on how many features are desired.

\subsubsection{Feature Matching Parameters}

The matching process in the detection step and the refinement step have some
parameters in common. It is possible to plug different matching algorithms into
\tod. The choice is denoted by {\tt detect\_matcher\_type} ({\tt
refine\_matcher\_type}), but partly dictated by the prior choice of feature
descriptor. A binary feature descriptor calls for a matcher that is
well-designed to work with binary vectors. Also, brute-force is hardly a choice
except for use as comparison to approximate nearest neighbour search
algorithms.  We chose the Locally Sensitive Hashing algorithm implemented in
\tod.  Parameter {\tt detect\_knn} ({\tt refine\_knn}) is a hint to the
matching algorithm about how many neighbours per query feature shall be
retrieved. The ratio test (\cite{Lowe2004}) can be enabled or disabled by {\tt
detect\_do\_ratio\_test} and configured by {\tt detect\_ratio\_threshold} for
the detection step. In the refinement step it does not make sense and is
disabled by default.

\subsubsection{Pose Estimation Parameters}

The pose estimation step in detection and refinement depends on three
parameters each. RANSAC is controlled by {\tt
detect\_ransac\_iterations\_count} ({\tt refine\_ransac\_iterations\_count})
and {\tt detect\_max\_projection\_error} ({\tt
refine\_max\_projection\_error}). No further pose estimates for an object are
generated once RANSAC returns an estimate with a consensus set of size less
than {\tt detect\_min\_inliers\_count} ({\tt refine\_min\_inliers\_count}).

\subsection{Measured Statistics}
\label{subsection:measured-statistics}

This subsection describes statistics that are recorded by \clutseg for a set of
scenes where ground truth is available. We explain how errors and scores are
computed for a given query scene or whole sets of query scenes. We cover the
statistics that are recorded in the learning and the recognition process, which
are aimed at simplifying the reasoning about the system's performance and about
the influence of parameters. 

\subsubsection{Learning Statistics}

When constructing the models from multiple views, some simple statistics are
recorded. The model construction is part of \tod's codebase. It had to be
modified in order to keep track of the minimum, maximum, and average number of
features extracted on the views. Also, the number of times, where the
computation of the object-camera transformation failed due to undetected
chessboards, is tracked.  The time required for building the modelbase is
recorded as well. Results are presented in \refChapter{chapter:evaluation}.

\subsubsection{Ground Truth}

The error \clutseg makes on a query scene can only be computed when ground
truth is available. The ground truth data for a scene consist of a set of labels
that tell which objects can be seen at which location and which orientation in
the scene. Thus, the ground truth $G$ can be described as a set of labels
\begin{equation}
    G = \cbr{ (i,\ \ocTi{i}) : \text{object $i$ is on scene at pose $\ocTi{}$} }
\end{equation}

\subsubsection{Classification Error}

\clutseg only attempts to partially estimate ground truth. It does not try to
label all objects in the scene, but only one. Mapping to ROC terminology is not
straight-forward. Ignoring the estimated pose, it is possible to put a guess
made by \clutseg into one of four categories: (a) True positive, if the guessed
object is actually on the scene; (b) false positive if, the guessed object is
not on the scene; (c) true negative, if there is no guess and the scene is
empty; (d) false negative, if the scene shows objects but \clutseg did not make
a guess.

Unfortunately, this scheme does not cover the case in which two objects have
been confused. In the application scenario, several objects are expected to be
on the scene. 

\subsubsection{Pose Estimation Error}

The error in the estimated pose can only be computed in case the guessed object
is actually visible in the scene. In case it is visible, the translational
error $e_t$ given ground truth pose $\ocT$ and pose estimate $\ocTh$ is given
by the distance between the estimated location and the true location of the
object origin $\vec{0} = (0, 0, 0)^T$:

\begin{equation}
    e_t = \normtwo{\ \ocTh \pth{ \vec{0} } -\ \ocT \pth{ \vec{0} }\ }
\end{equation}

The orientation error $e_\alpha$ is computed from the ground truth orientation $r$
and estimated orientation $\hat{r}$, specified in axis-angle representation, and 
using the dot-product, we have

\begin{equation}
    e_\alpha = \norm{ \arccos \pth{ \frac{\vec{r}^t\ \hat{\vec{r}}}{\normtwo{\vec{r}}\ \normtwo{\hat{\vec{r}}}} } }
\end{equation}

Rodrigues' rotation formula can be used for efficiently converting back and
forth between rotation matrix and axis-angle representation, and is readily
available in \opencv.

\subsubsection{Guess Scores}

Here, we present a score that measures \clutseg's performance on a single
query scene. The goal is to find a score function that closely models the
utility of a guess made for a query scene.

If the guessed object is not on the scene, clearly the utility of such a guess
is zero. Also, we consider the utility to be zero, if the orientation and
translation error is beyond a certain margin of error. The idea is therefore to
introduce a combined classification and estimation score. 

Let $\alpha_{max} = \frac{\pi}{9}$ and $t_{max} = 3\mbox{cm}$ be the maximum
tolerable errors in orientation and translation, respectively. The choices are
consistent with the {\it Solutions in Perceptions Challenge 2011}.  Then, for a
query scene with ground truth $G$ and a guess $g$, we define the {\it \gls{guess score}}
function $u$ as

\begin{equation}
u: G, g \mapsto
    \begin{dcases}
         1 & \text{if scene is empty and no guess made} \\
         0 & \text{if guessed object is not on scene} \\
         1 - \min\{1, \frac{e_t^2}{t_{max}^2} + \frac{e_\alpha^2}{\alpha_{max}^2}\} & \text{if guessed object is on scene}
    \end{dcases}
\end{equation}

The presented guess score function is piecewise continuous. Thus, it retains
more information than a simple statistic that records the binary observation
whether the guess is within or beyond an error margin. The influence of error
in pose is bounded, and outliers do not influence average the score function
too much. On the other hand, it does not provide useful gradient information in
regions of bad parameter sets, given the function's flat surface beyond a
certain region defined by $t_{max}$ and $\alpha_{max}$.

\subsubsection{Success Rate}

We define an object to be successfully recognized if and only if its estimated
pose does not exceed the error margins for translation ($t_{max}$) and rotation
($\alpha_{max}$). \clutseg produces only one guess per scene. The {\it success
rate} is the ratio between the number of scenes where \clutseg successfully
recognized an object and the number of scenes in total. 

\subsubsection{Recognition Statistics}

\clutseg also collects statistics about the recognition process that do not involve
ground truth. These statistics cover the number of guesses, matches, inliers in
the detection and in the refinement stage. The runtime is recorded as well.

\subsection{Experiment Strategy}

Experience showed that it takes around 2-7 minutes to evaluate \clutseg's
performance on a validation set with 21 images. Hence it was vital to make a
decision which regions of parameter space to explore first and which regions to
explore next. In this work, the strategy was to select a few regions, and then
to pursue search in the neighbourhood of those parameter sets that performed
well in the experiments conducted so far. To a certain extent, the strategy
resembles genetic programming.

\subsection{Experiment Runner}

In this subsection we describe \clutseg's experiment runner that permits to
test different parameter sets against a set of scenes. We describe how a
database was employed for keeping track of parameter sets and experiment
results, the visualization tools that make the results accessible for analysis,
and the major implementation decisions that saved computing resources.

\subsubsection{Experiment Database}

\clutseg uses a \sqlite database for keeping track of parameter sets and the
recorded statistics, as introduced in Sections~\ref{subsection:parameter-space}
and \ref{subsection:measured-statistics}, respectively. The experiment runner
can be executed in the background. When being idle, it polls the database for
new parameter sets.  New parameter sets are tested for validity with
computationally cheap tests.  The failing ones are marked and skipped, thus one
failing experiment does not kill the process. A no-frills object-relational
mapper provides a convenient interface to work with the database. The primary
purpose of the database consists in selecting experiment inputs and outputs in
a central, accessible and convenient location.  Few efforts have been spent on
normalization, performance tuning or any other aspects that were not related to
its primary purpose. For convenience, data can be selected from various
database views.

\subsubsection{Modelbase Cache}

Extracting models is computationally expensive and may take several minutes.
As long as the feature parameters do not change in several experiments, there
is no need to regenerate the models, and the modelbase is retrieved by the SHA1
hashcode of the feature parameters. A modelbase for four template objects takes
up around five Megabytes on the filesystem, depending on the choice of feature
detector and descriptor. \tod stores the modelbase as compressed YAML (Yet
Another Markup Language) text files. If disk space were scarce and reading the
modelbase quickly into memory were an issue, then binary files would be a
better choice.

\subsubsection{Visualization Tools}

\clutseg employs \opencv and \rproject for visualizing the recognition results
in the query scene and the statistical data acquired in experiments.  It
provides functions for visualizing guesses and ground truth in parallel,
showing inliers, translational and rotational errors in the manner of a
heads-up display. It provides command-line tools to inspect views of the
modelbase, that is the region of interest, the extracted keypoints, and the
estimated pose.  Imagemagick is used to generate montage images that show the
results of one experiment at once. Finally, \tod allows to visualize matches
between the query scene and the modelbase. \rproject is used for pulling data
from the database and for plotting the data. 

