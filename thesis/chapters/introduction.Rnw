\section{Intended Purpose}

Object recognition is one of the fundamental tasks in computer vision. One of
its applications is in robotics. Especially in household environments it is
often not only necessary for a robot to detect the presence of an object, but
also to locate this object in order to grasp it, and to make further decisions
on a semantic level.

A particular application addressed by this work is a robot in a household
environment which is given the task to go shopping. The robot then empties the
shopping bag onto a table, and then plans to move the items bought to
appropriate storage facilities. The shopping items on the table form a
cluttered scene. Most of the shopping items are rigid bodies and can be
distinguished by characteristic texture, composed of pictorial descriptions of
the product, labels, and company logos. High levels of occlusion and the
presence of several such textured objects in a single scene renders recognition
of these shopping items difficult. The objects might be arbitrarily oriented,
and prior assumptions about their principal axes are likely invalid.

The system presented in this work enables a PR2 robot to locate one object in
the cluttered scene and provide sufficient information to a grasping pipeline,
such that the clutter can be resolved by removing items one-by-one from the
scene. Although, in the shopping scenario, the robot might have prior knowledge
about which objects to locate in the scene, this information is not used in
this work, in order to develop a system that is general enough to be applied to
other application scenarios.

\section{Problem Statement}
\label{section:problem-statement}

The challenge is therefore to correctly identify a rigid textured object and
determine its pose in a scene with clutter and high occlusion, and subsequently
provide enough information to allow the robot to grasp the object and to remove it
from the cluttered scene.

\section{Related Work}

In literature many approaches to object recognition are described. They differ
in scope, targeted application and techniques. Those techniques using local
features have been especially relevant to this work. 

Tuytelaars and Mikolajczyk qualitatively review local feature detectors
and descriptors \cite{Tuytelaars2007}, and provide guidance in making an
appropriate choice between a multitude of available detectors and descriptors.
Lowe's work on the scale-invariant feature transform explains the principles
of the scale-space pyramid and rotation invariance of features \cite{Lowe1999}.
Rosten and Drummond introduce the FAST feature detector \cite{Rosten2006}.
% while Agrawal et al. presented the CenSurE feature detector \cite{Agrawal2008},
% both designed to be efficient.
Calonder et al. describe BRIEF, a feature descriptor whose generated descriptor
vectors are efficient to match \cite{Calonder2010}. Oriented BRIEF, derived
from FAST and BRIEF, is the feature detector and descriptor chosen in this
work, although literature had not been present at the time of writing.
Matching observed features with model features is a common sub-task in object
recognition. 
% For this, Muja described the FLANN library \cite{Muja2009}.
Gionis et al. introduce, and Slaney and Casey summarize Locality Sensitive
Hashing \cite{Gionis1999, Slaney2008}, which finds approximate
nearest neighbors.
% Lowe showed how to recognize
% objects up to a 20 degree rotation using the scale-invariant feature transform
% (SIFT), making use of the Hough Transform where correspondences vote for the
% pose of an object \cite{Lowe1999}.
% Hough voting has also been used in a system presented by Drost et
% al. \cite{Drost2010}.
Fischler and Bolles excellently explain RANSAC and deal with the
perspective-n-point problem \cite{Fischler1981}.  Dogar and Srinivasa recently
researched the problem of robustly grasping objects in a cluttered scene
\cite{Dogar2010}. Forsyth and Ponce's work on computer vision served as a
reference and influenced the terminology in this work \cite{Forsyth2003}. The
same applies to Gonzalez and Woods in the field of image processing
\cite{Gonzalez2010}.  Alpaydin's excellent introduction to machine learning
\cite{Alpaydin2010} has influenced our experiment design and parameter
optimization. Fawcett \cite{Fawcett2006} published a detailed guide on how to
evaluate the performance of classifiers through analysis of receiver operating
characteristics. Melsa and Cohn \cite{Melsa1978} cover decision and estimation
theory.

% Haltakov and Pangercic have developed the ODUFinder library for the Robot
% Operating System to detect both non-textured objects and textured objects,
% the latter by using SIFT and vocabulary trees.

\section{Selected Approach}

\subsubsection{General Approach}

Objects generally differ in shape and appearance. Recognizing the shopping
items as shown in \refFigure{figure:intro} (a) is difficult. The shopping items
are often shaped similarly. Worse, different products are sold in packagings of
the same shape. The distinguishable feature of these objects is texture.

Thus, it seemed natural to rely on texture to recognize objects. Originally, it
had been considered to combine the information from textured surface with the
information contained in the shape of the rigid bodies. Finally, we
concentrated on using texture information exclusively, and based our system on
the existing Textured Object Detection (\tod) stack in the Robot Operating
System.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=\textwidth]{media/clutter-pr2-kinect}
    \end{center}
    \caption{}
    \label{figure:clutter-pr2-kinect}
\end{figure}

We required our system to run on a PR2 robot at Technische Universität München,
as depicted in \refFigure{figure:intro} (b). The PR2 is a robotic platform
produced by Willow Garage. A Kinect RGB-depth camera is mounted on top of the
robot. It simultaneously provides color images and 3D depth information. The
Kinect camera measures the time-of-flight of near-infrared rays to compute the
distance between camera and points in 3D space within a range of 0.8-3.5 meters
\cite{PrimeSense2010}, and an example is shown in \refFigure{figure:intro} (c).

In the presented approach, we first learn a model from each rigid textured
object, one by one. An object is rotated in front of the camera, which takes
images and 3D scans from different views of the object. Then, local 2D features
are extracted from each grayscale image. The local 2D features, together with
the associated 3D points on the object's surface, form the model of this
object. The model is a sparse point cloud and is described in a standard
coordinate system.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=\textwidth]{media/hm-features-model}
    \end{center}
    \caption{}
    \label{figure:hm-features-model}
\end{figure}

Given a single view of a scene, we use the models to recognize instances of the
learnt template objects in this query scene. First, local 2D features are
extracted from a grayscale image depicting the query scene. Nearest-neighbor
search permits to find correspondences between the query features and the
models. Based on these correspondences, a model-fitting algorithm that is
robust to outliers, aligns models to the 3D scene such that their projections
onto the image plane best explain the observed data.  Finally, if an aligned
model sufficiently explains the correspondences, the object is said to appear
in the query scene at the aligned pose, or equivalently a {\it guess} is made.
A {\it confidence value} measures the degree to which a guess explains the
observed data. Although we do not explicitly make use of a statistical model,
this is a likelihood-based approach.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=\textwidth]{media/correspondences-pose}
    \end{center}
    \caption{}
    \label{figure:correspondences-pose}
\end{figure}

\subsubsection{Implementation}

The basic steps in learning models and recognizing objects are already
implemented in the Textured Object Detection stack. \tod supports the use
different local 2D feature detectors and descriptors. It supports a range of
nearest-neighbor search algorithms that find correspondences between feature
sets. Finally, \tod uses RANSAC as the afore-mentioned model-fitting algorithm
to estimate the object poses. The confidence value is measured by the number of
correspondences consistent with this pose. 

We present a new system called \clutseg, built on top of \tod. Its purpose is
three-fold. Primarily, it adapts \tod to our purpose of resolving a cluttered
scene.  Second, it helps choose between the multitude of available feature
detectors and descriptors and select one of the nearest-neighbor search
algorithms. Third, for \tod and \clutseg have many parameters, experiment
runner helps find good parameter values.

The cluttered scenes (\refFigure{figure:intro} (b)) in this work do not only
exhibit high rates of occlusion, but also show objects in arbitrary
orientation.  For our requirements, it is sufficient to resolve the clutter
object by object. Hence, we chose to concentrate on finding the pose of only
one object in the scene. \clutseg does this by ranking the initial guesses from
\tod by confidence value. Then, the best-ranked guesses are refined until a
guess with sufficiently high confidence value is found.

We chose Oriented BRIEF as a feature detector and descriptor because its binary
features were appealing and it had not been subject to research so far. We
traded correctness for speed and chose Locality Sensitive Hashing to find
approximate nearest neighbors.

% The new Oriented
% BRIEF 2D local feature detector additionally aroused our curiosity.

% Our system has been built on top of the Textured Object Detection packa
% targeting the PR2 robot. It describes objects by a set of local 2D features
% extracted from multiple views in order to account for instances appearing in
% arbitrary orientations. Oriented BRIEF, a feature detector and binary feature
% descriptor, is used to generate local 2D features for both the models and the
% query scenes. Local Sensitive Hashing, an approximate nearest neighbor search
% algorithm, finds correspondences between features of the models and the query
% scene. RANSAC is used to solve the perspective-n-point problem to generate a
% set of pose estimates. The pose estimate with the largest consensus set
% returned by RANSAC is then refined. The refinement involves matching features
% against a particular model and repeating the pose estimation process.

% Larger consensus sets are preferred, because they make robotic grasping easier.
% The system heavily exploits the requirement that a robot needs only to locate
% one object in the scene. An experiment runner conveniently permits to test
% different parameter configurations against a validation set, and such supports
% parameter optimization.

