<<echo=FALSE>>=
    source("../scripts/common.R")
@

This chapter presents the experimental results of the \clutseg system, on the
validation set, on a test set and results from a live test on a PR2 robot. It
covers how the modelbase was built, how validation data, ground truth and the
test data have been obtained. The parameter set found by experimentation is is
discussed. Scenarios are presented in which the system performed well, and also
limitations and issues that became apparent during the evaluation.

\section{Data Acquisition}
\label{section:data-acquisition}

This section shows how all the data required for constructing the modelbases,
the validation set and the test set was obtained. Those three artifacts all
required raw data, which was collected in a similar fashion, using the same
hardware and software setup.

% The Robot
% Operating System running on the PR2 robot is organized as a distributed system
% of nodes, which communicate via topics. Nodes can subscribe and publish to
% topics. Data from topics can be recorded in bag files.

\subsection{Common Setup}

We used the Willow Garage PR2 robot at TUM to collect raw data.  On top of the
PR2, a Kinect RGB-D camera was mounted. A wooden board with two chessboards
attached was setup in about TODO meters distance from the camera. This board
served as a rotating table. The chessboards were printed on white DIN-A4
paper using a laser printer, and around them was enough white margin as
recommended in the documentation of method {\tt findChessboardCorners} in
OpenCV. The recording took place in an indoor laboratory environment whose
lighting conditions were roughly similar to the ones to be expected in an
application scenario. The board was manually rotated, carefully and not too
fast.  The objects must not move with respect to the fiducial markers, which
are attached to the object coordinate system.

  The PR2 robot was controlled by the Robot Operating System, which is a
distributed system where nodes communicate by publishing and subscribing to
topics. In our case, the interesting topics were the image and the 3-d point
cloud, together forming the sought-after depth images. In ROS, data can be
recorded in so-called bags. For depth images, these bags quickly grow large.

  The resolution of the image was 1280x1024 pixels, and the dense point cloud
had a resolution of 640x480. The sampling rate was set to about one depth image
per second. The camera matrix needs to be recorded as well, for it is required
at least in solving the point-to-point correspondence problem in pose
estimation. 

% TODO: write chessboard, not chessboard, in order to be consistent with OpenCV, in order to be consistent with OpenCV


\subsection{Modelbases}
\label{subsection:modelbase}

% Acquiring modelbases was done in two independent steps. First, raw data
% was collected once. Second, several modelbases were built according to 
% the feature parameter specifications in the experiments. 

\tod requires multiple views from the template object. For the modelbases, we
saved about 60-70 views per object to a bag file, which has been observed to
work better than with only 40 views. For each object, around 1 Gigabyte of data
was recorded. The next preparatory step was to uncompress the bag files with a
script included in both \clutseg and \tod, then perform pose estimation and
masking as described in Section \ref{subsection:learning}. This being done, the
final step of model extraction was left up to the experiment runner, since it
depends on the feature parameters. The experiment runner needs access to the
raw data. The resulting models of each template object were much smaller in
size, and took up around 1 Megabyte of disk space each, three magnitudes less
than the original raw data.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{media/modelbase}
        \caption[Modelbase used in evaluation]{The modelbase with four different template objects used in evaluation}
    \end{center}
\end{figure}

Raw data was collected for four different household objects, commonly found in
German supermarkets. These are {\it assam\_tea}, {\it haltbare\_milch}, {\it
icedtea} and {\it jacobs\_coffee}. The objects have different properties. For
example, {\it assam\_tea} is smaller than {\it icedtea} whose frontside shows
much more characteristic texture than its back that is full of smallprint. {\it
jacobs\_coffee} show texture that repeats on different sides of the object.
Finally, {\it haltbare\_milch} contains characteristic texture which is
confined only to certain regions on its surface.

Equation~\ref{equation:fiducial-object} requires that a template object must
not move with respect to the fiducial markers when collecting raw data. This
would be equivalent to redefining the object coordinate system.

\subsection{Validation Set}

The validation set, which was used to optimize parameter sets, was collected
similarly to the data for the modelbases. The fiducial markers were required
for computing ground truth for the objects on the validation scenes. We
recorded around 50-60 scenes, each of them showing three instances of objects
in the modelbase. Out of those, we chose 21 scenes at roughly equal angle steps
to form the validation set.

Given a scene, ground truth requires to compute the object-view transformations
$_oT^v_i$, $_oT^v_j$, and $_oT^v_k$ for the three objects $i$, $j$ and $k$ in
the scene. We placed object $j$ in the center between the fiducial markers, such
that 

\begin{equation}
    _oT^v_j: \vec{p} \mapsto\ _fT^v \left(\vec{p}\right)
\end{equation}

In other words, object $j$ was placed on the rotating table exactly as when raw
data for modelbase had been collected. Pencil markers on the template objects
helped us remembered the correct arrangement.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{media/fiducial-object-3}
        \caption{Relation between fiducial markers and three object coordinate systems}
    \end{center}
\end{figure}

The other two objects $i$ and $k$ were placed upright next to object $i$, with
offset of $-0.15$ and $0.13$ in x-direction of fiducial coordinates

\begin{equation}
    _oT^v_i: \vec{p} \mapsto\ _fT^v\left(\vec{p} - \begin{pmatrix} 0.15 \\ 0 \\ 0 \end{pmatrix} \right)
\end{equation}

\begin{equation}
    _oT^v_k: \vec{p} \mapsto\ _fT^v\left(\vec{p} + \begin{pmatrix} 0.13 \\ 0 \\ 0 \end{pmatrix} \right)
\end{equation}

Units are specified in metres.

% TODO: future work might involve computation of ground truth in more complex
% configurations


% TODO: figure object-coordinate-syste

\subsection{Test Set}

The test set was made of cluttered scenes of various character. All four
objects are shown on the test scene. Some scenes showed objects in upright
position, but some objects occluding others. Other scenes showed objects in
arbitrary orientation and medium occlusion. Other, yet more difficult scenes,
showed objects in arbitrary orientation and severe occlusion.

The test set data has been collected similarly to the validation set, but
ground truth was not computed. The evaluation on the test set was therefore
restricted to visual analysis.

\section{Experiments}

This section describes the regions in parameter space that have been covered by
experiments. This knowledge is required to interpret data from multiple
experiments. It also presents a selected parameter set that has been found to
work well on the validation set.

\subsection{Searched Parameter Space}


<<echo=FALSE>>=
    # Get the number of experiments in total
    e_all = dbGetQuery(con, paste(
        "select count(e.id) as cnt,",
        "sum(r.train_runtime) as total_train_runtime,",
        "avg(r.train_runtime) as avg_train_runtime,",
        "sum(r.test_runtime) as total_test_runtime,",
        "avg(r.test_runtime) as avg_test_runtime",
        "from experiment e",
        "join response r on e.response_id = r.id"));
    # Get the number of experiments with FAST and RBRIEF
    e_fast_rbrief = dbGetQuery(con, paste(
          "select count(e.id) as cnt",
          "from experiment e",
          "join paramset p on e.paramset_id = p.id",
          "join pms_fe t on p.train_pms_fe_id = t.id",
          "join pms_fe r on p.recog_pms_fe_id = r.id",
          "where e.response_id not null",
          "and t.detector_type = 'FAST'",
          "and t.descriptor_type = 'rBRIEF'",
          "and r.detector_type = 'FAST'",
          "and r.descriptor_type = 'rBRIEF'"))
    # Get the number of experiments with ORB
    e_orb = dbGetQuery(con, paste(
          "select count(e.id) as cnt",
          "from experiment e",
          "join paramset p on e.paramset_id = p.id",
          "join pms_fe t on p.train_pms_fe_id = t.id",
          "join pms_fe r on p.recog_pms_fe_id = r.id",
          "where e.response_id not null",
          "and t.detector_type = 'ORB'",
          "and t.descriptor_type = 'ORB'",
          "and t.extractor_type = 'ORB'",
          "and r.detector_type = 'ORB'",
          "and r.descriptor_type = 'ORB'",
          "and r.extractor_type = 'ORB'"))
@

In total, \Sexpr{e_all$cnt} experiments with different parameter sets have been
conducted. In \Sexpr{e_fast_rbrief$cnt} experiments, we used FAST as a feature
detector and rBRIEF as a feature descriptor. In \Sexpr{e_orb$cnt} experiments,
we chose ORB instead. 

Altogether, the experiments took more than \Sexpr{floor(e_all$total_test_runtime / 3600)}~hours
of wall-time to compute, an average of
\Sexpr{round(e_all$avg_test_runtime / 60, 1)}~minutes per experiment.
Table~\ref{table:hardsoftware-parameter-optimization} lists the hardware and software
configuration. Note that the hardware is more than six years old.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{lll}
        Component & Type \\
        \hline
        Mainboard & ASUS K8V SE Deluxe \\
        CPU & AMD Athlon64 3000+ S754 \\
        RAM & 1 Gigabyte DDR-RAM & \\
        OS & Ubuntu 10.10 Maverick \\
    \end{tabular}
    \caption{Hard- and software configuration for parameter optimization}
    \label{table:hardsoftware-parameter-optimization}
  \end{center}
\end{table}

\begin{figure}
    \begin{center}
<<fig=true,png=true,echo=false,size=0.5>>=
    r_all = dbGetQuery(con, paste(
        "select succ_rate",
        "from response"));
    hist(r_all$succ_rate, xlab = "Success Rate", main = "Histogram of Success Rates")
@
    \caption{Histogram of success rates in experiments}
    \end{center}
\end{figure}

<<echo=false>>=
    rm(e_all)
    rm(e_fast_rbrief)
    rm(e_orb)
@

\subsection{Observations}

\subsection{Selected Parameter Set}

<<echo=false>>=
    sps = dbGetQuery(con, paste(
          "select e.id as id,",
          # parameters
          "tf.detector_type as detector_type,",
          "tf.descriptor_type as descriptor_type,",
          "tf.extractor_type as extractor_type,",
          "tf.n_features as n_features,",
          "tf.octaves as octaves,",
          "tf.scale_factor as scale_factor,",
          "dm.matcher_type as detect_matcher_type,",
          "dm.knn as detect_knn,",
          "dm.do_ratio_test as detect_do_ratio_test,",
          "dm.ratio_threshold as detect_ratio_threshold,",
          "dg.ransac_iterations_count as detect_ransac_iterations_count,",
          "dg.max_projection_error as detect_max_projection_error,",
          "dg.min_inliers_count as detect_min_inliers_count,",
          "rm.matcher_type as refine_matcher_type,",
          "rm.knn as refine_knn,",
          "rm.do_ratio_test as refine_do_ratio_test,",
          "rm.ratio_threshold as refine_ratio_threshold,",
          "rg.ransac_iterations_count as refine_ransac_iterations_count,",
          "rg.max_projection_error as refine_max_projection_error,",
          "rg.min_inliers_count as refine_min_inliers_count,",
          "g.accept_threshold as accept_threshold,",
          # statistics
          "r.succ_rate as succ_rate,",
          "r.value as value",
          # joins 
          "from experiment e",
          "join response r on e.response_id = r.id",
          "join paramset p on e.paramset_id = p.id",
          "join pms_fe tf on p.train_pms_fe_id = tf.id",
          "join pms_fe rf on p.recog_pms_fe_id = rf.id",
          "join pms_match dm on p.detect_pms_match_id = dm.id",
          "join pms_match rm on p.refine_pms_match_id = rm.id",
          "join pms_guess dg on p.detect_pms_guess_id = dg.id",
          "join pms_guess rg on p.refine_pms_guess_id = rg.id",
          "join pms_clutseg g on p.recog_pms_clutseg_id = g.id",
          "where tf.detector_type = 'ORB'",
          "and tf.descriptor_type = 'ORB'",
          "and tf.extractor_type = 'ORB'",
          "and rf.detector_type = tf.detector_type",
          "and rf.descriptor_type = tf.descriptor_type",
          "and rf.extractor_type = tf.extractor_type",
          "and rf.n_features = tf.n_features",
          "and rf.octaves = tf.octaves",
          "and rf.scale_factor = tf.scale_factor",
          "order by r.succ_rate desc limit 1"))
@

Here we present a parameter set, dubbed SPS, that has been found to work well
on the validation set. It contains values for all configurable system
parameters. Table~\ref{table:selected-parameters-features} provides values for
feature extraction. We used the same feature extraction parameters for the
modelbase and for the query scenes. We used the default values from OpenCV for
parameters {\tt octaves} and {\tt scale\_factor}.
% TODO: introduce \n_features octaves and scale\_factor in theoretical and implementation part

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{ll}
        Parameter & Value \\
        \hline
        {\tt detector\_type} & \Sexpr{sps$detector_type} \\
        {\tt descriptor\_type} & \Sexpr{sps$descriptor_type} \\
        {\tt extractor\_type} & \Sexpr{sps$extractor_type} \\
        {\tt n\_features} & \Sexpr{sps$n_features} \\
        {\tt octaves} & \Sexpr{sps$octaves} \\
        {\tt scale\_factor} & \Sexpr{sps$scale_factor} \\
    \end{tabular}
    \caption{Selected \clutseg parameters for feature extraction}
    \label{table:selected-parameters-features}
  \end{center}
\end{table}

Table~\ref{table:sps-match-guess} shows parameter values for the
detection and refinement stages. We used Locality Sensitive Hashing in both
stages to match the binary features produced by Oriented BRIEF.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{ll}
        Parameter & Value \\
        \hline
        {\tt detect\_matcher\_type} & \Sexpr{sps$detect_matcher_type} \\
        {\tt detect\_knn} & \Sexpr{sps$detect_knn} \\
        {\tt detect\_do\_ratio\_test} & \Sexpr{sps$detect_do_ratio_test} \\
        {\tt detect\_ratio\_threshold} & \Sexpr{sps$detect_ratio_threshold} \\
        {\tt detect\_ransac\_iterations\_count} & \Sexpr{sps$detect_ransac_iterations_count} \\
        {\tt detect\_max\_projection\_error} & \Sexpr{sps$detect_max_projection_error} \\
        {\tt detect\_min\_inliers\_count} & \Sexpr{sps$detect_min_inliers_count} \\
        {\tt refine\_matcher\_type} & \Sexpr{sps$refine_matcher_type} \\
        {\tt refine\_knn} & \Sexpr{sps$refine_knn} \\
        {\tt refine\_ransac\_iterations\_count} & \Sexpr{sps$refine_ransac_iterations_count} \\
        {\tt refine\_max\_projection\_error} & \Sexpr{sps$refine_max_projection_error} \\
        {\tt refine\_min\_inliers\_count} & \Sexpr{sps$refine_min_inliers_count} \\
        {\tt accept\_threshold} & \Sexpr{sps$accept_threshold}
    \end{tabular}
    \caption{Selected \clutseg parameters for matching and pose estimation}
    \label{table:sps-match-guess}
  \end{center}
\end{table}

\section{Performance}

Performance of the \clutseg system has been tested on the validation set, on a
separate test set and in a live test on the PR2 robot. This section reviews
performance for a parameter set that worked well on the validation set. It also
covers observations made on a broad range of experiments.

\subsection{Performance on Validation Set}

The \clutseg system correctly located and labeled an object within error bounds
of 3 cm and 20 degrees in rotation in \Sexpr{floor(sps$succ_rate * 100)}\% of
the validation scenes. It got an average scene score of \Sexpr{round(sps$value, 2)}.
The success rate is not predictive, though, as the validation set was used for
optimizing the system. Because ground truth was available, more precise data
can be presented on the validation set than for the test set. 

% TODO: talk about detection stage

\subsection{Performance on Test Set}
\subsection{Performance in Live Test}

\section{Issues}

% \subsection{Prior Assumption about Orientation}

% many presentations of object recognition systems seem to make the prior assumption of objects appearing in upright pose.
% the performance on the test set which showed objects in arbitrary orientation was below expectations
% the model features were extracted by a scan of the object from positions on a circle
% the query features are observed from a position on a sphere around the object
% features are rotation-invariant only to rotation of objects around an axis parallel to the optical axis 
% the number of matches significantly reduced on the images with objects in arbitrary orientation

% Many presentations of object recognition systems in media and literature seem
% to make the prior assumption that objects in query scenes are oriented in
% upright position. Our test set shows scenes with objects not only in clutter,
% but also in arbitrary orientation.  This adds extra difficulty. \clutseg had
% problems with locating an object at least in TODO of TODO test scenes. When we
% collected raw data for the modelbase, we made an implicit prior assumption
% about the orientation of objects in query scenes. We scanned the template
% object from several points on a circle around the object. Yet, the query scenes
% show instances from arbitrary points on spheres around them.

% Rotation-invariant feature detectors cope with 3D rotation of objects around an
% axis parallel to the optical axis. SIFT features still operate up to 20 degrees
% of rotation around an axis parallel to the image plane \cite{Lowe1999}. % TODO: confirm this

\subsection{Incomplete model}

The bottom of the template object is facing the rotating table and is invisible
to the camera from all viewpoints. This does not matter as long as instances of
these template objects are standing upright. This does matter when instances occur
in arbitrary orientation.

\subsection{Repetitions in texture}

The system had difficulties to recognize {\it icedtea} when shown from a
certain angle.  It turned out that {\it icedtea} has smallprint almost all over
its backside. The same set of letters repeatably appear over a large area,
which probably makes it hard to obtain informative features.

\subsection{Maximum projection error in RANSAC}

Generally, the system relies on the size of the consensus set to be a strong
indicator for the goodness of an estimate. It was observed, though, that in one
experiment, the model was badly aligned, yet had strong support by many
inliers. The reason for the large consensus set is that the projection error is
a measure on the image plane, and matching keypoints are designated inliers as
long as they do not exceed the maximum projection error which is a circle
induced by L2-norm on the image plane. The problem can be traced back to the
fact objects that are far away from the camera merely degenerate to a point on
the image plane. All correspondences within the circle are designated inliers.

% TODO: reformulate

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.6\textwidth]{media/ransac-max-projection-error-deficiency}
        \caption[Schematic explanation of distant pose estimate with many inliers]{Far-away pose estimate can result in many inliers}
    \end{center}
\end{figure}

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.6\textwidth]{media/ransac-max-projection-error-deficiency-sample}
        \caption[Sample of distant pose estimate with many inliers]{On a test scene, the badly aligned pose still results in many inliers}
    \end{center}
\end{figure}




