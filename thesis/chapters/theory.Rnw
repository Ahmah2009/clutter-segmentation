This chapter provides the theoretical foundations for building the 3-d object
recognition system, as will be presented in chapter
\ref{chapter:implementation}. It provides a short review of required
mathematical tools and the basics of multi-view 3-d object recognition. It
covers the description of images using local features with state-of-the-art
techniques, how to quickly establish correspondences between known and unseen
images, and finally how to estimate poses under uncertainty and noise using.
Also, this chapter shows how the implemented system relates to the mathematical
framework of classifiers and estimators, which is helpful for the evaluation
given in chapter \ref{chapter:evaluation}.

\section{Prerequisites in Geometry}

This section establishs mathematical tools and notation that are used
throughout this work, and especially become significant in sections
\ref{subsection:pose-estimation}, \ref{subsection:learning},
\ref{subsection:recognition}, and \ref{section:data-acquisition}.

\subsection{Rigid Transformations}

% TODO: reference to definition of rigid transformations

A {\it rigid transformation} is defined as combination of an orthogonal
transformation and translation in Euclidean space. A {\it proper rigid
transformation} requires the orthogonal transformation to be a rotation. In
three dimensions, a proper rigid transformation can be represented by a
rotation matrix $R \in \mathbb{R}^3$ and a translation vector $\vec{t} \in
\mathbb{R}^3$.

\begin{equation}
    T: \mathbb{R}^3 \to \mathbb{R}^3, \vec{v} \mapsto R\vec{v} + \vec{t}
\end{equation}

A proper rigid transformation, abbreviated by PRT in the following, can be
used to describe a rigid motion of an object. It can also be used to transform
the coordinate systems, whilst objects in the scene remain unchanged with
respect to a fixed world frame. The latter interpretation is prevalent in this
work. The inverse transformation is given by
 
\begin{equation}
    T^{-1}: \mathbb{R}^3 \to \mathbb{R}^3, \vec{v} \mapsto R^{-1}(\vec{v} - \vec{t})
\end{equation}

\subsection{Object-View Transformations}

Coordinates of points in 3-d space can be specified with respect to any
coordinate frame. In this work, two different kinds of coordinate systems are
used. The first kind is attached to the camera and is called {\it view
coordinate frame}. The second kind is attached to an object or its model and is
called {\it object coordinate frame}. The PRT between object coordinates and
view coordinates will be called {\it object-view transformation}.  Object
coordinates are commonly denoted by $(x, y, z)$, and view coordinates by $(u,
v, w)$, the object-view transformation subsequently $_{xyz}T^{uvw}$. Finally, the inverse
PRT is the {\it view-object transformation} $_{uvw}T^{xyz}$ where

\begin{equation}
_{uvw}T^{xyz} = \left( _{xyz}T^{uvw}\right)^{-1}
\end{equation}

See figure
\ref{figure:coordinate-frames} that shows camera and object from a fixed world
reference frame.

Here are two examples to show the usefulness of these definitions. If you like
to know the origin of the object in view coordinates, you can apply the
object-view transformation to the zero vector, and compute the distance $d$
between camera and object origin as

$$ d = \left|\left|\ _{xyz}T^{uvw}\left[ \vec{0} \right]\ \right|\right|_2$$

Another example, you can compute the distance $d_{12}$ between the two objects
in a scene by

$$ d_{12} = \left|\left|\ _{xyz}T_1^{uvw}\left[ \vec{0} \right] -
                  _{xyz}T_2^{uvw}\left[ \vec{0} \right]\ \right|\right|_2$$

\begin{figure}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{media/coordinate-frames}
    \end{center}
    \caption{Transformation between object and view coordinate frames}
    \label{figure:coordinate-frames}
\end{figure}

Note that there exists a coordinate frame for every object and view.  When
talking in general about object recognition, it is often more intuitive to talk
about the object's {\em pose}. If precise wording is required, it is preferred
to talk about the object-view transformation that brings object coordinates
into view coordinates for a specific view-object pair. This will become
apparent in section \ref{subsection:learning}.

\section{Object Recognition}

Object recognition is the problem of finding or labeling objects in a query
scene. This section describes relevant mathematics and techniques used in
multi-view 3-d object recognition using local 2-d features. References to other
approaches, such as with true 3-d features or object recognition using parametric
models are provided but, being out of scope, not covered in detail.

% \subsubsection{Applications of Object Recognition}
% Mars mission
% Flood detection
% Earthquake detection
% Industrial quality control

\subsection{Local Features}

In object recognition, the question arises on how to generate description of
objects and find candidates that match the description in the query scene.  We
can distinguish between local and global approaches. An example for a global
approach is template matching. Yet, in scenes with occlusion, local methods are
clearly preferred. Object recognition using local features is based on the
detection of keypoints in an image. There is a multitude of different methods
available, but they all aim at choosing keypoints that are repeatable for the
same object over different images. A feature consists of a keypoint and a
feature descriptor that captures information about its local image
neighborhood. Feature descriptors are often vectors and their distance provides
a means of measuring similarity between two features. 
% TODO: cite, maybe Lowe

\subsubsection{Single-View 3-d Object Recognition using Local Features}

In single-view 3-d object recognition, a description of a template object is
extracted from a single view, and matched against a query image. An example for
such an approach can be found in \cite{Lowe1999}.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.4\textwidth]{media/single-view-or}
        \caption{Single-view 3-d object recognition scheme}
    \end{center}
\end{figure}

% TODO: citation
This approach works well, if the object in the query scene has approximately
the same orientation as in the template view. Lowe reports that using SIFT
features, which are discussed in section \ref{subsection:SIFT}, it is possible
to recognize objects up to a 20 degree rotation \cite{Lowe1999}. Unfortunately,
considering only a single view is insufficient if query scenes show objects in
arbitrary orientation, which is especially apparent when considering a 180
degree rotation. 

\subsubsection{Multi-View 3-d Object Recognition using Local Features}
\label{subsubsection:multi-view-or}

In multi-view 3-d object recognition, one recognizes an object in a query scene
by matching it against multiple views of the template object, which have been
recorded from different viewpoints. As such, the multi-view scheme extends the
single-view scheme and allows for recognition of objects with arbitrary
orientation. 

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{media/multi-view-or}
        \caption{Multi-view 3-d object recognition scheme}
    \end{center}
\end{figure}

Local features are extracted from a set of template views. The strategy on how
to choose the template views in the first place has an impact on on both memory
and computation time, and has been subject of research. % TODO: cite Section
The multi-view 3-d object recognition scheme is the basis of this work.
Sections \ref{subsection:feature-detectors} and
\ref{subsection:feature-descriptors} show how to extract a description of the
query image and the template views. Section \ref{subsection:feature-matching}
presents methods for fast establishment of correspondences, and section
\ref{subsection:pose-estimation} shows how to infer on object poses. 

\subsection{Feature Detectors}
\label{subsection:feature-detectors}

% TODO: strip section about feature detectors down to the bare essence

A feature detector designates keypoints on an image.  Some feature detector
algorithms assign orientations to keypoints. Detectors can be roughly
partitioned into three groups. First, there are {\it corner detectors}, such as
Harris and SUSAN, that select points with high curvature. Second, {\it blob
detectors} such as SURF or DoG look for points that are surrounded by all
brighter or darker pixels. Third, there are {\it region detectors}, such as
MSER, which extract features by finding image regions.

There is generally no such thing as the best feature detector. As often in
engineering, all we can do is making the best choice for the task at hand.  An
ideal feature detector though, should be {\it efficient} and it should find
features that are {\it repeatable}, {\it informative}, {\it local}, and {\it
accurate} \cite{Tuytelaars2007}.

Repeatability describes the chances that the same point on an object shown from
two different viewpoints and viewing conditions is detected on both images.
Thus, repeatability is very important when trying to find correspondences
between two images. A feature detector designates keypoints after examining the
local image neighborhood. These neighborhoods can for example be deformed by
noise, image discretization, a change of viewpoint or a change in lighting
conditions \cite{Tuytelaars2007}. These deformations have to be addressed by a
feature detector to achieve repeatability.

An informative feature belongs to an image neighborhood that shows high
variation in intensity. If not, the features would be difficult to distinguish.
For example, we would not like to have features detected on non-textured
background or areas that carry little information.

A local feature only carries information about a small neighborhood. That
ensures that it is still repeatable despite of occlusion. A small neighborhood
also leads to less informative features, enforcing a trade-off between locality
and informativeness.


\subsection{Feature Descriptors}
\label{subsection:feature-descriptors}

A feature descriptor generally computes a vector that describes the local
neighborhood of a keypoint. Ideally, two corresponding keypoints should have
very similar descriptor vectors, such that correspondences be found by
nearest neighbor search. A feature descriptor that allows for efficient
matching is BRIEF \cite{Calonder2010}. 

\begin{table}
    \begin{center}
        \begin{tabular}{lcccc}
            Detector & Multi-scale & Oriented Keypoints \\
            \hline
            SIFT & yes & yes \\
            SURF & yes & yes \\
            FAST & no & no \\
            ORB & yes & yes 
        \end{tabular}
    \end{center}
    \caption{Properties of selected feature detectors}
\end{table}

\subsubsection{SIFT}
\label{subsection:SIFT}

The scale-invariant feature transform became popular in object recognition.  It
is desirable that images of objects at different scales produce more or less
the same features. SIFT achieves this by selecting keypoints from a scale-space
pyramid in order to generate scale-invariant features. SIFT is both a feature
detector and descriptor. Lowe further described a system that lets
correspondences vote in a Hough space, which is dimensioned by location,
orientation and scale \cite{Lowe1999}. When bins contain more than a predefined
number of votes, the object is said to be recognized at this pose.  SIFT was
successfully applied to TODO.

\subsubsection{Oriented BRIEF}

{\it Oriented BRIEF} or in short {\it ORB} is a combination of the FAST feature
detector and the BRIEF feature descriptor, plus modifications that render the
extracted features orientation-invariant. It was designed to outperform SIFT
and SURF at least in terms of computation time. An implementation is already
available in the development branch (trunk) of OpenCV, a corresponding paper is
expected to be published at ICCV 2011.

FAST is a feature detector based on analyzing Bresenham circles around
candidate keypoints. Generated features are neither orientation nor rotation
invariant.

BRIEF is a binary feature descriptor, which is sensitive to rotation. It
computes a bit string for the image neighborhood of a given keypoint. Each bit
stores the result of the comparison of pixel intensities between two 2-d points
in the neighborhood, which are called {\it test locations}. The test locations
were randomly sampled from a 2-d Gaussian distribution and together form a {\it
test pattern}. Finding nearest neighbors of binary descriptors according to
Hamming distance is faster than for real-valued features using the L2 norm.

% TODO: include image for test locations from orb\_patterns in opencv trunk

ORB computes image moments to define orientation for keypoints detected by
FAST. Image moments of order $(p + q)$ are defined for a 2-d digital image $f$
of dimension $MxN$ as

\begin{equation}
    m_{pq} = \sum_{x=0}^{M-1} \sum_{y=0}^{N-1} x^p y^q f(x, y)
\end{equation}

ORB computes moments $m_{01}$, and $m_{10}$ for neighborhoods $f$ of the
keypoints and designates orientations $\alpha$ as

\begin{equation}
    \alpha = \arctan \frac{m_{01}}{m_{10}}
\end{equation}

Consider the vector $\left( m_{01}, m_{10}\right)^T$. The angle of the vector
becomes the orientation of the keypoint. If $f$ were a probability
distribution, then this vector would contain the means of the marginal
distributions of x and y, respectively.

% TODO: verify correlation problem and relation to descriptor entropy

A problem arises, when rotating the BRIEF test pattern according to the FAST
keypoint orientation. There is correlation between the image moments and the
BRIEF vectors obtained from rotated test patterns. Such correlation reduces
entropy of the BRIEF descriptor. The solution is to compute test patterns that
have only little correlation.

A more detailed theoretical treatment of ORB should be deferred until the
publication of the corresponding authoritative paper. The description about ORB
given here was based on knowledge extracted from a presentation at ICRA 2011 in
Shanghai and the ORB implementation in OpenCV trunk, and is thus to be handled
with care.

\subsection{Feature Matching}
\label{subsection:feature-matching}

% TODO: speed-up must be backed up by a benchmark in the evaluation section

In multi-view object recognition using local features, correspondences between
known views of the object and the query image can be established by matching
features from the query images to the features from the model. Computation time
depends on the size of the descriptor vector, the nearest neighbor algorithm
and the used distance metric. Great speed-ups can be achieved by weakening the
requirements and allowing for approximate nearest neighbors. Computing the
distance between binary features is faster than with real-valued feature
descriptors.

\subsubsection{Locality Sensitive Hashing}

% TODO: consistent use of uppercase and lowercase in algorithm names

SIFT, ORB and other feature detectors and descriptors are used to describe an
image in terms of local features. Nearest neighbor matching can then find
correspondences between two feature sets. Locality Sensitive Hashing (LSH)
quickly finds nearest neighbors, and trades correctness in favor of a reduction
in computation time. Not always does it return the absolute nearest neighbor.
Locality Sensitive Hashing is a randomized algorithm that performs multiple
scalar projections of a high-dimensional vector and reasons that the scalar
projections are similar for similar input vectors. It can be extended to find
nearest neighbors for binary features. LSH has applications in object
recognition, image retrieval, music rerieval, identification of duplicates
\cite{Slaney2008}.

\subsection{Pose Estimation}
\label{subsection:pose-estimation}

% TODO: relation to PnP problem
% TODO: I will not show how to solve this problem

This section shows how to estimate the pose of objects, given the set of
correspondences obtained from feature matching. It covers the mathematical
formulae and notation about coordinate system transformations that are required
when dealing with a scene that is recorded from different viewpoints.

Estimating the pose of an object in a scene is nothing more than estimating 
the object-view transformation, as the latter uniquely defines where each
3-d point belonging to the object is located with respect to the view coordinate
frame, i.e. the camera.

\subsubsection{Hough Transform}

% TODO: decide whether Hough Transform is really necessary, probably luxury section
% that can be skipped

\subsubsection{RANSAC}

RANSAC stands for {\it RANdom SAmple Consensus} and is a randomized algorithm
that fits a parametrized model to given data, which is robust to outliers.  The
pose estimation problem in this context can be reduced to the location
determination problem addressed by the paper that introduced RANSAC
\cite{Fischler1981}.  Generally, this algorithm randomly chooses a minimum set
of data points that fully determine the model parameters. Then it forms a
consensus set of {\it inliers}, that is, all data points that are consistent
with the model up to a certain margin of error. RANSAC constructs a fixed
number of such consensus sets and then chooses the model with the largest
consensus as an estimate.

Pose estimation using RANSAC can be categorized as a method of {\it pose
consistency} or {\it alignment}, where first a pose is hypothesized, and
then the hypothesized pose is verified for support by the remaining data
\cite{Forsyth2003}.

There are six degrees of freedom for an object to be located and oriented in
space, three for translation and three for orientation. 
% TODO: how many projected points do we need
% Three 2-d points in perspective projection are INsufficient to fully determine a pose.
If there were no noise in the set of 2-d correspondences, we would be able to
find poses of an object by simply taking any three correspondences belonging to
this object. Unfortunately, image noise, discretization effects and approximate
nearest neighbors asks for a more sophisticated approach that incorporates
strategies to deal with noise and inconsistency. RANSAC iteratively selects
three correspondences to compute pose estimates. The 3-d model of the template
object aligned to the estimated pose is then projected on the query image plane
to compute the projection error and the consensus set. The size of the
resulting consensus sets plays an important role in this work.

\section{Machine Learning}

This section shows how object recognition relates to machine learning,
especially to classification and estimation theory. It also covers the basics
of a machine learning experiment, which are required for optimizing parameters
and understanding the strengths and weaknesses of an object recognition system.

\subsection{Classification}

Assume, there were no interest in the pose of an object. Assume that objects
are unique on an image. Then object recognition can be seen as a {\it
multi-label classification} problem, as described in \cite{Tsoumakas2007}.  An
image is described by a set of labels, and symmetrically, the classification
result is a set of labels that predict which objects are on the image.
Receiver operating characteristics (ROC) can be used to evaluate standard
two-class classifiers \cite{Fawcett2006}. In case of multi-label
classification, it is possible to define similar metrics \cite{Tsoumakas2007}.
Unfortunately, even if object pose were not an issue, assuming objects to
appear uniquely on an image might be unrealistic. For example, a shopping bag
as in the application scenario of this work is best represented by a multiset,
and not by a multi-label set.

\subsection{Estimation}

Estimation can be regarded as the continuous extension of classification
\cite{Melsa1978}. Clearly, if it is known which objects are on a scene, object
recognition can be seen as an estimation problem.

\subsection{Recognition}

The recognition problem, where objects have to be both correctly labeled and
its location determined therefore neither fits a pure classification nor a pure
estimation problem. In this work, measures of ``goodness'' see the object
recognition system as a combination of classifier. Estimation ``goodness'' is
conditioned to the classifier making the right choice.

\subsection{Parameter Optimization}

In the presented system, the algorithms for detecting, describing and matching
features are parametrized, as well as the pose estimation and the final
decision process. This leads to the question on how to find a parameter set
such that the system will perform well in the application scenario.

\subsubsection{Validation Set}

Choosing parameters for parametrized algorithms is a very general problem in
computer science. Solutions for these problems include adaptive methods that
estimate the parameters directly from the data. For example, in adaptive
numerical quadrature, grid points are selected according to an error estimate
gained directly from function samples. Optimization techniques find a parameter
set that optimizes performance on a validation set, where ground truth is
avaible. This optimization technique is well-known in machine learning.

\subsubsection{Test Set}

It is interesting to estimate the error which a classifier or estimator is
expected to make when working on unseen instances. This error cannot be
estimated on the validation set, Instead a separate test set is required
\cite{Alpaydin2010}.
% , and is related to the problem of assuming appropriate priors
% in Bayesian inference.

